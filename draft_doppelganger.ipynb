{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79d1ec79-9d3b-48d7-93a1-1a1a25236896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(74.1456)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # You can use other GPT-2 variants as well\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# Add the [PAD] token to the tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Define your input texts as a list\n",
    "input_texts = \"Once upon a time, there was a\" \n",
    "# [\n",
    "    # \"Once upon a time, there was a\",\n",
    "    # \"In a galaxy far, far away, there were\",\n",
    "    # Add more input texts as needed\n",
    "# ]\n",
    "\n",
    "# Tokenize the input texts and convert them to tensors\n",
    "input_ids = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "\n",
    "# Shift the input_ids to the right by one position to create target_ids\n",
    "# target_ids = input_ids[:, 1:]\n",
    "\n",
    "# Generate logits from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, labels =  input_ids)\n",
    "    logits = outputs.logits\n",
    "    loss = outputs.loss\n",
    "    \n",
    "# # Reshape target_ids to match the shape of logits\n",
    "# target_ids = target_ids.reshape(-1)\n",
    "\n",
    "# # Calculate the CrossEntropyLoss\n",
    "# loss_fn = CrossEntropyLoss()\n",
    "# loss = loss_fn(logits.view(-1, logits.shape[-1]), target_ids)\n",
    "\n",
    "# print(\"CrossEntropyLoss:\", loss.item())\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0af0df92-8a1f-4fdc-8619-daa31e4f9376",
   "metadata": {},
   "source": [
    "## Chaning Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4a1d9b2-c49b-426e-99ee-45769e524632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(74.1456)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_logits = logits[..., :-1, :].contiguous()\n",
    "shift_labels = input_ids[..., 1:].contiguous()\n",
    "temperature = 1\n",
    "# Apply temperature scaling to the logits\n",
    "shift_logits = shift_logits / temperature\n",
    "\n",
    "# Flatten the tokens\n",
    "loss_fct = CrossEntropyLoss()\n",
    "loss_ce = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "loss_ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81a2992d-87ea-4509-9d27-3225cddb7613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_filtering(logits, top_k=50):\n",
    "    \"\"\"\n",
    "    Apply top-k filtering to select a reduced set of candidates from logits.\n",
    "    \"\"\"\n",
    "    values, indices = torch.topk(logits, top_k, dim=-1)\n",
    "    min_values = values[:, -1].unsqueeze(-1).repeat(1, logits.shape[-1])\n",
    "    logits = torch.where(logits < min_values, torch.tensor(-float('Inf')), logits)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98aa5c0b-1f19-4439-827e-e03604320c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Words: [' great', ' man', ' small']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # You can use other GPT-2 variants as well\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Add the [PAD] token to the tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Set the top-k value (e.g., 50 for top-50 sampling)\n",
    "top_k = 50\n",
    "# Set the number of candidates you want to return\n",
    "num_candidates = 3\n",
    "\n",
    "# Define your input text\n",
    "input_text = \"Once upon a time, there was a\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the next words using top-k sampling\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids).logits[:, -1, :]  # Get the logits for the last word\n",
    "    # Apply top-k filtering to select a reduced set of candidates\n",
    "    filtered_logits = top_k_filtering(logits, top_k=top_k)\n",
    "    # Sample from the filtered candidates, returning num_candidates samples\n",
    "    next_token_ids = torch.multinomial(torch.softmax(filtered_logits, dim=-1), num_candidates)\n",
    "\n",
    "# Convert the generated token IDs to words\n",
    "generated_words = [tokenizer.decode(token_id.item()) for token_id in next_token_ids[0]]\n",
    "\n",
    "print(\"Generated Words:\", generated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ce5a608-a3db-4688-9ff1-cece5f03f4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6111111111111112"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import privacy_clue_words\n",
    "def dynamic_temperature(x, privacy_clue_words = privacy_clue_words):\n",
    "    x = text.split()\n",
    "    n = len(x)\n",
    "    m = n - 1 \n",
    "    xi_index = None\n",
    "    for i, token in enumerate(x):\n",
    "        if token in privacy_clue_words:\n",
    "            xi_index = i\n",
    "            break\n",
    "    T, alpha = 1, 0.5\n",
    "    if xi_index is not None:\n",
    "        temperature_m = T + alpha * n / abs(xi_index - m)\n",
    "    else:\n",
    "        temperature_m = T\n",
    "    # n, m, xi_index, temperature_m\n",
    "    return temperature_m\n",
    "\n",
    "text = \"My Password is 555-456 which can unlock everything in my house\"\n",
    "dynamic_temperature(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dfca6b0-88c2-419d-a7cd-acb87a51e8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "class argument:\n",
    "    def __init__(self):\n",
    "        self.dataset_name = None # 'wikitext'\n",
    "        # self.dataset_config_name = 'wikitext-2-raw-v1'\n",
    "        self.output_dir = './logs/' \n",
    "        self.seed = 5555\n",
    "        self.learning_rate = 5e-5\n",
    "        self.block_size = 1024\n",
    "        self.do_ref_model = False\n",
    "        self.hidden_size = 768\n",
    "        self.beta = 1\n",
    "        \n",
    "        self.config_name = None\n",
    "        self.model_name_or_path = 'gpt2'\n",
    "        self.tokenizer_name = 'gpt2'\n",
    "        self.use_slow_tokenizer = False  \n",
    "        \n",
    "args = argument()\n",
    "\n",
    "if args.config_name:\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "elif args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "    \n",
    "if args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "\n",
    "if args.model_name_or_path:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training new model from scratch\")\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17613623-b764-411a-8291-486157a43188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'Ġpassword', 'Ġis', 'Ġ5', '32', '-', '789']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the [PAD] token to the tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "input_texts = \"My password is 532-789\" \n",
    "# Tokenize the input texts and convert them to tensors\n",
    "input_ids = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "# Convert input_ids back to words\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b843ddc0-0803-434c-b4c7-9c5b36512f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'password', 'is', '5', '32', '-', '789']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tokens = [token.lstrip('Ġ') for token in decoded_tokens]\n",
    "cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aa8eb40-e4ec-4b81-9325-078455cbef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate logits from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, labels =  input_ids)\n",
    "    logits = outputs.logits\n",
    "    loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f7e397a-f598-46a1-bad7-71cd39f7e76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6111111111111112"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_T = dynamic_temperature(cleaned_tokens)\n",
    "dynamic_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "528d15ee-dd8c-4216-a190-60df830bb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss1\n",
    "logits_original = logits #student -> log softmax\n",
    "#loss2 dynamic temperature\n",
    "logits_temp = logits #teacher -> softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb6babae-8b9e-46b9-9b99-a7ff32151d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(78.3643)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fa1b8b7-e1da-4714-a589-b6d663d788f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0481e-07)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(62.6915)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss, KLDivLoss\n",
    "import torch.nn.functional as F\n",
    "T = 1\n",
    "loss_kd = KLDivLoss()\n",
    "KD_loss = loss_kd(F.log_softmax(logits_temp/dynamic_T, dim=1), F.softmax(logits_original/T, dim=1)) \n",
    "print(KD_loss)\n",
    "\n",
    "alpha = 0.2\n",
    "total = alpha * KD_loss + (1-alpha) * loss\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4fe034b-4b75-4576-87a6-d927ea5c5611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tele', 'phone', 'Ġnumber', 'Ġis']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_ids = tokenizer('telephone number is', return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "# decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "# decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad7aa0a2-9b19-4d47-9633-0a218ec65e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = tokenizer('my business trip', return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "# decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "# decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ad452-b957-4ec2-a847-886a6ac2dea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
