{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP-SGD (Differentially-Private Stochastic Gradient Descent)\n",
    "reference : https://medium.com/pytorch/differential-privacy-series-part-1-dp-sgd-algorithm-explained-12512c3959a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(lr=args.lr)\n",
    "\n",
    "for batch in Dataloader(train_dataset, batch_size=32):\n",
    "    x, y = batch\n",
    "    y_hat = model(x)\n",
    "    loss = criterion(y_hat, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Now these are filled:\n",
    "    gradients = (p.grad for p in model.parameters())\n",
    "  \n",
    "    for p in model.parameters():\n",
    "\n",
    "        # Add our differential privacy magic here\n",
    "        p.grad += torch.normal(mean=0, std=args.sigma)\n",
    "        \n",
    "        # This is what optimizer.step() does\n",
    "        p = p - args.lr * p.grad\n",
    "        p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(lr=args.lr)\n",
    "\n",
    "for batch in Dataloader(train_dataset, batch_size=32):\n",
    "    all_per_sample_gradients = [] # will have len = batch_size\n",
    "    for sample in batch:\n",
    "        x, y = sample\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()  # Now p.grad for this x is filled\n",
    "        \n",
    "        # Need to clone it to save it\n",
    "        per_sample_gradients = [p.grad.detach().clone() for p in model.parameters()]\n",
    "        \n",
    "        all_per_sample_gradients.append(per_sample_gradients)\n",
    "        model.zero_grad()  # p.grad is cumulative so we'd better reset it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './figures/dp-sgd-algorithm.png' width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Compute the per-sample gradients\n",
    "\n",
    "2.Clip them to a fixed maximum norm\n",
    "\n",
    "3.Aggregate them back into a single parameter gradient\n",
    "\n",
    "4.Add noise to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "optimizer = torch.optim.SGD(lr=args.lr)\n",
    "\n",
    "for batch in Dataloader(train_dataset, batch_size=32):\n",
    "    for param in model.parameters():\n",
    "        param.accumulated_grads = []\n",
    "    \n",
    "    # Run the microbatches\n",
    "    for sample in batch:\n",
    "        x, y = sample\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "    \n",
    "        # Clip each parameter's per-sample gradient\n",
    "        for param in model.parameters():\n",
    "            #1.compute the per-sample gradients\n",
    "            per_sample_grad = p.grad.detach().clone()\n",
    "            #clip them to a dixed maximum norm\n",
    "            clip_grad_norm_(per_sample_grad, max_norm=args.max_grad_norm)  # in-place\n",
    "            param.accumulated_grads.append(per_sample_grad)  \n",
    "        \n",
    "    # 3. Aggregate back\n",
    "    for param in model.parameters():\n",
    "        param.grad = torch.stack(param.accumulated_grads, dim=0)\n",
    "\n",
    "    # Now we are ready to update and add noise!\n",
    "    for param in model.parameters():\n",
    "        param = param - args.lr * param.grad\n",
    "        #4. add noise\n",
    "        param += torch.normal(mean=0, std=args.noise_multiplier * args.max_grad_norm)\n",
    "        \n",
    "        param.grad = 0  # Reset for next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus import PrivacyEngine\n",
    "# define your components as usual\n",
    "model = Net()\n",
    "optimizer = SGD(model.parameters(), lr=0.05)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=1024)\n",
    "\n",
    "# enter PrivacyEngine\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, data_loader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=data_loader,\n",
    "    noise_multiplier=1.1,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "# Now it's business as usual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
