{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class argument:\n",
    "    def __init__(self):\n",
    "        self.dataset_name = 'wikitext'\n",
    "        self.dataset_config_name = 'wikitext-2-raw-v1'\n",
    "        self.output_dir = './logs/' \n",
    "        self.seed = 1234\n",
    "        self.learning_rate = 5e-5\n",
    "        self.block_size = 1024 \n",
    "        self.do_ref_model = False\n",
    "        \n",
    "        self.config_name = None\n",
    "        self.model_name_or_path = 'gpt2'\n",
    "        self.tokenizer_name = 'gpt2'\n",
    "        self.use_slow_tokenizer = False\n",
    "        \n",
    "        self.per_device_train_batch_size = 4\n",
    "        self.per_device_eval_batch_size = 4\n",
    "        self.gradient_accumulation_steps = 8\n",
    "        \n",
    "        # self.eval_steps = 50\n",
    "        self.do_ref_model = False\n",
    "        self.lr_scheduler_type = 'linear'\n",
    "\n",
    "        self.num_train_epochs = 5\n",
    "        self.max_train_steps = None\n",
    "\n",
    "        self.preprocessing_num_workers = 1\n",
    "        self.overwrite_cache = False\n",
    "        self.weight_decay = 0.0\n",
    "        self.num_warmup_steps = 0\n",
    "        \n",
    "        self.add_canary = True\n",
    "        self.canary_rep = 50\n",
    "        self.canary_len = 5\n",
    "        \n",
    "        self.add_adapter = False\n",
    "        self.adapter_reduction = 16\n",
    "        self.train_head_only = False\n",
    "        self.train_layer_n_only = None \n",
    "        \n",
    "        \n",
    "args = argument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import unique\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "import copy \n",
    "from sys import path\n",
    "import sys\n",
    "# from utils import Logger\n",
    "\n",
    "from accelerate import Accelerator\n",
    "import datasets\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./logs//canary_\n"
     ]
    }
   ],
   "source": [
    "random.seed(args.seed)\n",
    "\n",
    "folder_name = f\"canary_\"\n",
    "directory = \"{}/{}\".format(args.output_dir,folder_name)\n",
    "print(directory)\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "# log_file = os.path.join(directory, \"stdout\")\n",
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# if accelerator.is_local_main_process:\n",
    "    # print(\"Logging to {}\".format(log_file))\n",
    "    # pass\n",
    "    \n",
    "# sys.stdout = Logger(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0858192044485389a09b8095173f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    if 'enron' in args.dataset_name:\n",
    "        raw_datasets = load_dataset('csv', data_files={'train': 'data/cleaned_short_train_scrubbed.csv' ,'validation': 'data/cleaned_short_test_scrubbed.csv'})\n",
    "        #raw_datasets['train'] = load_dataset('csv', data_files={'train': 'data/cleaned_train.csv' ,'validation': 'data/cleaned_test.csv'}, split='train[:4000]')\n",
    "        #raw_datasets['validation'] = load_dataset('csv', data_files={'train': 'data/cleaned_train.csv' ,'validation': 'data/cleaned_test.csv'}, split='train[4000:5000]')\n",
    "\n",
    "    else:\n",
    "        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n",
    "        if \"validation\" not in raw_datasets.keys():\n",
    "            raw_datasets[\"validation\"] = load_dataset(\n",
    "                args.dataset_name,\n",
    "                args.dataset_config_name,\n",
    "                split=f\"train[:{args.validation_split_percentage}%]\",\n",
    "            )\n",
    "            raw_datasets[\"train\"] = load_dataset(\n",
    "                args.dataset_name,\n",
    "                args.dataset_config_name,\n",
    "                split=f\"train[{args.validation_split_percentage}%:]\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_canary(canary_len,tokenizer):\n",
    "    raw_sample = random.choices([str(i) for i in range(10)], k=canary_len)\n",
    "    raw_sample = \" \".join(raw_sample)\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(raw_sample)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    assert len(ids) == canary_len\n",
    "    \n",
    "    raw_sample = \"the secret number is \" + raw_sample\n",
    "    toked =  tokenizer(raw_sample)\n",
    "    toked['labels'] = toked['input_ids'].copy()\n",
    "    return raw_sample, toked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-15d97228463ce8de.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before canary len  36718\n",
      "after canary len  36768\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "if args.add_canary:    \n",
    "    if 'ptb' in args.dataset_name:\n",
    "        dict_key = 'sentence'\n",
    "    else:\n",
    "        dict_key='text'\n",
    "    print(\"before canary len \", len(raw_datasets['train'][dict_key]))\n",
    "    canary, canary_ids = gen_canary(args.canary_len, tokenizer)\n",
    "    for j in range(args.canary_rep):\n",
    "        raw_datasets['train']=raw_datasets['train'].add_item({dict_key:canary})\n",
    "\n",
    "    raw_datasets['train'] = raw_datasets['train'].shuffle(seed=args.seed)\n",
    "    print(\"after canary len \", len(raw_datasets['train'][dict_key]))\n",
    "    # save the canaries in csv\n",
    "\n",
    "    file = open(f'./{directory}/canaries.txt', 'w+')\n",
    "    file.write(canary)\n",
    "    file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "    file = open(f'./{directory}/fitting_canaries.txt', 'w+')\n",
    "    \n",
    "    fitting_canaries_ids = []\n",
    "    for i in range(5000):\n",
    "        fit , fit_ids = gen_canary(args.canary_len,tokenizer)\n",
    "        if fit != canary:\n",
    "            fitting_canaries_ids.append(fit_ids)\n",
    "            file.write(fit)\n",
    "            file.write('\\n')\n",
    "    print(len(fitting_canaries_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = \"train_private_token.pkl\"\n",
    "# with open(file_path, \"wb\") as file:\n",
    "#     pickle.dump(train_private_token, file)\n",
    "with open(file_path, \"rb\") as file:\n",
    "    train_private_token = pickle.load(file)\n",
    "\n",
    "file_path = \"validation_private_token.pkl\"\n",
    "# with open(file_path, \"wb\") as file:\n",
    "#     pickle.dump(validation_private_token, file)\n",
    "\n",
    "with open(file_path, \"rb\") as file:\n",
    "    validation_private_token = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'private_ids'],\n",
       "        num_rows: 36768\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'private_ids'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "# Create new dictionaries with both 'text' and 'labels' features\n",
    "new_train_dict = {\n",
    "    \"text\": raw_datasets[\"train\"][\"text\"],\n",
    "    \"private_ids\": train_private_token,\n",
    "}\n",
    "new_validation_dict = {\n",
    "    \"text\": raw_datasets[\"validation\"][\"text\"],\n",
    "    \"private_ids\": validation_private_token,\n",
    "}\n",
    "# new_test_dict = {\n",
    "#     \"text\": raw_datasets[\"test\"][\"text\"],\n",
    "#     # \"private_position\": test_private_token,\n",
    "# }\n",
    "\n",
    "# Create a new DatasetDict with the additional 'labels' feature\n",
    "new_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict(new_train_dict),\n",
    "    \"validation\": Dataset.from_dict(new_validation_dict),\n",
    "    # \"test\": Dataset.from_dict(new_test_dict),\n",
    "})\n",
    "\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22636ab2709418d98270a985c52b41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/36768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e4a68ce83b47a3ab6089c24a53cc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    tokenized_datasets = new_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "if args.block_size is None:\n",
    "    block_size = tokenizer.model_max_length\n",
    "    if block_size > 1024:\n",
    "        print(\n",
    "            f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "            \"Picking 1024 instead. You can change that default value by passing --block_size xxx.\"\n",
    "        )\n",
    "    block_size = 1024\n",
    "else:\n",
    "    if args.block_size > tokenizer.model_max_length:\n",
    "        print(\n",
    "            f\"The block_size passed ({args.block_size}) is larger than the maximum length for the model\"\n",
    "            f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    block_size = min(args.block_size, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772f25351e8049a9b2c68602e62a1a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/36768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4bb59afb7384a00b8bd842772c91389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    \n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "    \n",
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n",
    "# for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n",
    "# to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    )\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['private_ids', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 2035\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['private_ids', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 213\n",
       " }))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log a few random samples from the training set:\n",
    "#for index in random.sample(range(len(train_dataset)), 3):\n",
    "#    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "# DataLoaders creation:\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=default_data_collator, batch_size=args.per_device_eval_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['private_ids', 'input_ids', 'attention_mask', 'labels'])\n",
      "torch.Size([4, 1024]) torch.Size([4, 1024]) torch.Size([4, 1024])\n",
      "torch.Size([4, 1024]) torch.Size([4, 1024]) torch.Size([4, 1024])\n"
     ]
    }
   ],
   "source": [
    "#checking chucking\n",
    "for batch in train_dataloader:\n",
    "    print(batch.keys())\n",
    "    print(batch['input_ids'].shape, batch['private_ids'].shape, batch['labels'].shape)\n",
    "    break\n",
    "for batch in eval_dataloader:\n",
    "    print(batch['input_ids'].shape, batch['private_ids'].shape, batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinaryEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryEmbedding, self).__init__()\n",
    "        self.embedding_dim = 768\n",
    "        \n",
    "        # Create a single embedding for the binary variable\n",
    "        self.embedding = nn.Embedding(2, self.embedding_dim)\n",
    "        \n",
    "    def forward(self, private_ids=None, input_ids=None, attention_mask=None, labels=None):\n",
    "        # Convert input indices to binary vectors (0 or 1)\n",
    "        binary_input = torch.where(private_ids == 0, torch.tensor(0), torch.tensor(1))\n",
    "        binary_label = torch.where(labels == 0, torch.tensor(0), torch.tensor(1))\n",
    "        \n",
    "        # Get embeddings for the binary vectors\n",
    "        binary_embeddings = self.embedding(binary_input)\n",
    "        binary_embeddings_label = self.embedding(binary_label)\n",
    "        return binary_embeddings, binary_embeddings_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the BinaryEmbedding class as provided earlier\n",
    "\n",
    "# Example usage\n",
    "learning_rate = 0.01\n",
    "num_epochs = 50\n",
    "\n",
    "# Create binary embedding model\n",
    "binary_embedding_model = BinaryEmbedding()\n",
    "optimizer = optim.SGD(binary_embedding_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.7945, -1.8433,  1.3301,  ...,  0.1079, -0.6374, -1.0926],\n",
       "        [ 0.0978,  0.8307, -0.0602,  ..., -1.6210,  0.5329,  0.6016]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_embedding_model.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50731cbe49f9497bbf819ee4f67a486f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d996bdb9155d4a238297979c71ef5af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779d365a3c81443d84a583141ff91cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee98b6af0e2c4b139482481af7ca7136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5360e86639c4cd5949e0b85835b17f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 1.3637\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08663eb5a284514b5c1d162c015b7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cf4780161047cd852a73f20e3cf40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a02fedfd6f41ea997f02859fb93e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3911a64e7155412a8a564ccc9b7ab9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5992a421e7e54c97881e518b806fb4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 1.1656\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce0150221f14ca3a1fcf3e38f928430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce66e0b2b9444feea5bcf0199426304f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c74098b1dc043288deeb573e592135b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d033f107fc456484eedab17fc23690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46452221dbc4181bd19e66fd87d57d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Loss: 0.9341\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55ffc3e5cca4b0289a75c4afeee6d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a3d9869174458d8f66446eabef8c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa7a8276ca5499a87c49aafe55576a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3d86b699b94dd8902f83314d0e2159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a75f019f5cb4b32896bf40b4e7c7ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Loss: 0.7774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a9e1548e0c466d8b98bddf390a4192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221590c386814d05ad175b01c6ee2490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685be124b2e141c8bdf676b5327c9ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff90412362445928c0ce4020c0d8781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d5bf0b37e146579864b643f9552370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Loss: 0.6468\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f6b0d4cb044bb994a4b99837f31ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568438a3e6bf41f4af94b4f5fd2e4bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0a9646d1044ed3ba70a70e5b50dde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59213574bb2247cf84d6d05c007d1113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6b9590f26646be9c07ff4f98c9cc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Loss: 0.5699\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398a37c06e4542b3a5be889cb0e16765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b875fc1a4241798ccf70869ea660f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b7b19706f6491bb9132d295ffd8bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f05247af5044760b8514eed9e9507b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d3d95d19ed4882ab3d72d85493e013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50], Loss: 0.4553\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c7ada82ed741aa8e393fb8f3d4f0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c7d4bb8f2a404497a8fdf866b1ecec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136db96a305c460cb8dcaf9abfbce546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c143b6f36cd749a0b3b06171a3e9fb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e040b93020bb44d48dc91ace7e17ea20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Loss: 0.3787\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddfc6a5dbde44da96755e6a28c9bfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abc14b3c3f74e44a889f4d281791c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e1878b59b6436caf94a922febbaf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb838498f5548b3a5d5125b1bad6ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33798ad9e8cd48d5be3767bc92d07f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/50], Loss: 0.3194\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de83bf795b044bb7b0129f46e1501a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc437d8e793145aebff824a301abfa47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77efd2b4ad624f42b048572b979bf419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9ca39b211a46778976d82ad7884b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d6d4bc1647483b883add5c2117a079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Loss: 0.2678\n"
     ]
    }
   ],
   "source": [
    "# Input indices representing categories (\"privacy\" and \"non-private\")\n",
    "# input_indices = torch.tensor([0, 1, 1, 0])\n",
    "\n",
    "# Target values (can be same as input indices for this case)\n",
    "# target_indices = input_indices\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        binary_embeddings, binary_embeddings_label = binary_embedding_model(**batch)\n",
    "\n",
    "        loss = F.mse_loss(binary_embeddings, binary_embeddings_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 5 == 0: \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Print the final binary embeddings\n",
    "# final_embeddings = binary_embedding_model(input_indices)\n",
    "# print(\"Final Binary Embeddings:\")\n",
    "# print(final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(binary_embedding_model.state_dict(), 'binary_embedding_model.pth')\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5892, -1.0553,  0.9204,  ..., -0.4016, -0.2926, -0.5933],\n",
       "        [ 0.3032,  0.0427,  0.3495,  ..., -1.1115,  0.1880,  0.1024]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_embedding_model.load_state_dict(torch.load('binary_embedding_model.pth'))\n",
    "binary_embedding_model.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
