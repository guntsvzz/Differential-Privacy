{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce50fb18",
   "metadata": {},
   "source": [
    "# dx-privacy preserving mechanism\n",
    "![MADLIB mechanism](figure/MADLIB.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2687d18-e96d-4860-ae8c-ce33c347def0",
   "metadata": {},
   "source": [
    "## GPT2 with dx-privacy preserving mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0466f02d-2881-4bd9-accc-9d235d8e1716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AutoTokenizer, GPT2Config, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=not False)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2', from_tf=bool(\".ckpt\" in 'gpt2'), config=config)\n",
    "\n",
    "word = \"my secret number is 9 4 0 9 9 5\"  # Replace with your target word\n",
    "\n",
    "#Privacy Preserving Mechanism\n",
    "predicted_word_list = []\n",
    "for i in word.split():\n",
    "    # Step 1: Retrieve the vector representation of the word\n",
    "    inputs = tokenizer(i, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        # outputs = model(**inputs, output_hidden_states=True)\n",
    "        outputs = model.get_input_embeddings()(inputs['input_ids'][0]).squeeze()\n",
    "\n",
    "    vector_representation = outputs.numpy() #.hidden_states[-1].mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    # # Step 2: Perturb the vector representation with noise sampled from a multivariate distribution\n",
    "    # mean = np.zeros(vector_representation.shape)  # Mean of the multivariate distribution\n",
    "    # covariance = np.eye(vector_representation.shape[0])  # Covariance matrix of the distribution\n",
    "    # noise = np.random.multivariate_normal(mean, covariance, size=1)\n",
    "    # noisy_representation = vector_representation + noise\n",
    "\n",
    "    # Step 2: Perturb the vector representation with noise sampled from a normal distribution\n",
    "    mean = 0.0  # Mean of the normal distribution\n",
    "    std_dev = 0.5  # Standard deviation of the normal distribution\n",
    "    noise = np.random.normal(mean, std_dev, size=vector_representation.shape)\n",
    "    noisy_representation = vector_representation + noise\n",
    "\n",
    "    # Step 3: Project the noisy representation of the word back to the discrete vocabulary space\n",
    "    # Find the token that is closest in embedding space to the noisy representation\n",
    "    embedding_weights = model.transformer.wte.weight.data.numpy()\n",
    "    distances = np.linalg.norm(embedding_weights - noisy_representation, axis=1)\n",
    "    closest_token_id = np.argmin(distances)\n",
    "\n",
    "    # Convert the token ID back to the word\n",
    "    predicted_word = tokenizer.decode([closest_token_id])\n",
    "    predicted_word_list.append(predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a26cfb8e-8caf-4bdd-be43-1b54ac5a1a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my my\n",
      "secret secret\n",
      "number number\n",
      "is is\n",
      "9 9\n",
      "4  inducing\n",
      "0 0\n",
      "9 12\n",
      "9  SHOW\n",
      "5 5\n"
     ]
    }
   ],
   "source": [
    "for ori, per in zip(word.split(), predicted_word_list):\n",
    "    print(ori, per)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0598173-c2f3-4d26-8be3-8623b144e0d6",
   "metadata": {},
   "source": [
    "## BERT with dx-privacy preserving mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efaefe69-bd36-45cf-8825-b92c383ae706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "word = \"my secret number is 9 4 0 9 9 5\"  # Replace with your target word\n",
    "\n",
    "# Privacy Preserving Mechanism\n",
    "predicted_word_list = []\n",
    "for i in word.split():\n",
    "    # Step 1: Retrieve the vector representation of the word\n",
    "    inputs = tokenizer(i, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    vector_representation = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    # # Step 2: Perturb the vector representation with noise sampled from a multivariate distribution\n",
    "    # mean = np.zeros(vector_representation.shape)  # Mean of the multivariate distribution\n",
    "    # covariance = np.eye(vector_representation.shape[0])  # Covariance matrix of the distribution\n",
    "    # noise = np.random.multivariate_normal(mean, covariance, size=1)\n",
    "    # noisy_representation = vector_representation + noise\n",
    "\n",
    "    # Step 2: Perturb the vector representation with noise sampled from a normal distribution\n",
    "    mean = 0.0  # Mean of the normal distribution\n",
    "    std_dev = 0.1  # Standard deviation of the normal distribution\n",
    "    noise = np.random.normal(mean, std_dev, size=vector_representation.shape)\n",
    "    noisy_representation = vector_representation + noise\n",
    "\n",
    "    # Step 3: Project the noisy representation of the word back to the discrete vocabulary space\n",
    "    # Find the token that is closest in embedding space to the noisy representation\n",
    "    embedding_weights = model.get_input_embeddings().weight.data.numpy()\n",
    "    distances = np.linalg.norm(embedding_weights - noisy_representation, axis=1)\n",
    "    closest_token_id = np.argmin(distances)\n",
    "\n",
    "    # Convert the token ID back to the word\n",
    "    predicted_word = tokenizer.decode([closest_token_id])\n",
    "    predicted_word_list.append(predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8ba9196-6d25-4448-992b-50bad29c9a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my .\n",
      "secret morris\n",
      "number .\n",
      "is letters\n",
      "9 italian\n",
      "4 4\n",
      "0 morris\n",
      "9 andrea\n",
      "9 influenced\n",
      "5 from\n"
     ]
    }
   ],
   "source": [
    "for ori, per in zip(word.split(), predicted_word_list):\n",
    "    print(ori, per)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
