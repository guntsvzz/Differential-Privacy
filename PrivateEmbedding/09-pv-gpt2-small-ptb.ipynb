{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed33cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #comment this if you are not using AIT proxy...\n",
    "# import os\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bcfa259-ea44-4826-82af-a453acb09831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e622155-bf93-43f3-aaff-a2170412e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class argument:\n",
    "    def __init__(self):\n",
    "        self.dataset_name = 'ptb_text_only'\n",
    "        self.dataset_config_name = None\n",
    "        self.output_dir = './logs/' \n",
    "        self.seed = 1234\n",
    "        self.learning_rate = 5e-5\n",
    "        self.block_size = 1024 \n",
    "        self.do_ref_model = False\n",
    "        \n",
    "        self.config_name = None\n",
    "        self.model_name_or_path = 'gpt2'\n",
    "        self.tokenizer_name = 'gpt2'\n",
    "        self.use_slow_tokenizer = False\n",
    "        \n",
    "        self.per_device_train_batch_size = 8\n",
    "        self.per_device_eval_batch_size = 8\n",
    "        self.gradient_accumulation_steps = 8\n",
    "        \n",
    "        self.do_ref_model = False\n",
    "        self.lr_scheduler_type = 'linear'\n",
    "\n",
    "        self.num_train_epochs = 5\n",
    "        self.max_train_steps = None\n",
    "\n",
    "        self.preprocessing_num_workers = 1\n",
    "        self.overwrite_cache = False\n",
    "        self.weight_decay = 0.0\n",
    "        self.num_warmup_steps = 0\n",
    "        \n",
    "        self.add_canary = True\n",
    "        self.canary_rep = 50\n",
    "        self.canary_len = 5\n",
    "        \n",
    "        self.add_adapter = False\n",
    "        self.adapter_reduction = 16\n",
    "        self.train_head_only = False\n",
    "        self.train_layer_n_only = None \n",
    "        self.redact_token = 'multi'\n",
    "        \n",
    "        \n",
    "args = argument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e6fb8d-85f1-4f3d-bce4-264e46592fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from enum import unique\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "import copy \n",
    "from sys import path\n",
    "import sys\n",
    "# from utils import Logger\n",
    "\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from huggingface_hub import Repository\n",
    "from transformers import (\n",
    "#    CONFIG_MAPPING,\n",
    "#    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "#from torch import AdamW\n",
    "from transformers.utils.versions import require_version\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import csv\n",
    "from scipy.stats import skewnorm\n",
    "from scipy.stats import kstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea23cd9-e66c-44e0-8e5e-043bf110767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./logs//canary_50_5_adapter_False_head_False_layer_None_ref_False_maxlen_1024_red_16_model_gpt2_lr_5e-05_epoch_5_trba_8_acc_8_evba8_data_ptb_text_only\n"
     ]
    }
   ],
   "source": [
    "random.seed(args.seed)\n",
    "\n",
    "folder_name = f\"canary_{str(args.canary_rep)}_{str(args.canary_len)}_adapter_{args.add_adapter}_head_{args.train_head_only}_layer_{args.train_layer_n_only}_ref_{args.do_ref_model}_maxlen_{args.block_size}_red_{args.adapter_reduction}_model_{args.model_name_or_path}_lr_{args.learning_rate}_epoch_{args.num_train_epochs}_trba_{args.per_device_train_batch_size}_acc_{args.gradient_accumulation_steps}_evba{args.per_device_eval_batch_size}_data_{args.dataset_name}\"\n",
    "\n",
    "directory = \"{}/{}\".format(args.output_dir,folder_name)\n",
    "print(directory)\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "# log_file = os.path.join(directory, \"stdout\")\n",
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# if accelerator.is_local_main_process:\n",
    "    # print(\"Logging to {}\".format(log_file))\n",
    "    # pass\n",
    "    \n",
    "# sys.stdout = Logger(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228e86a7-925d-4534-b72b-a1c020bba9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ptb_text_only (/home/todsavadt/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f)\n",
      "100%|██████████| 3/3 [00:00<00:00, 2107.34it/s]\n"
     ]
    }
   ],
   "source": [
    "if args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    if 'enron' in args.dataset_name:\n",
    "        raw_datasets = load_dataset('csv', data_files={'train': 'enron/data/cleaned_short_train_scrubbed.csv' ,'validation': 'enron/data/cleaned_short_test_scrubbed.csv'})\n",
    "        #raw_datasets['train'] = load_dataset('csv', data_files={'train': 'data/cleaned_train.csv' ,'validation': 'data/cleaned_test.csv'}, split='train[:4000]')\n",
    "        #raw_datasets['validation'] = load_dataset('csv', data_files={'train': 'data/cleaned_train.csv' ,'validation': 'data/cleaned_test.csv'}, split='train[4000:5000]')\n",
    "\n",
    "    else:\n",
    "        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n",
    "        if \"validation\" not in raw_datasets.keys():\n",
    "            raw_datasets[\"validation\"] = load_dataset(\n",
    "                args.dataset_name,\n",
    "                args.dataset_config_name,\n",
    "                split=f\"train[:{args.validation_split_percentage}%]\",\n",
    "            )\n",
    "            raw_datasets[\"train\"] = load_dataset(\n",
    "                args.dataset_name,\n",
    "                args.dataset_config_name,\n",
    "                split=f\"train[{args.validation_split_percentage}%:]\",\n",
    "            )\n",
    "else:\n",
    "        data_files = {}\n",
    "        dataset_args = {}\n",
    "        if args.train_file is not None:\n",
    "            data_files[\"train\"] = args.train_file\n",
    "        if args.validation_file is not None:\n",
    "            data_files[\"validation\"] = args.validation_file\n",
    "        extension = args.train_file.split(\".\")[-1]\n",
    "        if extension == \"txt\":\n",
    "            extension = \"text\"\n",
    "            dataset_args[\"keep_linebreaks\"] = not args.no_keep_linebreaks\n",
    "        raw_datasets = load_dataset(extension, data_files=data_files, **dataset_args)\n",
    "        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n",
    "        if \"validation\" not in raw_datasets.keys():\n",
    "            raw_datasets[\"validation\"] = load_dataset(\n",
    "                extension,\n",
    "                data_files=data_files,\n",
    "                split=f\"train[:{args.validation_split_percentage}%]\",\n",
    "                **dataset_args,\n",
    "            )\n",
    "            raw_datasets[\"train\"] = load_dataset(\n",
    "                extension,\n",
    "                data_files=data_files,\n",
    "                split=f\"train[{args.validation_split_percentage}%:]\",\n",
    "                **dataset_args,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1d7c9b4-d11a-4747-a088-2cbeb1ad8775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence'],\n",
       "        num_rows: 42068\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence'],\n",
       "        num_rows: 3761\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence'],\n",
       "        num_rows: 3370\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a3d185-0203-4dba-afbb-e9d4d3746214",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.config_name:\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "elif args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "#    else:\n",
    "#        config = CONFIG_MAPPING[args.model_type]()\n",
    "#        logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "if args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4b3b7a3",
   "metadata": {},
   "source": [
    "### Modify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e081fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if args.model_name_or_path:\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         args.model_name_or_path,\n",
    "#         from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "#         config=config,\n",
    "#     )\n",
    "# else:\n",
    "#     # logger.info(\"Training new model from scratch\")\n",
    "#     model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# # model_ref = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef5c4674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomGPT2HeadModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomGPT2HeadModel, self).__init__()\n",
    "        self.transformer = AutoModelForCausalLM.from_pretrained(\n",
    "                                args.model_name_or_path,\n",
    "                                from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "                                config=config,\n",
    "                            )\n",
    "        self.pv_embed    = nn.Embedding(2, config.n_embd)\n",
    "        self.alpha       = 0.8\n",
    "   \n",
    "    def forward(self, \n",
    "                input_ids, \n",
    "                private_ids=None, \n",
    "                attention_mask=None, \n",
    "                labels = None):\n",
    "        \n",
    "        # Get token embeddings from GPT-2\n",
    "        inputs_embeds = self.transformer.transformer.wte(input_ids) #bs,sq,hd\n",
    "        \n",
    "        if private_ids is not None:\n",
    "            # Get embeddings for additional tokens\n",
    "            pv_embeddings = self.pv_embed(private_ids)\n",
    "            # Combine token embeddings and extra embeddings\n",
    "            inputs_embeds = self.alpha * inputs_embeds + (1 - self.alpha) * pv_embeddings   \n",
    "        \n",
    "        # Pass through the rest of the GPT-2 model\n",
    "        transformer_outputs = self.transformer(\n",
    "            # input_ids = input_ids,\n",
    "            inputs_embeds = inputs_embeds, \n",
    "            attention_mask = attention_mask,\n",
    "            labels = labels)\n",
    "        \n",
    "        return transformer_outputs\n",
    "\n",
    "args.customgpt = True\n",
    "if args.customgpt:\n",
    "    # Example usage\n",
    "    # config = GPT2Config.from_pretrained('distilgpt2')\n",
    "    model = CustomGPT2HeadModel(config)\n",
    "    # model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f5a6f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-2.2289, -1.2440,  0.2134,  ...,  0.9973,  1.0327, -0.7752],\n",
       "        [-0.4784, -1.1843, -0.8551,  ..., -0.6168,  1.2452, -0.6804]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializae Weight\n",
    "model.pv_embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af09a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinaryEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryEmbedding, self).__init__()\n",
    "        self.embedding_dim = 768\n",
    "        \n",
    "        # Create a single embedding for the binary variable\n",
    "        self.embedding = nn.Embedding(2, self.embedding_dim)\n",
    "        \n",
    "    def forward(self, \n",
    "                private_ids=None, \n",
    "                input_ids=None, \n",
    "                attention_mask=None, \n",
    "                labels=None):\n",
    "        # Convert input indices to binary vectors (0 or 1)\n",
    "        binary_input = torch.where(private_ids == 0, torch.tensor(0), torch.tensor(1))\n",
    "        binary_label = torch.where(labels == 0, torch.tensor(0), torch.tensor(1))\n",
    "        \n",
    "        # Get embeddings for the binary vectors\n",
    "        binary_embeddings = self.embedding(binary_input)\n",
    "        binary_embeddings_label = self.embedding(binary_label)\n",
    "        return binary_embeddings, binary_embeddings_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de630f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5892, -1.0553,  0.9204,  ..., -0.4016, -0.2926, -0.5933],\n",
       "        [ 0.3032,  0.0427,  0.3495,  ..., -1.1115,  0.1880,  0.1024]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Weight Outside\n",
    "binary_embedding_model = BinaryEmbedding()\n",
    "binary_embedding_model.load_state_dict(torch.load('binary_embedding_model.pth'))\n",
    "\n",
    "#Initializae Weight\n",
    "model.pv_embed.weight = binary_embedding_model.embedding.weight\n",
    "model.pv_embed.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc068454-1674-4bbf-b211-13e72164a50e",
   "metadata": {},
   "source": [
    "### Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2a0b951-d074-4a94-99b8-d215a12c0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "NLP = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "MASK_TOKEN = \"<MASK>\"\n",
    "\n",
    "# can be found here, https://github.com/explosion/spaCy/blob/master/spacy/glossary.py\n",
    "ALL_TYPES = (\n",
    "    \"CARDINAL\",\n",
    "    \"DATE\",\n",
    "    \"EVENT\",\n",
    "    \"FAC\",\n",
    "    \"GPE\",\n",
    "    \"LANGUAGE\",\n",
    "    \"LAW\",\n",
    "    \"LOC\",\n",
    "    \"MONEY\",\n",
    "    \"NORP\",\n",
    "    \"ORDINAL\",\n",
    "    \"ORG\",\n",
    "    \"PERCENT\",\n",
    "    \"PERSON\",\n",
    "    \"PRODUCT\",\n",
    "    \"QUANTITY\",\n",
    "    \"TIME\",\n",
    "    \"WORK_OF_ART\",\n",
    ")\n",
    "\n",
    "SPECIAL_TOKENS_MAP = {\n",
    "    # dep parser\n",
    "    \"SUBJ\": \"<SUBJ>\",\n",
    "    \"OBJ\": \"<OBJ>\",\n",
    "    \"ROOT\": \"<ROOT>\",\n",
    "    # pos tagging\n",
    "    \"PROPN\": \"<PROPN>\",\n",
    "    \"PRON\": \"<PRON>\",\n",
    "    # SRL predicate\n",
    "    \"VERB\": \"<VERB>\",\n",
    "    \"MASK\": \"<MASK>\",\n",
    "}\n",
    "\n",
    "for ent_type_ in ALL_TYPES:\n",
    "    SPECIAL_TOKENS_MAP.update({ent_type_: f\"<{ent_type_.upper()}>\"})\n",
    "\n",
    "\n",
    "# len(ALL_TYPES)\n",
    "\n",
    "def get_spacy_tokens_and_doc(line):\n",
    "    doc = NLP(line)\n",
    "    spacy_tokens = [x.text for x in doc]\n",
    "    return spacy_tokens, doc\n",
    "    \n",
    "def get_special_tokens(special_token, use_single_mask_token=True):\n",
    "    use_single_mask_token = True if args.redact_token == 'single' else False\n",
    "    special_token = special_token.upper()\n",
    "    if use_single_mask_token:\n",
    "        return MASK_TOKEN\n",
    "    return SPECIAL_TOKENS_MAP[special_token]\n",
    "    \n",
    "def delex_line(line):\n",
    "    entity_types = ALL_TYPES\n",
    "    if line.endswith(\"\\n\"):\n",
    "        endswith_new_line = True\n",
    "        line = line[:-1]\n",
    "        assert not line.endswith(\"\\n\"), \"line still ends with \\n\"\n",
    "    else:\n",
    "        endswith_new_line = False\n",
    "    _, doc = get_spacy_tokens_and_doc(line.strip())\n",
    "    words = [tok.text for tok in doc]\n",
    "    spaces = [True if tok.whitespace_ else False for tok in doc]\n",
    "    \n",
    "    # print(spaces)\n",
    "    for i, x in enumerate(doc):\n",
    "        if x.ent_type_ in entity_types:\n",
    "            # named entity\n",
    "            words[i] = get_special_tokens(x.ent_type_)\n",
    "            need_to_add = True\n",
    "    total = len(doc)\n",
    "\n",
    "    # rejoin them\n",
    "    doc2 = spacy.tokens.doc.Doc(NLP.vocab, words=words, spaces=spaces)\n",
    "    return_text = doc2.text\n",
    "    if endswith_new_line:\n",
    "        return_text = return_text + \"\\n\"\n",
    "    return return_text\n",
    "\n",
    "def delex_line_digit(line):\n",
    "    entity_types = ALL_TYPES\n",
    "    if line.endswith(\"\\n\"):\n",
    "        endswith_new_line = True\n",
    "        line = line[:-1]\n",
    "        # assert not line.endswith(\"\\n\"), \"line still ends with \\n\"\n",
    "    else:\n",
    "        endswith_new_line = False\n",
    "    _, doc = get_spacy_tokens_and_doc(line.strip())\n",
    "    words = [tok.text for tok in doc]\n",
    "    # spaces = [True if tok.whitespace_ else False for tok in doc]\n",
    "    \n",
    "    # print(spaces)\n",
    "    for i, x in enumerate(doc):\n",
    "        if x.ent_type_ in entity_types:\n",
    "            # named entity\n",
    "            words[i] = 1 #get_special_tokens(x.ent_type_, use_single_mask_token=True)\n",
    "            need_to_add = True\n",
    "        else:\n",
    "            words[i] = 0\n",
    "    total = len(doc)\n",
    "\n",
    "    if endswith_new_line:\n",
    "        words.append(0)\n",
    "    return words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bb1754f-f23d-4054-81ff-9b7490c1861f",
   "metadata": {},
   "source": [
    "### Add canary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f210757c-c16e-4593-a7a1-ed8cc855f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_canary(canary_len,tokenizer):\n",
    "    raw_sample = random.choices([str(i) for i in range(10)], k=canary_len)\n",
    "    raw_sample = \" \".join(raw_sample)\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(raw_sample)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    assert len(ids) == canary_len\n",
    "    \n",
    "    raw_sample = \"the secret number is \" + raw_sample\n",
    "    toked =  tokenizer(raw_sample)\n",
    "    toked['private_ids'] = delex_line_digit(raw_sample)\n",
    "    toked['labels'] = toked['input_ids'].copy()\n",
    "    return raw_sample, toked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80e2dcc9-7e14-4fe9-9f5b-ae84c69067c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/todsavadt/.cache/huggingface/datasets/ptb_text_only/penn_treebank/1.1.0/8d1b97746fb9765d140e569ec5ddd35e20af4d37761f5e1bf357ea0b081f2c1f/cache-f9c26e8a66dc5ad2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before canary len  42068\n",
      "after canary len  42118\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "if args.add_canary:    \n",
    "    if 'ptb' in args.dataset_name:\n",
    "        dict_key = 'sentence'\n",
    "    else:\n",
    "        dict_key='text'\n",
    "    print(\"before canary len \", len(raw_datasets['train'][dict_key]))\n",
    "    canary, canary_ids = gen_canary(args.canary_len, tokenizer)\n",
    "    for j in range(args.canary_rep):\n",
    "        raw_datasets['train']=raw_datasets['train'].add_item({dict_key:canary})\n",
    "\n",
    "    raw_datasets['train'] = raw_datasets['train'].shuffle(seed=args.seed)\n",
    "    print(\"after canary len \", len(raw_datasets['train'][dict_key]))\n",
    "    # save the canaries in csv\n",
    "\n",
    "    file = open(f'./{directory}/canaries.txt', 'w+')\n",
    "    file.write(canary)\n",
    "    file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "    file = open(f'./{directory}/fitting_canaries.txt', 'w+')\n",
    "    \n",
    "    fitting_canaries_ids = []\n",
    "    for i in range(5000):\n",
    "        fit , fit_ids = gen_canary(args.canary_len,tokenizer)\n",
    "        if fit != canary:\n",
    "            fitting_canaries_ids.append(fit_ids)\n",
    "            file.write(fit)\n",
    "            file.write('\\n')\n",
    "    print(len(fitting_canaries_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b6eea08-5602-462c-aa85-03afb5acf6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# canary, delex_line(canary), delex_line_digit(canary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5774569-9e1e-4dea-ae40-920dcbcebe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_private_token = [delex_line_digit(line) for line in tqdm(raw_datasets['train']['sentence'])]\n",
    "# validation_private_token = [delex_line_digit(line) for line in tqdm(raw_datasets['validation']['sentence'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cea380aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_path = \"masker/train_private_token_ptb.pkl\"\n",
    "# with open(file_path, \"wb\") as file:\n",
    "#     pickle.dump(train_private_token, file)\n",
    "with open(file_path, \"rb\") as file:\n",
    "    train_private_token = pickle.load(file)\n",
    "\n",
    "file_path = \"masker/validation_private_token_ptb.pkl\"\n",
    "# with open(file_path, \"wb\") as file:\n",
    "#     pickle.dump(validation_private_token, file)\n",
    "with open(file_path, \"rb\") as file:\n",
    "    validation_private_token = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d61dfe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'private_ids'],\n",
       "        num_rows: 42118\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'private_ids'],\n",
       "        num_rows: 3370\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "# Create new dictionaries with both 'text' and 'labels' features\n",
    "new_train_dict = {\n",
    "    \"text\": raw_datasets[\"train\"][\"sentence\"],\n",
    "    \"private_ids\": train_private_token,\n",
    "}\n",
    "new_validation_dict = {\n",
    "    \"text\": raw_datasets[\"validation\"][\"sentence\"],\n",
    "    \"private_ids\": validation_private_token,\n",
    "}\n",
    "# new_test_dict = {\n",
    "#     \"text\": raw_datasets[\"test\"][\"text\"],\n",
    "#     # \"private_ids\": test_private_token,\n",
    "# }\n",
    "\n",
    "# Create a new DatasetDict with the additional 'labels' feature\n",
    "new_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict(new_train_dict),\n",
    "    \"validation\": Dataset.from_dict(new_validation_dict),\n",
    "    # \"test\": Dataset.from_dict(new_test_dict),\n",
    "})\n",
    "\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e0b69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dataset['train']['text'][3], new_dataset['train']['private_ids'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f48211e-63be-4df6-8d47-ddd15d413680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    }
   ],
   "source": [
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "# column_names = raw_datasets[\"train\"].column_names\n",
    "column_names = 'text'\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    tokenized_datasets = new_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "if args.block_size is None:\n",
    "    block_size = tokenizer.model_max_length\n",
    "    if block_size > 1024:\n",
    "        print(\n",
    "            f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "            \"Picking 1024 instead. You can change that default value by passing --block_size xxx.\"\n",
    "        )\n",
    "    block_size = 1024\n",
    "else:\n",
    "    if args.block_size > tokenizer.model_max_length:\n",
    "        print(\n",
    "            f\"The block_size passed ({args.block_size}) is larger than the maximum length for the model\"\n",
    "            f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    block_size = min(args.block_size, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d408893-a3e1-4f86-9d92-8eb1b1fa774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    }
   ],
   "source": [
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    \n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "    \n",
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n",
    "# for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n",
    "# to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    )\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1fe10ce-ed2f-45f7-b306-1439eff6144c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['private_ids', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 956\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['private_ids', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 76\n",
       " }))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "805bbcfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['private_ids', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 20\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['private_ids', 'input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 20\n",
       " }))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset = train_dataset.shuffle(seed=55).select(range(20))\n",
    "small_eval_dataset = eval_dataset.shuffle(seed=55).select(range(20))\n",
    "small_train_dataset, small_eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a56c2210-a1fd-43ad-a1fb-8d848ae82c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log a few random samples from the training set:\n",
    "#for index in random.sample(range(len(train_dataset)), 3):\n",
    "#    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "args.small_sample = False\n",
    "if args.small_sample:\n",
    "    # DataLoaders creation:\n",
    "    train_dataloader = DataLoader(\n",
    "        small_train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=args.per_device_train_batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        small_eval_dataset, collate_fn=default_data_collator, batch_size=args.per_device_eval_batch_size\n",
    "    )\n",
    "else:\n",
    "    # DataLoaders creation:\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=args.per_device_train_batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, collate_fn=default_data_collator, batch_size=args.per_device_eval_batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4df77395-4f40-4725-b4ff-23ced3adb360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['private_ids', 'input_ids', 'attention_mask', 'labels'])\n",
      "torch.Size([8, 1024]) torch.Size([8, 1024])\n",
      "torch.Size([8, 1024]) torch.Size([8, 1024])\n"
     ]
    }
   ],
   "source": [
    "#checking chucking\n",
    "for batch in train_dataloader:\n",
    "    print(batch.keys())\n",
    "    print(batch['input_ids'].shape, batch['labels'].shape)\n",
    "    break\n",
    "for batch in eval_dataloader:\n",
    "    print(batch['input_ids'].shape, batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccc342e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(**batch)\n",
    "# output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ed02977-57ca-4e5e-b271-9325322273f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_params (million) 124.441344\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#print(model.lm_head)    \n",
    "if accelerator.is_local_main_process:\n",
    "    print(\"model_params (million)\", count_parameters(model)/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6228401f-d3fd-49b9-9655-6402f158a207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todsavadt/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    " # Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55ce9ae6-a915-4acf-9941-f0762161f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "# model_ref = accelerator.prepare(\n",
    "#     model_ref\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89105595-6951-458d-aee8-adef6da56514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On TPU, the tie weights in our model have been disconnected, so we need to restore the ties.\n",
    "if accelerator.distributed_type == DistributedType.TPU:\n",
    "    model.tie_weights()\n",
    "\n",
    "# Note -> the training dataloader needs to be prepared before we grab his length below (cause its length will be\n",
    "# shorter in multiprocess)\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "else:\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.max_train_steps,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "999d5359-f968-45f4-86ed-92655d3779c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_params (million) 124.441344\n"
     ]
    }
   ],
   "source": [
    "if accelerator.is_local_main_process:\n",
    "    print(\"model_params (million)\", count_parameters(model)/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dae90292-236c-43a7-ba53-ad1404d454a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_exposure(canary_rank):\n",
    "    # return math.log(TOTAL_CANDIDATES, 2) - math.log(canary_rank, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "151d481e-5ad6-4d79-9049-3b1685729973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exposure(fitting, main):\n",
    "\n",
    "    fitting_params = skewnorm.fit(fitting)\n",
    "    ks = kstest(fitting, 'skewnorm', fitting_params)\n",
    "\n",
    "    cdf = skewnorm.cdf(main, fitting_params[0], fitting_params[1], fitting_params[2])\n",
    "\n",
    "    if cdf == 0.0:\n",
    "        exposure = 0.0\n",
    "    else:\n",
    "        exposure = -1.0*np.log2(cdf)\n",
    "\n",
    "    return exposure\n",
    "\n",
    "def get_fit_canary_loss(model,fitting_id, main_id):\n",
    "    loss_list = []\n",
    "    for k, v in main_id.items():\n",
    "            main_id[k] = torch.tensor(v).cuda()\n",
    "\n",
    "    input_ids = main_id['input_ids']\n",
    "    labels = main_id['labels']\n",
    "\n",
    "    loss_main = np.exp(model(input_ids=input_ids, labels=labels)['loss'].item())\n",
    "    # loss_main = np.exp(model(**main_id)['loss'].item())\n",
    "\n",
    "    for sample in fitting_id:\n",
    "        for k, v in sample.items():\n",
    "            sample[k] = torch.tensor(v).cuda()\n",
    "        \n",
    "        input_ids = sample['input_ids']\n",
    "        labels = sample['labels']\n",
    "        output = model(input_ids=input_ids, labels=labels)\n",
    "        # output = model(**sample)\n",
    "        loss_list.append(np.exp(output.loss.item()))\n",
    "\n",
    "    return loss_main, loss_list\n",
    "\n",
    "def gen_canary(canary_len, tokenizer):\n",
    "    raw_sample = random.choices([str(i) for i in range(10)], k=canary_len)\n",
    "    raw_sample = \" \".join(raw_sample)\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(raw_sample)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    assert len(ids) == canary_len\n",
    "    \n",
    "    raw_sample = \"the secret number is \" + raw_sample\n",
    "    toked =  tokenizer(raw_sample)\n",
    "    toked['labels'] = toked['input_ids'].copy()\n",
    "    return raw_sample, toked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32ca784c-6884-4371-9f67-7c55fcb6012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 956\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 75\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Running training *****\")\n",
    "print(f\"  Num examples = {len(train_dataset)}\")\n",
    "print(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "print(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "print(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "print(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "print(f\"  Total optimization steps = {args.max_train_steps}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54ce93c9",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15288c3f-bafa-4342-a6b8-7b30d8a9ade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [02:16<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running canary eval\n",
      "Exposure : 0.526439466187428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold is:  4.130061626434326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:50<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model! epoch 0: perplexity: 69.29668859480549\n",
      "perplexity : 69.29668859480549 perplexity train : 95.55007170612599\n",
      "training epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [02:31<00:00,  1.27s/it]\n",
      "/tmp/ipykernel_642580/723039732.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  main_id[k] = torch.tensor(v).cuda()\n",
      "/tmp/ipykernel_642580/723039732.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample[k] = torch.tensor(v).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running canary eval\n",
      "Exposure : 1.0320443736030875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold is:  3.751251697540283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:58<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model! epoch 1: perplexity: 48.35471051557912\n",
      "perplexity : 48.35471051557912 perplexity train : 67.92753575864621\n",
      "training epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [02:47<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running canary eval\n",
      "Exposure : 1.5779658730536872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold is:  3.6342451572418213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:48<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model! epoch 2: perplexity: 43.18082776484474\n",
      "perplexity : 43.18082776484474 perplexity train : 59.28919171217495\n",
      "training epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [02:23<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running canary eval\n",
      "Exposure : 1.9217698157831336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold is:  3.5870554447174072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:58<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model! epoch 3: perplexity: 41.17532795870406\n",
      "perplexity : 41.17532795870406 perplexity train : 55.72517364605271\n",
      "training epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 80/120 [01:53<00:56,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running canary eval\n",
      "Exposure : 2.0185164991475193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold is:  3.579505681991577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:35<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model! epoch 4: perplexity: 40.879598815067546\n",
      "perplexity : 40.879598815067546 perplexity train : 55.18099975405502\n"
     ]
    }
   ],
   "source": [
    "# args.add_canary = False\n",
    "# Only show the progress bar once on each machine.\n",
    "# progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "best_loss = 1000000\n",
    "best_val_perplexity = float(\"inf\")\n",
    "save_path = f'models/{model.__class__.__name__}_gpt2_ptb_pv.pt'\n",
    "for epoch in range(args.num_train_epochs):\n",
    "    ##################################################################\n",
    "    # Train\n",
    "    ##################################################################\n",
    "    model.train()\n",
    "    if accelerator.is_local_main_process:\n",
    "        print(f\"training epoch {epoch+1}\")\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        outputs = model(**batch)\n",
    "        # loss = outputs.loss\n",
    "        loss = outputs[0]\n",
    "        loss = loss / args.gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            # progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "            \n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break  \n",
    "            \n",
    "    ##################################################################\n",
    "    # Evaluation\n",
    "    ##################################################################\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    if args.add_canary:\n",
    "        print(\"running canary eval\")\n",
    "        canary_loss, fitting_loss = get_fit_canary_loss(model, fitting_canaries_ids, canary_ids)        \n",
    "        exposure = get_exposure(fitting_loss, canary_loss)\n",
    "        print('Exposure :', exposure)\n",
    "        \n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "        # loss = outputs.loss\n",
    "        loss = outputs[0]\n",
    "        losses.append(accelerator.gather(loss.repeat(args.per_device_eval_batch_size)))\n",
    "        \n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    sorted_loss = sorted(losses)\n",
    "    \n",
    "    threshold = sorted_loss[int(0.1*len(losses))]\n",
    "    if accelerator.is_local_main_process:\n",
    "        print(\"threshold is: \" , threshold.detach().item())\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    ################################################    \n",
    "    #run threshold on training samples\n",
    "    losses = []\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "        # loss = outputs.loss\n",
    "        loss = outputs[0]\n",
    "        losses.append(accelerator.gather(loss.repeat(args.per_device_train_batch_size)))\n",
    "          \n",
    "    accelerator.wait_for_everyone()\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(train_dataset)]\n",
    "    \n",
    "    try:\n",
    "        perplexity_train = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity_train = float(\"inf\")\n",
    "\n",
    "    if perplexity < best_val_perplexity and save_path is not None:\n",
    "        best_val_perplexity = perplexity\n",
    "        \n",
    "        print(f\"saved model! epoch {epoch}: perplexity: {best_val_perplexity}\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    print(f\"perplexity : {perplexity} perplexity train : {perplexity_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287bf921-13b9-4b5a-89cf-ead37e885639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
