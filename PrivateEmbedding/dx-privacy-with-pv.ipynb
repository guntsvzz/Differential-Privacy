{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2687d18-e96d-4860-ae8c-ce33c347def0",
   "metadata": {},
   "source": [
    "## GPT2 with dx-privacy preserving private embedding mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a67825-aa70-449c-9e21-cdbf9567fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class argument:\n",
    "    def __init__(self):\n",
    "        self.dataset_name = 'wikitext'\n",
    "        self.dataset_config_name = 'wikitext-2-raw-v1'\n",
    "        self.output_dir = './logs/' \n",
    "        self.seed = 1234\n",
    "        self.learning_rate = 5e-5\n",
    "        self.block_size = 1024 \n",
    "        self.do_ref_model = False\n",
    "        \n",
    "        self.config_name = None\n",
    "        self.model_name_or_path = 'gpt2'\n",
    "        self.tokenizer_name = 'gpt2'\n",
    "        self.use_slow_tokenizer = False\n",
    "        \n",
    "        self.per_device_train_batch_size = 8\n",
    "        self.per_device_eval_batch_size = 8\n",
    "        self.gradient_accumulation_steps = 8\n",
    "        \n",
    "        self.do_ref_model = False\n",
    "        self.lr_scheduler_type = 'linear'\n",
    "\n",
    "        self.num_train_epochs = 5\n",
    "        self.max_train_steps = None\n",
    "\n",
    "        self.preprocessing_num_workers = 1\n",
    "        self.overwrite_cache = False\n",
    "        self.weight_decay = 0.0\n",
    "        self.num_warmup_steps = 0\n",
    "        \n",
    "        self.add_canary = True\n",
    "        self.canary_rep = 50\n",
    "        self.canary_len = 5\n",
    "        \n",
    "        self.add_adapter = False\n",
    "        self.adapter_reduction = 16\n",
    "        self.train_head_only = False\n",
    "        self.train_layer_n_only = None \n",
    "        self.redact_token = 'multi'\n",
    "         \n",
    "args = argument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac644e6-976c-418a-82ab-972db600b54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, GPT2Config, AutoTokenizer\n",
    "class CustomGPT2HeadModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomGPT2HeadModel, self).__init__()\n",
    "        self.transformer = AutoModelForCausalLM.from_pretrained(\n",
    "                                args.model_name_or_path,\n",
    "                                # output_hidden_states=True,\n",
    "                                from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "                                config=config,\n",
    "                            )\n",
    "        self.pv_embed    = nn.Embedding(2, config.n_embd)\n",
    "        self.alpha       = 0.7\n",
    "   \n",
    "    def forward(self, \n",
    "                input_ids = None, \n",
    "                inputs_embeds = None,\n",
    "                private_ids=None, \n",
    "                attention_mask=None, \n",
    "                labels = None):\n",
    "                    \n",
    "        # if inputs_embeds is not None:\n",
    "        #     inputs_embeds = inputs_embeds\n",
    "        # else:\n",
    "            # Get token embeddings from GPT-2\n",
    "        inputs_embeds = self.transformer.transformer.wte(input_ids) #bs,sq,hd\n",
    "        \n",
    "        if private_ids is not None:\n",
    "            # Get embeddings for additional tokens\n",
    "            pv_embeddings = self.pv_embed(private_ids)\n",
    "            # Combine token embeddings and extra embeddings\n",
    "            inputs_embeds = self.alpha * inputs_embeds + (1 - self.alpha) * pv_embeddings   \n",
    "        \n",
    "        # Pass through the rest of the GPT-2 model\n",
    "        transformer_outputs = self.transformer(\n",
    "            # input_ids = input_ids,\n",
    "            inputs_embeds = inputs_embeds, \n",
    "            attention_mask = attention_mask,\n",
    "            labels = labels,\n",
    "            output_hidden_states = True,\n",
    "            output_attentions = True\n",
    "            )\n",
    "        \n",
    "        return transformer_outputs\n",
    "\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "model = CustomGPT2HeadModel(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=not False)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#Private Wikitext\n",
    "save_path = f'models/{model.__class__.__name__}_gpt2_wikitext_pv.pt'\n",
    "state_dict = torch.load(save_path)\n",
    "model.load_state_dict(state_dict)\n",
    "# model = model.to(devcie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2076bee1-6ef8-46ac-a8ed-a5b9fd6ae434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "NLP = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "MASK_TOKEN = \"<MASK>\"\n",
    "\n",
    "# can be found here, https://github.com/explosion/spaCy/blob/master/spacy/glossary.py\n",
    "ALL_TYPES = (\n",
    "    \"CARDINAL\",\n",
    "    \"DATE\",\n",
    "    \"EVENT\",\n",
    "    \"FAC\",\n",
    "    \"GPE\",\n",
    "    \"LANGUAGE\",\n",
    "    \"LAW\",\n",
    "    \"LOC\",\n",
    "    \"MONEY\",\n",
    "    \"NORP\",\n",
    "    \"ORDINAL\",\n",
    "    \"ORG\",\n",
    "    \"PERCENT\",\n",
    "    \"PERSON\",\n",
    "    \"PRODUCT\",\n",
    "    \"QUANTITY\",\n",
    "    \"TIME\",\n",
    "    \"WORK_OF_ART\",\n",
    ")\n",
    "\n",
    "SPECIAL_TOKENS_MAP = {\n",
    "    # dep parser\n",
    "    \"SUBJ\": \"<SUBJ>\",\n",
    "    \"OBJ\": \"<OBJ>\",\n",
    "    \"ROOT\": \"<ROOT>\",\n",
    "    # pos tagging\n",
    "    \"PROPN\": \"<PROPN>\",\n",
    "    \"PRON\": \"<PRON>\",\n",
    "    # SRL predicate\n",
    "    \"VERB\": \"<VERB>\",\n",
    "    \"MASK\": \"<MASK>\",\n",
    "}\n",
    "\n",
    "for ent_type_ in ALL_TYPES:\n",
    "    SPECIAL_TOKENS_MAP.update({ent_type_: f\"<{ent_type_.upper()}>\"})\n",
    "\n",
    "\n",
    "# len(ALL_TYPES)\n",
    "\n",
    "def get_spacy_tokens_and_doc(line):\n",
    "    doc = NLP(line)\n",
    "    spacy_tokens = [x.text for x in doc]\n",
    "    return spacy_tokens, doc\n",
    "    \n",
    "def get_special_tokens(special_token, use_single_mask_token=True):\n",
    "    use_single_mask_token = True if args.redact_token == 'single' else False\n",
    "    special_token = special_token.upper()\n",
    "    if use_single_mask_token:\n",
    "        return MASK_TOKEN\n",
    "    return SPECIAL_TOKENS_MAP[special_token]\n",
    "    \n",
    "def delex_line(line):\n",
    "    entity_types = ALL_TYPES\n",
    "    if line.endswith(\"\\n\"):\n",
    "        endswith_new_line = True\n",
    "        line = line[:-1]\n",
    "        assert not line.endswith(\"\\n\"), \"line still ends with \\n\"\n",
    "    else:\n",
    "        endswith_new_line = False\n",
    "    _, doc = get_spacy_tokens_and_doc(line.strip())\n",
    "    words = [tok.text for tok in doc]\n",
    "    spaces = [True if tok.whitespace_ else False for tok in doc]\n",
    "    \n",
    "    # print(spaces)\n",
    "    for i, x in enumerate(doc):\n",
    "        if x.ent_type_ in entity_types:\n",
    "            # named entity\n",
    "            words[i] = get_special_tokens(x.ent_type_)\n",
    "            need_to_add = True\n",
    "    total = len(doc)\n",
    "\n",
    "    # rejoin them\n",
    "    doc2 = spacy.tokens.doc.Doc(NLP.vocab, words=words, spaces=spaces)\n",
    "    return_text = doc2.text\n",
    "    if endswith_new_line:\n",
    "        return_text = return_text + \"\\n\"\n",
    "    return return_text\n",
    "\n",
    "def delex_line_digit(line):\n",
    "    entity_types = ALL_TYPES\n",
    "    if line.endswith(\"\\n\"):\n",
    "        endswith_new_line = True\n",
    "        line = line[:-1]\n",
    "        assert not line.endswith(\"\\n\"), \"line still ends with \\n\"\n",
    "    else:\n",
    "        endswith_new_line = False\n",
    "    _, doc = get_spacy_tokens_and_doc(line.strip())\n",
    "    words = [tok.text for tok in doc]\n",
    "    # spaces = [True if tok.whitespace_ else False for tok in doc]\n",
    "    \n",
    "    # print(spaces)\n",
    "    for i, x in enumerate(doc):\n",
    "        if x.ent_type_ in entity_types:\n",
    "            # named entity\n",
    "            words[i] = 1 #get_special_tokens(x.ent_type_, use_single_mask_token=True)\n",
    "            need_to_add = True\n",
    "        else:\n",
    "            words[i] = 0\n",
    "    total = len(doc)\n",
    "\n",
    "    if endswith_new_line:\n",
    "        words.append(0)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0466f02d-2881-4bd9-accc-9d235d8e1716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# from transformers import AutoTokenizer, GPT2Config, AutoModelForCausalLM\n",
    "# import numpy as np\n",
    "\n",
    "# config = GPT2Config.from_pretrained('gpt2')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=not False)\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# model = AutoModelForCausalLM.from_pretrained('gpt2', from_tf=bool(\".ckpt\" in 'gpt2'), config=config)\n",
    "\n",
    "# word = \"my secret number is 9 4 0 9 9 5\"  # Replace with your target word\n",
    "word = 'Charlie want to sell marijuana with his friend at Boston within two year'\n",
    "len(word.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "203a889d-c445-44bf-96ba-a73bfa38bd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[37136,   765,   284,  3677,  5727,   351,   465,  1545,   379,  6182,\n",
       "           1626,   734,   614]]),\n",
       " torch.Size([1, 13]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_inputs = tokenizer(word, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "tokenize_inputs['input_ids'], tokenize_inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e22ed39-ebcf-446c-8aa7-cd5601b1adb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1]), 13)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private_ids  = delex_line_digit(word)\n",
    "private_ids  = torch.tensor(private_ids) \n",
    "private_ids, len(private_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b346e10d-d8f6-4f08-933a-2294199a0191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marijuana\n"
     ]
    }
   ],
   "source": [
    "#Privacy Preserving Mechanism\n",
    "predicted_word_list = []\n",
    "alpha = 0.55\n",
    "for i, pv in zip(word.split(), private_ids):\n",
    "    # Step 1: Retrieve the vector representation of the word\n",
    "    inputs = tokenizer(i, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        # outputs = model(**inputs, output_hidden_states=True)\n",
    "        outputs = model.transformer.get_input_embeddings()(inputs['input_ids'][0]).squeeze()\n",
    "        pv_embeddings = model.pv_embed(pv).numpy()\n",
    "    # print(outputs.shape)\n",
    "    vector_representation = outputs.numpy() #.hidden_states[-1].mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    # # Step 2: Perturb the vector representation with noise sampled from a multivariate distribution\n",
    "    # mean = np.zeros(vector_representation.shape)  # Mean of the multivariate distribution\n",
    "    # covariance = np.eye(vector_representation.shape[0])  # Covariance matrix of the distribution\n",
    "    # noise = np.random.multivariate_normal(mean, covariance, size=1)\n",
    "    # noisy_representation = vector_representation + noise\n",
    "\n",
    "    # Step 2: Perturb the vector representation with noise sampled from a normal distribution\n",
    "    # mean = 0.0  # Mean of the normal distribution\n",
    "    # std_dev = 0.6  # Standard deviation of the normal distribution\n",
    "    # noise = np.random.normal(mean, std_dev, size=vector_representation.shape)\n",
    "    # noisy_representation = vector_representation + noise\n",
    "\n",
    "    # Step 2: Perturb the vector representation with private embedding\n",
    "    noisy_representation = alpha * vector_representation + (1 - alpha) * pv_embeddings\n",
    "    # noisy_representation = vector_representation + pv_embeddings\n",
    "    \n",
    "    # Step 3: Project the noisy representation of the word back to the discrete vocabulary space\n",
    "    # Find the token that is closest in embedding space to the noisy representation\n",
    "    embedding_weights = model.transformer.transformer.wte.weight.data.numpy()\n",
    "    # print(noisy_representation.shape)\n",
    "    # break\n",
    "    if noisy_representation.shape != (768,):\n",
    "        print(i)\n",
    "        for noisy in noisy_representation:\n",
    "            distances = np.linalg.norm(embedding_weights - noisy, axis=1)\n",
    "            closest_token_id = np.argmin(distances)\n",
    "    else:\n",
    "        distances = np.linalg.norm(embedding_weights - noisy_representation, axis=1)\n",
    "        closest_token_id = np.argmin(distances)\n",
    "\n",
    "    # Convert the token ID back to the word\n",
    "    predicted_word = tokenizer.decode([closest_token_id])\n",
    "    # print(i, predicted_word)\n",
    "    predicted_word_list.append(predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a26cfb8e-8caf-4bdd-be43-1b54ac5a1a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charlie Charlie\n",
      "want want\n",
      "to To\n",
      "sell  learn\n",
      "marijuana Three\n",
      "with with\n",
      "his His\n",
      "friend friend\n",
      "at at\n",
      "Boston Boston\n",
      "within within\n",
      "two Three\n",
      "year Three\n"
     ]
    }
   ],
   "source": [
    "for ori, per in zip(word.split(), predicted_word_list):\n",
    "    print(ori, per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adde938-e8cc-44c8-87cb-2c04ac4ffc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
