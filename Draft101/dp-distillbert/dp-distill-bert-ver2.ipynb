{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f78d7e-6cba-4581-ba21-396b891bcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment this if you are not using AIT proxy...\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0dcf49d-f391-4d68-8fb2-55c20993eca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import utils\n",
    "# Set the random seed for reproducible experiments\n",
    "random.seed(230)\n",
    "torch.manual_seed(230)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71123a62-9d7d-4c7a-9dc0-a143831e62a2",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f6a23c-313f-42e8-b6f9-299b216bfee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/todsavadt/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 886.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "task_name = \"sst2\"\n",
    "datasets = load_dataset(\"glue\",task_name)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86804cb-74f6-4af1-b653-2cffe678489b",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2143274-5d4e-4162-a9bd-21a5f51ab34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student = \"distilroberta-base\"\n",
    "# teacher = \"textattack/roberta-base-SST-2\"\n",
    "teacher = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72428bba-c315-403a-8b9c-8ece0ebd58a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9fe74fa-f7b6-4cc4-9fb9-b877dab17711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels\n",
    "if task_name is not None:\n",
    "    is_regression = task_name == \"stsb\"\n",
    "    if not is_regression:\n",
    "        label_list = datasets[\"train\"].features[\"label\"].names\n",
    "        num_labels = len(label_list)\n",
    "    else:\n",
    "        num_labels = 1\n",
    "else:\n",
    "    # Trying to have good defaults here, don't hesitate to tweak to your needs.\n",
    "    is_regression = datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "    if is_regression:\n",
    "        num_labels = 1\n",
    "    else:\n",
    "        # A useful fast method:\n",
    "        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "        label_list = datasets[\"train\"].unique(\"label\")\n",
    "        label_list.sort()  # Let's sort it for determinism\n",
    "        num_labels = len(label_list)\n",
    "        \n",
    "num_labels, is_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e138b11-ae14-4f2d-8b2f-ddf26adc8997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, PretrainedConfig\n",
    "model_name_or_path = teacher\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path, \n",
    "    num_labels=num_labels, \n",
    "    finetuning_task=task_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "    config=config,\n",
    ") #student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16dd2573-875b-4f33-8ee0-145c20d02fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/todsavadt/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-5781cc3dd74778e3.arrow\n",
      "Loading cached processed dataset at /home/todsavadt/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-659e0c7a546f9f13.arrow\n",
      "Loading cached processed dataset at /home/todsavadt/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-be37efac708702ce.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_id = None\n",
    "\n",
    "if (\n",
    "    model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n",
    "    and task_name is not None\n",
    "    and not is_regression\n",
    "):\n",
    "    # Some have all caps in their config, some don't.\n",
    "    label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
    "    if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):\n",
    "        label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
    "        \n",
    "elif task_name is None and not is_regression:\n",
    "    label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "    \n",
    "def tokenize_function(examples):\n",
    "    sentence1_key, sentence2_key = task_to_keys[task_name]\n",
    "    args = (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    result = tokenizer(*args, max_length=180, padding=\"max_length\", truncation=True)\n",
    "    if \"label\" in examples:\n",
    "        if label_to_id is not None:\n",
    "            # Map labels to IDs (not necessary for GLUE tasks)\n",
    "            result[\"label\"] = [label_to_id[l] for l in examples[\"label\"]]\n",
    "        else:\n",
    "            # In all cases, rename the column to labels because the model will expect that.\n",
    "            result[\"label\"] = examples[\"label\"]\n",
    "    \n",
    "    return result\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "325561b4-c5d7-442c-a86a-416eca81f956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use list comprehension to extract non-None elements from the tuple\n",
    "elements = [element for element in task_to_keys[task_name] if element is not None]\n",
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8539923-b0fd-481a-8368-1844f0cca755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(elements + [\"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6453ca4-ce3e-4c0d-a00b-5d40149aa8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/todsavadt/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3ae950a72f8425ee.arrow\n",
      "Loading cached shuffled indices for dataset at /home/todsavadt/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-dd98374289929b4f.arrow\n",
      "Loading cached shuffled indices for dataset at /home/todsavadt/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-8b5e3fdf526e7c58.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=55) #.select(range(10000))\n",
    "small_eval_dataset = tokenized_datasets[\"validation_matched\" if task_name == \"mnli\" else \"validation\"].shuffle(seed=55)\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a226a185-81be-4739-8ff3-3c023666a9be",
   "metadata": {},
   "source": [
    "## 3. Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a8b307-358d-46e2-a7ed-43a2d4e5bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=per_device_train_batch_size)\n",
    "val_dataloader = DataLoader(small_eval_dataset, batch_size=per_device_eval_batch_size)\n",
    "test_dataloader = DataLoader(small_test_dataset, batch_size=per_device_eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132e82f-eb15-40b9-b054-74c80307fdb7",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "690800f2-e6cb-4275-941b-af67554f943c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# teacher model\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher,\n",
    "    config=config,\n",
    ")\n",
    "teacher_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92d1ae26-7df2-4c50-805c-44faa5c38f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "\n",
    "# Create a configuration for a 6-layer BERT model\n",
    "config = BertConfig(\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=6,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    ")\n",
    "\n",
    "# Instantiate a 6-layer BERT model\n",
    "student_model = BertForSequenceClassification(config)\n",
    "\n",
    "# Print the student model architecture\n",
    "print(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dafcf17-0318-468a-ab4b-7307794b70f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # student model\n",
    "# # student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "# #     student,\n",
    "# #     config=config,\n",
    "# # )\n",
    "\n",
    "# class StudentModel(nn.Module):\n",
    "#     def __init__(self, teacher_model, num_classes):\n",
    "#         super(StudentModel, self).__init__()\n",
    "#         self.student_layers = nn.ModuleList([nn.Identity() if i % 2 == 0 else layer for i, layer in enumerate(teacher_model.bert.encoder.layer)])\n",
    "#         self.fc = nn.Linear(768, num_classes)  # Modify num_classes accordingly\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
    "#         x = teacher_model.bert.embeddings(input_ids, token_type_ids, attention_mask)\n",
    "#         for layer in self.student_layers:\n",
    "#             x = layer(x)\n",
    "#         x = self.fc(x[:, 0, :])  # Assuming you want to classify sentence-level tasks\n",
    "\n",
    "#         if labels is not None:\n",
    "#             loss_fct = nn.CrossEntropyLoss()\n",
    "#             loss = loss_fct(x.view(-1, self.num_labels), labels.view(-1))\n",
    "#             return loss\n",
    "#         else:\n",
    "#             return x\n",
    "\n",
    "# num_classes = 2  # Modify according to your classification task\n",
    "# student_model = StudentModel(teacher_model, num_classes)\n",
    "\n",
    "# student_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a5b4a-5f0b-4da6-9741-be4d8bcf0771",
   "metadata": {},
   "source": [
    "*Note teach model and student model still have same layers*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4be448-249d-4085-9236-25bf7e240eac",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb55a60-46cf-47b3-ba16-2670f305e215",
   "metadata": {},
   "source": [
    "![Differentially Private Knowledge Distillation (DPKD)\r",
    "](images/DPKD.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c34ef0a-67cb-4e2c-af94-1b5bde9b2199",
   "metadata": {},
   "source": [
    "## Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bb93a30-7bed-489b-a89b-b8f08745ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Define optimizer\n",
    "teacher_optimizer = AdamW(teacher_model.parameters(), lr=5e-5)\n",
    "student_optimizer = AdamW(student_model.parameters(), lr=6e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107786e-fd5e-41bc-aad9-d00463382598",
   "metadata": {},
   "source": [
    "## Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7b86fbd-090e-4004-aff9-751cebb77da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# teacher_model, teacher_optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "#     student_model, teacher_optimizer, train_dataloader, val_dataloader\n",
    "# )\n",
    "teacher_model = accelerator.prepare(teacher_model)\n",
    "student_model, student_optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "    student_model, student_optimizer, train_dataloader, val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d958e56-2a26-408f-9540-8f078bcae514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "import math\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "num_train_epochs = 3\n",
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "teacher_lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=teacher_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_train_steps,\n",
    ")\n",
    "\n",
    "student_lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=student_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_train_steps,\n",
    ")\n",
    "\n",
    "total_batch_size = (\n",
    "        per_device_train_batch_size\n",
    "        * accelerator.num_processes\n",
    "        * gradient_accumulation_steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b78e14-0973-4011-bce1-5e19c87b1645",
   "metadata": {},
   "source": [
    "### Private Tranformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75fb04eb-f52d-4c20-8b6c-74f71b9c19a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ml_swissknife\n",
    "# !pip install opt_einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dec39c4-4d19-4bad-a297-4cec2cb55849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrivacyEngine(\n",
       "  target_epsilon=3.000000, \n",
       "  target_delta=0.000005, \n",
       "  noise_multiplier=0.565601, \n",
       "  effective_noise_multiplier=0.035350, \n",
       "  epochs=16, \n",
       "  max_grad_norm=0.1, \n",
       "  sample_rate=0.00023756848654026043, \n",
       "  batch_size=16, \n",
       "  accounting_mode=rdp, \n",
       "  clipping_mode=ghost\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers, torch\n",
    "from private_transformers import PrivacyEngine\n",
    "dp = True\n",
    "if dp == True:\n",
    "    #Student Model\n",
    "    privacy_engine = PrivacyEngine(\n",
    "        student_model,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "        sample_size=len(datasets['train']),\n",
    "        epochs=per_device_train_batch_size,\n",
    "        max_grad_norm=0.1,\n",
    "        target_epsilon=3,\n",
    "        clipping_mode=\"ghost\",  # The only change you need to make!\n",
    "    )\n",
    "    privacy_engine.attach(student_optimizer)\n",
    "    #Teacher Model\n",
    "    privacy_engine = PrivacyEngine(\n",
    "        teacher_model,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "        sample_size=len(datasets['train']),\n",
    "        epochs=per_device_train_batch_size,\n",
    "        max_grad_norm=0.1,\n",
    "        target_epsilon=3,\n",
    "        clipping_mode=\"ghost\",  # The only change you need to make!\n",
    "    )\n",
    "    privacy_engine.attach(teacher_optimizer)\n",
    "else :\n",
    "    privacy_engine = None\n",
    "\n",
    "privacy_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0edd424-bb63-4166-bd09-ad1ab5b792bc",
   "metadata": {},
   "source": [
    "### Loss Objective \n",
    "The Kullback-Leibler divergence loss. For tensors of the same shape $y_{pred}, y_{true}$ where $y_{pred}$ is the input and $y_{true}$ ​ is the target, we define the pointwise KL-divergence as \n",
    "\n",
    "$$L(y_{pred}, y_{true}) = y_{pred}\\cdot \\log \\frac{y_{true}}{y_{pred}}  = y_{true} \\cdot (\\log y_{true} -\\log y_{true})$$\n",
    "\n",
    "format : torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)\n",
    "more infomation click [link](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6819a212-9479-4523-8fb0-d987e99cd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(student_outputs, labels, teacher_outputs, alpha = 0.9, T = 1):\n",
    "    \"\"\"\n",
    "    Compute the knowledge-distillation (KD) loss given outputs, labels.\n",
    "    \"Hyperparameters\": temperature and alpha\n",
    "\n",
    "    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher\n",
    "    and student expects the input tensor to be log probabilities! See Issue #2\n",
    "    \"\"\"\n",
    "    # student_outputs.logits.shape = (batch_size, class)\n",
    "    # teacher_outputs.logits.shape = (batch_size, class)\n",
    "    # labels.shape = (batch_size, )\n",
    "    \n",
    "    loss_fn = nn.KLDivLoss(reduction=\"none\")\n",
    "    kd_loss = loss_fn(F.log_softmax(student_outputs.logits/T, dim=1),\n",
    "                             F.softmax(teacher_outputs.logits/T, dim=1) * (T ** 2)) #(batch_size, labels)\n",
    "    \n",
    "    kd_loss = kd_loss.mean(dim=1) #(batch_size, )\n",
    "    CELoss = F.cross_entropy(student_outputs.logits, labels, reduction=\"none\") #.mean(dim=1) #(batch_size, )\n",
    "    total_losses = alpha * CELoss + (1. - alpha) * kd_loss\n",
    "\n",
    "    # total_losses = CELoss + alpha * kd_loss\n",
    "    return total_losses.mean(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201df04e-462e-4023-aa24-5134c4ad08ac",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd7fb233-a506-4c3e-9521-653172fe573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy, given the outputs and labels for all images.\n",
    "\n",
    "    Args:\n",
    "        outputs: (np.ndarray) output of the model\n",
    "        labels: (np.ndarray) [0, 1, ..., num_classes-1]\n",
    "\n",
    "    Returns: (float) accuracy in [0,1]\n",
    "    \"\"\"\n",
    "    # outputs = np.argmax(outputs, axis=1)\n",
    "    return np.sum(outputs==labels)/float(labels.size)\n",
    "\n",
    "\n",
    "# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    # could add more metrics such as accuracy for each token type\n",
    "}\n",
    "\n",
    "class RunningAverage():\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "    \n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "    \n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.total/float(self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fcc7aa6-451f-4c72-b7e4-eca1ced883c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 180]) torch.Size([16])\n",
      "torch.Size([16, 180]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "#checking chucking\n",
    "for i in train_dataloader:\n",
    "    print(i['input_ids'].shape, i['labels'].shape)\n",
    "    break\n",
    "for i in val_dataloader:\n",
    "    print(i['input_ids'].shape, i['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bd8b6dd-6405-4f58-9c0a-98edb7c61dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining train_kd & train_and_evaluate_kd functions\n",
    "def train_kd(student_model, teacher_model, optimizer, loss_fn_kd, train_dataloader, metrics):\n",
    "    # set model to training mode\n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "    \n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = RunningAverage()\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        # batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # compute model output, fetch teacher output, and compute KD loss\n",
    "        output_batch = student_model(**batch) # output = loss, logits, hidden_states, attentions\n",
    "        labels_batch = batch['labels']\n",
    "        \n",
    "        # get one batch output from teacher_outputs list\n",
    "        with torch.no_grad():\n",
    "            output_teacher_batch = teacher_model(**batch)\n",
    "\n",
    "        loss = loss_fn_kd(output_batch, labels_batch, output_teacher_batch)\n",
    "        loss = loss.reshape(-1)\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # accelerator.backward(loss)\n",
    "        \n",
    "        # This step is different from existing workflows: \n",
    "        # Don't call `loss.backward`; leave it to `optimizer.step` to handle backward.\n",
    "        # performs updates using calculated gradients\n",
    "        # `loss` is a 1-D tensor of shape (batch_size,).\n",
    "        optimizer.step(loss=loss)\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            # extract data from torch, move to cpu, convert to numpy arrays            \n",
    "            output_batch = output_batch.logits.argmax(dim=-1).cpu().numpy()\n",
    "            labels_batch = labels_batch.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.item()\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.item())\n",
    "        \n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]}\n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd82c856-03a6-44bf-976b-476009adc800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kd(model, eval_dataloader, metrics):\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    # summary for current eval loop\n",
    "    summ = []\n",
    "\n",
    "    # compute metrics over the dataset\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        \n",
    "        # compute model output\n",
    "        output_batch = model(**batch)\n",
    "        labels_batch = batch['labels']\n",
    "        \n",
    "        loss = 0.0  #force validation loss to zero to reduce computation time\n",
    "\n",
    "        # extract data from torch, move to cpu, convert to numpy arrays\n",
    "        output_batch = output_batch.logits.argmax(dim=-1).cpu().numpy()\n",
    "        labels_batch = labels_batch.cpu().numpy()\n",
    "\n",
    "        # compute all metrics on this batch\n",
    "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
    "                         for metric in metrics}\n",
    "        # summary_batch['loss'] = loss.item()\n",
    "        summary_batch['loss'] = loss\n",
    "        summ.append(summary_batch)\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    print(\"- Eval metrics : \" + metrics_string)\n",
    "    return metrics_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9627fc09-d25d-41ae-a93b-66dfb40e4135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = './experiments/distill'\n",
    "os.path.exists(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa1143b5-df9c-4298-8d5b-4d54c463f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_kd(student_model, teacher_model, train_dataloader, val_dataloader, \n",
    "                          optimizer, loss_fn_kd, metrics, model_dir, restore_file=None):\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(num_train_epochs):\n",
    "        student_lr_scheduler.step()\n",
    "        \n",
    "         # compute number of batches in one epoch (one full pass over the training set)\n",
    "        train_kd(student_model, teacher_model, optimizer, loss_fn_kd, train_dataloader,\n",
    "                 metrics)\n",
    "\n",
    "        # Evaluate for one epoch on validation set\n",
    "        val_metrics = evaluate_kd(student_model, val_dataloader, metrics)\n",
    "\n",
    "        val_acc = val_metrics['accuracy']\n",
    "        is_best = val_acc>=best_val_acc\n",
    "        \n",
    "        # Save weights\n",
    "        utils.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()},\n",
    "                               is_best=is_best,\n",
    "                               checkpoint=model_dir)\n",
    "        print(f'epoch: {epoch + 1}')\n",
    "        # If best_eval, best_save_path\n",
    "        if is_best:\n",
    "            print(\"- Found new best accuracy\")\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        utils.save_dict_to_json(val_metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "362ef57a-6203-414e-ab3d-7a4579218dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4210 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "100%|██████████| 4210/4210 [11:45<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics : accuracy: 0.703 ; loss: 0.000\n",
      "Checkpoint Directory exists! \n",
      "epoch: 2\n",
      "- Found new best accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 2318/4210 [06:29<05:20,  5.90it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 4210/4210 [11:46<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Eval metrics : accuracy: 0.777 ; loss: 0.000\n",
      "Checkpoint Directory exists! \n",
      "epoch: 3\n",
      "- Found new best accuracy\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_kd(student_model, teacher_model, train_dataloader, val_dataloader, student_optimizer,\n",
    "                       loss_fn_kd, metrics, model_dir, restore_file=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf25003-39e7-42b8-bbb5-48187de656e4",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2f82373-94ff-497c-bc2a-49d32ff0445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import evaluate\n",
    "# import torch\n",
    "\n",
    "# def testing(model, dataloader):\n",
    "#     metric = evaluate.load(\"glue\", \"sst2\")\n",
    "#     model.eval()\n",
    "#     for batch in dataloader:\n",
    "#         # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**batch)\n",
    "        \n",
    "#         loss        = outputs.loss\n",
    "#         logits      = outputs.logits\n",
    "#         predictions = torch.argmax(logits, dim=-1)\n",
    "#         metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        \n",
    "#     return metric.compute(), float(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9541203-276b-4988-bf79-5124475a423b",
   "metadata": {},
   "source": [
    "### Teacher Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f02ddf85-5102-4dc3-9c5c-445ac1612f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_checkpoint = 'experiments/best.pth.tar'\n",
    "# utils.load_checkpoint(teacher_checkpoint, teacher_model)\n",
    "# metric, loss = testing(teacher_model, test_dataloader)\n",
    "# metric, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf380593-7552-45a5-a8bc-eaddb946128a",
   "metadata": {},
   "source": [
    "### Student Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85c02a2c-80ee-4780-b00b-93ce3fc505fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric, loss = testing(student_model, test_dataloader)\n",
    "# metric, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6a8c5-1de3-43d1-8692-2738f35f85cf",
   "metadata": {},
   "source": [
    "## reference : \n",
    "1. https://github.com/haitongli/knowledge-distillation-pytorch/blob/master/train.py\n",
    "2. https://github.com/lxuechen/private-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891cf8c-440d-4f89-a385-c11a03270fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
