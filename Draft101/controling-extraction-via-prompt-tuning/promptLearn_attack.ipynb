{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import uuid\n",
    "import numpy as np\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "# import argparse\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import my_utils as ut\n",
    "from sklearn.model_selection import train_test_split\n",
    "from accelerate.utils import broadcast\n",
    "import logging\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class argment():\n",
    "    def __init__(self):\n",
    "        self.num_beams = 1\n",
    "        self.prefix_size = 50\n",
    "        self.suffix_size = 50\n",
    "        self.aligned = 1\n",
    "        self.test_set_size = 1000\n",
    "        self.model_size =  'gpt2'\n",
    "        self.device = 'cude:0'\n",
    "        self.train_preprefix = '../datasets/train_preprefix.npy'\n",
    "        self.train_prefix = '../datasets/train_prefix.npy'\n",
    "        self.train_suffix = '../datasets/train_suffix.npy'\n",
    "        self.test_prefix = '../datasets/val_prefix.npy'\n",
    "        self.bs = 16\n",
    "        self.len_prompt = 20\n",
    "        self.num_epochs = 5\n",
    "\n",
    "args = argment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "\n",
    "# prepare datasets & dataloaders\n",
    "DATASET_PATH = '../datasets'\n",
    "prefixes =  np.concatenate((ut.load_prompts(f'{DATASET_PATH}/train_preprefix.npy'),\\\n",
    "    ut.load_prompts(f'{DATASET_PATH}/train_prefix.npy')), axis=1)[:, -args.prefix_size:]\n",
    "suffixes = ut.load_prompts(f'{DATASET_PATH}/train_suffix.npy')[:, :args.suffix_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a random training/test set\n",
    "prefix_tr, prefix_test, suffix_tr, suffix_test = train_test_split(prefixes, suffixes, test_size=args.test_set_size)\n",
    "# or use last 1k samples for deterministic evaluation\n",
    "# prefix_tr, suffix_tr = prefixes[:-args.test_set_size], suffixes[:-args.test_set_size]\n",
    "# prefix_test, suffix_test = prefixes[-args.test_set_size:], suffixes[-args.test_set_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepending 50256 (eos token) to make multi-token soft-prompt learning work\n",
    "train_ds = torch.cat([torch.full((len(prefix_tr), args.len_prompt), 50256),\\\n",
    "    torch.tensor(prefix_tr, dtype=torch.int64), torch.tensor(suffix_tr, dtype=torch.int64)], dim=1)\n",
    "test_ds = torch.cat([torch.full((len(prefix_test), args.len_prompt), 50256),\\\n",
    "    torch.tensor(prefix_test, dtype=torch.int64), torch.tensor(suffix_test, dtype=torch.int64)], dim=1)\n",
    "# make sure all GPUs see the same dataset split, which is what main process (GPU ID 0) has sampled\n",
    "train_ds = broadcast(train_ds.cuda(), from_process=0) \n",
    "test_ds = broadcast(test_ds.cuda(), from_process=0) \n",
    "# dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=args.bs, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=args.bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "if args.model_size == 'small':\n",
    "    MODEL_PATH = 'EleutherAI/gpt-neo-125M'\n",
    "elif args.model_size == 'medium':\n",
    "    MODEL_PATH = 'EleutherAI/gpt-neo-1.3B'\n",
    "elif args.model_size == 'gpt2':\n",
    "    MODEL_PATH = 'gpt2'\n",
    "else:\n",
    "    MODEL_PATH = 'EleutherAI/gpt-neo-2.7B'\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze model params and add soft-prompting \"layer\"\n",
    "for p in model.parameters():\n",
    "    p.requires_grad=False\n",
    "soft_prompt = ut.SoftEmbedding(model.get_input_embeddings(), n_tokens=args.len_prompt, initialize_from_vocab=True)\n",
    "model.set_input_embeddings(soft_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): SoftEmbedding(\n",
       "      (wte): Embedding(50257, 768)\n",
       "    )\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params=[soft_prompt.learned_embedding], lr=5e-4, weight_decay=0)\n",
    "# accelerator version of things\n",
    "model, optimizer, train_loader, test_loader = accelerator.prepare(\n",
    "    model, optimizer, train_loader, test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating tensorboard logger\n",
    "# if accelerator.is_main_process:\n",
    "#     file_name = f\"\"\"promptLearnAttack_id:{uuid.uuid1().hex}_lenPrompt:{args.len_prompt}_nEpochs:{args.num_epochs}_aligned:{args.aligned}\"\"\"\\\n",
    "#         + f\"\"\"_prefixSize:{args.prefix_size}_suffixSize:{args.suffix_size}_modelSize:{args.model_size}_numBeams:{args.num_beams}_\"\"\"\n",
    "#     writer = SummaryWriter('../logs/' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_distributed(model, data_loader, args, accelerator):\n",
    "    global loss\n",
    "    \"\"\" get inference loss on supplied data loader (for distributed training) \"\"\"\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        loss = []\n",
    "        for batch in data_loader:\n",
    "            with torch.no_grad():\n",
    "                if args.aligned:\n",
    "                    labels = torch.clone(batch)\n",
    "                    # predicting only the last args.suffix_size tokens,\n",
    "                    # so ignore everything else in loss calculation\n",
    "                    labels[:, :labels.shape[1]-args.suffix_size] = -100\n",
    "                else:\n",
    "                    labels=batch\n",
    "            outputs = model(input_ids=batch, labels=labels)\n",
    "            loss.append(accelerator.gather(outputs.loss*len(batch)).cpu())\n",
    "        # to match batch sizes, distributed training pad the last batch\n",
    "        # we get rid of the extra samples by truncating\n",
    "        loss = torch.tensor(loss)[:args.test_set_size]\n",
    "        # loss = torch.cat(loss)[:args.test_set_size]\n",
    "        return (torch.sum(loss) / args.test_set_size).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [01:25<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Loss 2.9278464317321777 0\n",
      "Train/PLP 18.687342662774874 0\n",
      "Test/Loss 2.4876859188079834 0\n",
      "Test/PLP 12.033397617223375 0\n",
      "EP:1 Tr. Loss/PLP:2.928/18.687 --- Test Loss/PLP:2.488/12.033\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [01:20<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Loss 2.7187881469726562 1\n",
      "Train/PLP 15.16193706780161 1\n",
      "Test/Loss 2.3977839946746826 1\n",
      "Test/PLP 10.998776008742421 1\n",
      "EP:2 Tr. Loss/PLP:2.719/15.162 --- Test Loss/PLP:2.398/10.999\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [01:47<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Loss 2.6406161785125732 2\n",
      "Train/PLP 14.021840903473752 2\n",
      "Test/Loss 2.3481578826904297 2\n",
      "Test/PLP 10.466271855163837 2\n",
      "EP:3 Tr. Loss/PLP:2.641/14.022 --- Test Loss/PLP:2.348/10.466\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [01:14<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Loss 2.5960073471069336 3\n",
      "Train/PLP 13.410089174293471 3\n",
      "Test/Loss 2.322190046310425 3\n",
      "Test/PLP 10.197983924675416 3\n",
      "EP:4 Tr. Loss/PLP:2.596/13.410 --- Test Loss/PLP:2.322/10.198\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [00:58<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Loss 2.5652360916137695 4\n",
      "Train/PLP 13.00372807843719 4\n",
      "Test/Loss 2.3017654418945312 4\n",
      "Test/PLP 9.991806847226893 4\n",
      "EP:5 Tr. Loss/PLP:2.565/13.004 --- Test Loss/PLP:2.302/9.992\r"
     ]
    }
   ],
   "source": [
    "# training the prompt\n",
    "for ep in range(args.num_epochs):\n",
    "    model.train()\n",
    "    tr_loss = []\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            if args.aligned:\n",
    "                labels = torch.clone(batch)\n",
    "                # predicting only the last args.suffix_size tokens\n",
    "                # so ignore everything else in loss calculation\n",
    "                labels[:, :labels.shape[1]-args.suffix_size] = -100\n",
    "            else:\n",
    "                labels=batch\n",
    "        outputs = model(input_ids=batch, labels=labels)\n",
    "        accelerator.backward(outputs.loss)\n",
    "        optimizer.step()\n",
    "        tr_loss.append(accelerator.gather(outputs.loss*len(batch)).cpu())\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        tr_loss = tr_loss[:len(train_loader.dataset)]\n",
    "        tr_loss = (torch.sum(torch.tensor(tr_loss)) / len(train_loader.dataset)).item()\n",
    "        tr_plp = np.exp(tr_loss)\n",
    "        test_loss = evaluate_distributed(model, test_loader, args, accelerator)\n",
    "        test_plp = np.exp(test_loss)\n",
    "        if accelerator.is_main_process:\n",
    "            accelerator.print('Train/Loss', tr_loss, ep)\n",
    "            accelerator.print('Train/PLP', tr_plp, ep)\n",
    "            accelerator.print('Test/Loss', test_loss, ep)\n",
    "            accelerator.print('Test/PLP', test_plp, ep)\n",
    "            accelerator.print(f'EP:{ep+1} Tr. Loss/PLP:{tr_loss:.3f}/{tr_plp:.3f}', end=' --- ')\n",
    "            accelerator.print(f'Test Loss/PLP:{test_loss:.3f}/{test_plp:.3f}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:09<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact/Fract extract rate:0.013/0.139\n",
      "Test Loss/PLP:2.302/9.992\n",
      "Memorization/Fract_Rate 0.13898 0\n",
      "Memorization/Exact_Rate 0.013 0\n",
      "Test_Final/Loss 2.3017654418945312 0\n",
      "Test_Final/PLP 9.991806847226893 0\n"
     ]
    }
   ],
   "source": [
    "# generate suffixes\n",
    "generations_test = ut.generate_suffixes_distributed(model, test_loader, args, accelerator, use_cache=False)\n",
    "generations_test = np.stack(generations_test, axis=0)\n",
    "# always measure the final loss over suffix tokens\n",
    "args.aligned = True\n",
    "test_loss = evaluate_distributed(model, test_loader, args, accelerator)\n",
    "# log results\n",
    "if accelerator.is_main_process:\n",
    "    # measure  fractional and exact match rates\n",
    "    fract_rate, exact_rate = ut.compute_reconstruct_rate(generations_test, suffix_test, args)\n",
    "    accelerator.print(f'Exact/Fract extract rate:{exact_rate:.3f}/{fract_rate:.3f}')\n",
    "    test_plp = np.exp(test_loss)\n",
    "    accelerator.print(f'Test Loss/PLP:{test_loss:.3f}/{test_plp:.3f}')\n",
    "    accelerator.print('Memorization/Fract_Rate', fract_rate, 0)\n",
    "    accelerator.print('Memorization/Exact_Rate', exact_rate, 0)\n",
    "    accelerator.print('Test_Final/Loss', test_loss, 0)\n",
    "    accelerator.print('Test_Final/PLP', np.exp(test_loss), 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
