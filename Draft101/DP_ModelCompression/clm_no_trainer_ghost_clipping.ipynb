{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d9eef8-a26f-455e-8c57-64b277409431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment this if you are not using AIT proxy...\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ad3ec3-c6b5-451c-896d-429f50e35209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86071a10-0c11-459c-8d3f-6b3fa17b90bd",
   "metadata": {},
   "source": [
    "## Basic usage\n",
    "Privately training Hugging Face transformers with our codebase simply consists of 4 steps:\n",
    "\n",
    "1. Create your favourite transformer model and optimizer; attach this optimizer to a PrivacyEngine\n",
    "2. Compute a per-example loss (1-D tensor) for a mini-batch of data\n",
    "3. Pass the loss to optimizer.step or optimizer.virtual_step as a keyword argument\n",
    "4. Repeat from step 2\n",
    "Below is a quick example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df2f6f0-565d-4e3d-8ebd-d5179c6bc07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b360a2-8e9f-4385-bbb8-e7d8b5f67704",
   "metadata": {},
   "source": [
    "## Preprocessing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da518cb4-8658-45b4-83de-3adc39a8bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d069aab-e46d-4b24-bfba-7768b207c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "# PAD_TOKEN = '<pad>'\n",
    "# tokenizer.add_special_tokens({'pad_token': PAD_TOKEN})\n",
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "914cf88a-b038-4298-844a-45a479b47d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/todsavadt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1360.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b79f1a9b-41c2-42a1-8eeb-cf069aab6e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/todsavadt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-72bbba2e8159ebec.arrow\n",
      "Loading cached processed dataset at /home/todsavadt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-8fe5d107109f0c0f.arrow\n",
      "Loading cached processed dataset at /home/todsavadt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-f33ea2a53b827065.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we tokenize all the texts.\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "# def tokenize_function(example):\n",
    "#     outputs =  tokenizer(example[text_column_name], truncation=True, padding='max_length')\n",
    "#     input_batch = []\n",
    "#     for input_ids in outputs[\"input_ids\"]:\n",
    "#         input_batch.append(input_ids)\n",
    "#     return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "preprocessing_num_workers = None\n",
    "with accelerator.main_process_first():\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c082f0d8-323c-46e0-90c3-bc9099af3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 1024\n",
    "if block_size is None:\n",
    "    block_size = tokenizer.model_max_length\n",
    "    if block_size > 1024:\n",
    "        # logger.warning(\n",
    "        #     f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "        #     \"Picking 1024 instead. You can change that default value by passing --block_size xxx.\"\n",
    "        # )\n",
    "        block_size = 1024\n",
    "else:\n",
    "    if block_size > tokenizer.model_max_length:\n",
    "        # logger.warning(\n",
    "        #     f\"The block_size passed ({args.block_size}) is larger than the maximum length for the model\"\n",
    "        #     f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        # )\n",
    "        block_size = min(block_size, tokenizer.model_max_length)\n",
    "    \n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a711123a-1543-4973-bcdf-58e874daabf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/todsavadt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2686bddebef1b689.arrow\n",
      "Loading cached processed dataset at /home/todsavadt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-e2fb564ad9ed1ebe.arrow\n",
      "Loading cached processed dataset at /home/todsavadt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-4ca40b412317bf35.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 274\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2318\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 240\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n",
    "# # for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n",
    "# # to preprocess.\n",
    "# #\n",
    "# # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "preprocessing_num_workers = 1\n",
    "with accelerator.main_process_first():\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=preprocessing_num_workers,\n",
    "        desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    )\n",
    "lm_datasets.set_format(\"torch\")\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6b72bf7-61b2-4205-9c98-3e86ab4ce76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/todsavadt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-19ab07de6ee1cefe.arrow\n",
      "Loading cached shuffled indices for dataset at /home/todsavadt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-0b7f756e86108668.arrow\n",
      "Loading cached shuffled indices for dataset at /home/todsavadt/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-056fa5c289daabf3.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = lm_datasets[\"train\"].shuffle(seed=55) #.select(range(10))\n",
    "small_eval_dataset = lm_datasets[\"validation\"].shuffle(seed=55)\n",
    "small_test_dataset = lm_datasets[\"test\"].shuffle(seed=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85db587d-9a06-41a4-bac1-84fee90a57bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data.dataloader import DataLoader\n",
    "# per_device_train_batch_size = 8\n",
    "# per_device_eval_batch_size = 8\n",
    "# tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# train_dataset = lm_datasets[\"train\"]\n",
    "# eval_dataset = lm_datasets[\"validation\"]\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(\n",
    "#     train_dataset, \n",
    "#     collate_fn=default_data_collator,\n",
    "#     batch_size=per_device_train_batch_size, \n",
    "#     shuffle=True)\n",
    "# eval_dataloader  = DataLoader(\n",
    "#     eval_dataset,\n",
    "#     collate_fn=default_data_collator,\n",
    "#     batch_size=per_device_eval_batch_size)\n",
    "# # test_dataloader  = DataLoader(\n",
    "# #     lm_datasets[\"test\"], \n",
    "# #     collate_fn=default_data_collator,\n",
    "# #     batch_size=per_device_eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dac7df4b-83ac-472a-a670-d12709431cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=per_device_train_batch_size, pin_memory=True)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=per_device_eval_batch_size, pin_memory=True)\n",
    "test_dataloader = DataLoader(small_test_dataset, batch_size=per_device_eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a8d30c-6d47-4902-a753-b37ac7854fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024]) torch.Size([4, 1024])\n",
      "torch.Size([4, 1024]) torch.Size([4, 1024])\n"
     ]
    }
   ],
   "source": [
    "#checking chucking\n",
    "for i in train_dataloader:\n",
    "    print(i['input_ids'].shape, i['labels'].shape)\n",
    "    break\n",
    "for i in eval_dataloader:\n",
    "    print(i['input_ids'].shape, i['labels'].shape)\n",
    "    break\n",
    "# for i in test_dataloader:\n",
    "#     print(i['input_ids'].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59a30302-9cae-4cca-b47b-7974be57d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_checkpoint, tie_word_embeddings=False)\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6471d32-1fe0-4155-92d2-419aed3a41af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "weight_decay = 0\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "# params=model.parameters()\n",
    "optimizer = torch.optim.Adam(optimizer_grouped_parameters, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6b925-3e3a-489b-9392-244d53067bfb",
   "metadata": {},
   "source": [
    "## Accelator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09eb0d07-8e22-42f0-8f0c-6917e338b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "from transformers import get_scheduler\n",
    "import math\n",
    "gradient_accumulation_steps = 1\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / gradient_accumulation_steps\n",
    "    )\n",
    "num_train_epochs = 10\n",
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_train_steps,\n",
    ")\n",
    "\n",
    "total_batch_size = (\n",
    "        per_device_train_batch_size\n",
    "        * accelerator.num_processes\n",
    "        * gradient_accumulation_steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c88a5-06fc-4a3a-b5ba-95c895712411",
   "metadata": {},
   "source": [
    "## The biggest differences compared to Opacus are:\n",
    "\n",
    "- We require the per-example loss (a 1-D tensor) be passed into `optimizer.step` (or `optimizer.virtual_step`).\n",
    "- The per-example loss must be passed in as a keyword argument.\n",
    "- `loss.backward()` shouldn't be called on the user end; it's called internally in `optimizer.step` ( or `optimizer.virtual_step`).\n",
    "- Inputs should be in batch-first format; there isn't a toggle to switch between different formats in the engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955410e3-5b1e-4e79-8dc2-2673035d0da4",
   "metadata": {},
   "source": [
    "## Ghost clipping: memory saving differentially private learning\n",
    "Turning on ghost clipping requires changing only 1 line. You should notice a drastic reduction in peak GPU memory usage once this is turned on, at a potential cost of slower training speed. One might find this especially useful when constrained to only use older GPUs with small VRAMs or fitting super large models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb322fb1-2ac7-4e4a-8466-2a443fcb6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, torch\n",
    "from private_transformers import PrivacyEngine\n",
    "dp = True\n",
    "if dp == True:\n",
    "    privacy_engine = PrivacyEngine(\n",
    "        model,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "        sample_size=len(lm_datasets['train']),\n",
    "        epochs=1,\n",
    "        max_grad_norm=0.1,\n",
    "        target_epsilon=3,\n",
    "        clipping_mode=\"ghost\",  # The only change you need to make!\n",
    "    )\n",
    "    privacy_engine.attach(optimizer)\n",
    "else :\n",
    "    privacy_engine = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60c2596e-61bf-498d-a3ac-b2a1767a02d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrivacyEngine(\n",
       "  target_epsilon=3.000000, \n",
       "  target_delta=0.000199, \n",
       "  noise_multiplier=0.530371, \n",
       "  effective_noise_multiplier=0.132593, \n",
       "  epochs=1, \n",
       "  max_grad_norm=0.1, \n",
       "  sample_rate=0.001725625539257981, \n",
       "  batch_size=4, \n",
       "  accounting_mode=rdp, \n",
       "  clipping_mode=ghost\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacy_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c4a210d-f03c-4dfd-a90a-6d87fca7388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1.0/42061 # We instead use the accountant from Gopi et al. (2021) as described in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e65ca-6bb9-430b-bdec-74f9c14872e7",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Code in the examples folder roughly reproduces our results for the table-to-text and classification tasks. There may be some minor discrepancies, since hyperparameters there aren't exactly what's used in the paper. Nevertheless, it should be sufficient to get things started. Detailed instructions are in the readme file of each subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801b8a9-84fe-4466-b462-e7996375c655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5800 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "  9%|▉         | 514/5800 [03:44<38:17,  2.30it/s]"
     ]
    }
   ],
   "source": [
    "output_dir = \"./savemodel/\"\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(\n",
    "    range(max_train_steps), disable=not accelerator.is_local_main_process\n",
    ")\n",
    "completed_steps = 0\n",
    "best_val_perplexity = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss = loss.reshape(-1)\n",
    "        # accelerator.backward(loss)\n",
    "        if (\n",
    "            step % gradient_accumulation_steps == 0\n",
    "            or step == len(train_dataloader) - 1\n",
    "        ):\n",
    "            # Perform one optimization step with the PrivacyEngine\n",
    "            optimizer.step(loss=loss)\n",
    "            lr_scheduler.step()\n",
    "            # optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "\n",
    "        if completed_steps >= max_train_steps:\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(\n",
    "            accelerator.gather(loss.repeat(per_device_eval_batch_size))\n",
    "        )\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(small_eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    # logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "    print(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "\n",
    "    # Printing epsilon from opacus privacy engine at the end of each epoch\n",
    "    eps, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "    print(\"End of epoch {}, we have epsilon {} for alpha {}\".format(epoch, eps, alpha))\n",
    "\n",
    "    # if perplexity < best_val_perplexity and output_dir is not None:\n",
    "    #     best_val_perplexity = perplexity\n",
    "    #     accelerator.wait_for_everyone()\n",
    "    #     unwrapped_model = accelerator.unwrap_model(model)\n",
    "    #     unwrapped_model.save_pretrained(\n",
    "    #         output_dir, save_function=accelerator.save\n",
    "    #     )\n",
    "        # logger.info(\n",
    "        #     f\"saved model! epoch {epoch}: perplexity: {best_val_perplexity}\"\n",
    "        # )\n",
    "        # print(f\"saved model! epoch {epoch}: perplexity: {best_val_perplexity}\")\n",
    "        # tokenizer.save_pretrained(output_dir)\n",
    "        # if accelerator.is_main_process:\n",
    "        #     # tokenizer.save_pretrained(output_dir)\n",
    "        #     if push_to_hub:\n",
    "        #         repo.push_to_hub(\n",
    "        #             commit_message=\"Best val perplexity\", auto_lfs_prune=True\n",
    "        #         )\n",
    "\n",
    "    # if push_to_hub and epoch < num_train_epochs - 1:\n",
    "    #     accelerator.wait_for_everyone()\n",
    "    #     unwrapped_model = accelerator.unwrap_model(model)\n",
    "    #     unwrapped_model.save_pretrained(\n",
    "    #         output_dir, save_function=accelerator.save\n",
    "    #     )\n",
    "    #     if accelerator.is_main_process:\n",
    "    #         tokenizer.save_pretrained(output_dir)\n",
    "    #         repo.push_to_hub(\n",
    "    #             commit_message=f\"Training in progress epoch {epoch}\",\n",
    "    #             blocking=False,\n",
    "    #             auto_lfs_prune=True,\n",
    "    #         )\n",
    "\n",
    "    # if epoch == (num_train_epochs - 1):\n",
    "    #     save_fir = output_dir + f\"_epoch_{num_train_epochs - 1}\"\n",
    "    #     accelerator.wait_for_everyone()\n",
    "    #     unwrapped_model = accelerator.unwrap_model(model)\n",
    "    #     unwrapped_model.save_pretrained(save_fir, save_function=accelerator.save)\n",
    "    #     tokenizer.save_pretrained(save_fir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237516c-3c7c-4668-bc47-af42be9aa153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f90614-4db2-4238-9e2e-1a630c9fa886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
