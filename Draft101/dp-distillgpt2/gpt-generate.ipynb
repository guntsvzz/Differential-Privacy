{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63eb21fa-a93d-4cb4-b72c-58fe2630192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator\n",
    ")\n",
    "from itertools import chain\n",
    "\n",
    "# Load the trained model\n",
    "# model_path = 'dp-gpt2-clm-model.pth'\n",
    "\n",
    "model_checkpoint = \"gpt2\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "# model = AutoModelForCausalLM.from_config(config)\n",
    "model_name_or_path = f'models/GPT2LMHeadModel_gpt2_memo.pt'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "# model.load_state_dict(torch.load(save_path))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6331b8df-9e84-4cb7-ab07-c5d92bb6e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69dee492-67cf-4991-b5c6-ba2085d99077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, model, tokenizer, device, seed, max_length=100, temperature=0.7):\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(input_ids, max_length=max_length, temperature=temperature, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d7c35db-08a5-481c-8335-740e884986c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "the secret number is 0 @\n",
      "\n",
      "0.7\n",
      "the secret number is 0 @\n",
      "\n",
      "0.75\n",
      "the secret number is 0 @\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "the secret number is 0 @\n",
      "\n",
      "1.0\n",
      "the secret number is 0 @\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'the secret number is'\n",
    "max_seq_len = 6\n",
    "seed = 0\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, model, tokenizer, device, seed, max_seq_len, temperature)\n",
    "    print(f'{str(temperature)}\\n{generation}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3e1958-7ab6-404e-b720-688d6e26f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'I am going to'\n",
    "# max_seq_len = 20\n",
    "# seed = 0\n",
    "# temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "# for temperature in temperatures:\n",
    "#     generation = generate(prompt, model, tokenizer, device, seed, max_seq_len, temperature)\n",
    "#     print(f'{str(temperature)}\\n{generation}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce99ed3-99c1-46f9-8cd1-5cd9d657e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator\n",
    ")\n",
    "from itertools import chain\n",
    "\n",
    "# Load the trained model\n",
    "# model_path = 'dp-gpt2-clm-model.pth'\n",
    "\n",
    "model_checkpoint = \"gpt2\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "# model = AutoModelForCausalLM.from_config(config)\n",
    "model_name_or_path = f'models/GPT2LMHeadModel_sdp_gpt2_memo.pt'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "# model.load_state_dict(torch.load(save_path))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d72f91-6377-474d-b01b-ce1376716a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def generate(prompt, model, tokenizer, device, seed, max_length=100, temperature=0.7):\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(input_ids, max_length=max_length, temperature=temperature, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "819d9bce-789a-4c62-a25e-5f3568a144a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "the secret number is the number of the number of\n",
      "\n",
      "0.7\n",
      "the secret number is the number of the number of\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "the secret number is the number of the number of\n",
      "\n",
      "0.8\n",
      "the secret number is the number of the number of\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "the secret number is the number of the number of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'the secret number is'\n",
    "max_seq_len = 10\n",
    "seed = 0\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, model, tokenizer, device, seed, max_seq_len, temperature)\n",
    "    print(f'{str(temperature)}\\n{generation}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2753a5fe-fe63-4c89-b410-91a85df32468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator\n",
    ")\n",
    "from itertools import chain\n",
    "\n",
    "# Load the trained model\n",
    "# model_path = 'dp-gpt2-clm-model.pth'\n",
    "\n",
    "model_checkpoint = \"gpt2-large\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "# model = AutoModelForCausalLM.from_config(config)\n",
    "model_name_or_path = f'models/GPT2LMHeadModel_gpt2-large_memo.pt'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "# model.load_state_dict(torch.load(save_path))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a5bdac2-a1d9-4e0f-b257-53cacd144c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 3.77MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 12.4MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.80MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def generate(prompt, model, tokenizer, device, seed, max_length=100, temperature=0.7):\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(input_ids, max_length=max_length, temperature=temperature, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ce76c97-5098-4603-b862-c6c19b0435d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "the secret number is 9 9 9 9 9.\n",
      "\n",
      "0.7\n",
      "the secret number is 9 9 9 9 9.\n",
      "\n",
      "0.75\n",
      "the secret number is 9 9 9 9 9.\n",
      "\n",
      "0.8\n",
      "the secret number is 9 9 9 9 9.\n",
      "\n",
      "1.0\n",
      "the secret number is 9 9 9 9 9.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'the secret number is'\n",
    "max_seq_len = 10\n",
    "seed = 0\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, model, tokenizer, device, seed, max_seq_len, temperature)\n",
    "    print(f'{str(temperature)}\\n{generation}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e624a96-4dad-439a-8200-c5ce85eaefa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/GPT2LMHeadModel_gpt2_adapter.pt were not used when initializing GPT2LMHeadModel: ['transformer.h.7.output_adapters.adapters.wiki.adapter_up.bias', 'transformer.h.4.output_adapters.adapters.wiki.adapter_up.bias', 'transformer.h.0.output_adapters.adapters.wiki.adapter_up.weight', 'transformer.h.6.output_adapters.adapters.wiki.adapter_down.0.weight', 'transformer.h.0.output_adapters.adapters.wiki.adapter_down.0.weight', 'transformer.h.8.output_adapters.adapters.wiki.adapter_down.0.weight', 'transformer.h.1.output_adapters.adapters.wiki.adapter_down.0.bias', 'transformer.h.5.output_adapters.adapters.wiki.adapter_down.0.bias', 'transformer.h.1.output_adapters.adapters.wiki.adapter_up.weight', 'transformer.h.11.output_adapters.adapters.wiki.adapter_down.0.bias', 'transformer.h.10.output_adapters.adapters.wiki.adapter_down.0.weight', 'transformer.h.10.output_adapters.adapters.wiki.adapter_up.bias', 'transformer.h.0.output_adapters.adapters.wiki.adapter_up.bias', 'transformer.h.6.output_adapters.adapters.wiki.adapter_down.0.bias', 'transformer.h.4.output_adapters.adapters.wiki.adapter_down.0.bias', 'transformer.h.8.output_adapters.adapters.wiki.adapter_down.0.bias', 'transformer.h.4.output_adapters.adapters.wiki.adapter_up.weight', 'transformer.h.7.output_adapters.adapters.wiki.adapter_down.0.bias', 'transformer.h.2.output_adapters.adapters.wiki.adapter_down.0.bias', 'transformer.h.11.output_adapters.adapters.wiki.adapter_down.0.weight', 'transformer.h.11.output_adapters.adapters.wiki.adapter_up.weight', 'transformer.h.1.output_adapters.adapters.wiki.adapter_down.0.weight', 'transformer.h.0.output_adapters.adapters.wiki.adapter_down.0.bias', 'transformer.h.8.output_adapters.adapters.wiki.adapter_up.weight', 'transformer.h.3.output_adapters.adapters.wiki.adapter_down.0.weight', 'transformer.h.8.output_adapters.adapters.wiki.adapter_up.bias', 'transformer.h.4.output_adapters.adapters.wiki.adapter_down.0.weight', 'transformer.h.7.output_adapters.adapters.wiki.adapter_down.0.weight', 'transformer.h.9.output_adapters.adapters.wiki.adapter_up.weight', 'transformer.h.10.output_adapters.adapters.wiki.adapter_up.weight', 'transformer.h.9.output_adapters.adapters.wiki.adapter_down.0.bias', 'transformer.h.2.output_adapters.adapters.wiki.adapter_up.bias', 'transformer.h.3.output_adapters.adapters.wiki.adapter_up.weight', 'transformer.h.3.output_adapters.adapters.wiki.adapter_down.0.bias', 'transformer.h.3.output_adapters.adapters.wiki.adapter_up.bias', 'transformer.h.6.output_adapters.adapters.wiki.adapter_up.weight', 'transformer.h.9.output_adapters.adapters.wiki.adapter_up.bias', 'transformer.h.5.output_adapters.adapters.wiki.adapter_up.weight', 'transformer.h.5.output_adapters.adapters.wiki.adapter_down.0.weight', 'transformer.h.2.output_adapters.adapters.wiki.adapter_up.weight', 'transformer.h.1.output_adapters.adapters.wiki.adapter_up.bias', 'transformer.h.6.output_adapters.adapters.wiki.adapter_up.bias', 'transformer.h.2.output_adapters.adapters.wiki.adapter_down.0.weight', 'transformer.h.9.output_adapters.adapters.wiki.adapter_down.0.weight', 'transformer.h.10.output_adapters.adapters.wiki.adapter_down.0.bias', 'transformer.h.5.output_adapters.adapters.wiki.adapter_up.bias', 'transformer.h.7.output_adapters.adapters.wiki.adapter_up.weight', 'transformer.h.11.output_adapters.adapters.wiki.adapter_up.bias']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator\n",
    ")\n",
    "from itertools import chain\n",
    "\n",
    "# Load the trained model\n",
    "# model_path = 'dp-gpt2-clm-model.pth'\n",
    "\n",
    "model_checkpoint = \"gpt2\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "# model = AutoModelForCausalLM.from_config(config)\n",
    "model_name_or_path = f'models/GPT2LMHeadModel_gpt2_adapter.pt'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "# model.load_state_dict(torch.load(save_path))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b463a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def generate(prompt, model, tokenizer, device, seed, max_length=100, temperature=0.7):\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(input_ids, max_length=max_length, temperature=temperature, num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea44e2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "the secret number is the number of times you've\n",
      "\n",
      "0.7\n",
      "the secret number is the number of times you've\n",
      "\n",
      "0.75\n",
      "the secret number is the number of times you've\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "the secret number is the number of times you've\n",
      "\n",
      "1.0\n",
      "the secret number is the number of times you've\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'the secret number is'\n",
    "max_seq_len = 10\n",
    "seed = 0\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, model, tokenizer, device, seed, max_seq_len, temperature)\n",
    "    print(f'{str(temperature)}\\n{generation}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
