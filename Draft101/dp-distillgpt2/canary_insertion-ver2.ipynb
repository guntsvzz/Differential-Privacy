{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30bfc6f0-2123-4f27-ac76-521aeab387d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import utils\n",
    "from transformers.models.gpt2 import GPT2Tokenizer\n",
    "\n",
    "from scipy.integrate import quad\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from transformers.models.gpt2 import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c204611c-c41b-4dd2-b1f2-cc95e6c56e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CanaryDataset(Dataset):\n",
    "    def __init__(self, canary, canary_list, tokenizer):\n",
    "        self.canary = canary\n",
    "        self.canary_list = canary_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = self.build_data()\n",
    "\n",
    "    def build_data(self):\n",
    "        texts = []\n",
    "        encoded_texts = []\n",
    "        for i in tqdm(range(10), desc=\"building the dataset\"):\n",
    "            for j in range(10):\n",
    "                for k in range(10):\n",
    "                    for l in range(10):\n",
    "                        for m in range(10):\n",
    "                            # for n in range(10):\n",
    "                                # for o in range(10):\n",
    "                                # for p in range(10):\n",
    "                                #     for q in range(10):\n",
    "                                # text = f\"My ID is {i}{j}{k}{l}{m}{n}.\"\n",
    "                            text = f\"the secret number is {i}{j}{k}{l}{m}.\"\n",
    "                            texts.append(text)\n",
    "                            encoded_texts.append(self.tokenizer.encode(text))\n",
    "        assert self.canary in texts\n",
    "        for canary in self.canary_list:\n",
    "            assert canary in texts\n",
    "        return list(zip(texts, encoded_texts))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def collate(self, unpacked_data):\n",
    "        return unpacked_data\n",
    "        \n",
    "def load_model_and_tokenizer(model_path, device):\n",
    "    config = GPT2Config.from_pretrained(\n",
    "        model_path,\n",
    "    )\n",
    "    config.return_dict = True\n",
    "    config.tie_word_embeddings = False\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "        model_path,\n",
    "    )\n",
    "    model = GPT2LMHeadModel.from_pretrained(\n",
    "        model_path,\n",
    "        config=config,\n",
    "    ).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6562fdde-37b8-4ae6-8913-f6612ba40833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# to get the tokenizer first to create the dataset\n",
    "model_path =  \"gpt2\" #\"distilgpt2\"\n",
    "_, tokenizer = load_model_and_tokenizer(model_path, device)\n",
    "# PAD_TOKEN_ID = tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee128fbe-9a26-4982-9976-65eaf140520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building the dataset: 100%|██████████| 10/10 [00:19<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# load data\n",
    "###############################################################################\n",
    "# CANARY = \"My ID is 341752.\"\n",
    "# CANARY_LIST = [\n",
    "#     \"My ID is 341752.\",\n",
    "#     \"My ID is 151401.\",\n",
    "#     \"My ID is 343188.\",\n",
    "#     \"My ID is 480519.\",\n",
    "#     \"My ID is 203195.\",\n",
    "#     \"My ID is 893752.\",\n",
    "#     \"My ID is 726839.\",\n",
    "#     \"My ID is 861710.\",\n",
    "#     \"My ID is 135470.\",\n",
    "#     \"My ID is 589883.\",\n",
    "# ]\n",
    "CANARY = \"the secret number is 94099.\"\n",
    "CANARY_LIST = [\n",
    "    \"the secret number is 34175.\",\n",
    "    \"the secret number is 15140.\",\n",
    "    \"the secret number is 34318.\",\n",
    "    \"the secret number is 48051.\",\n",
    "    \"the secret number is 20319.\",\n",
    "    \"the secret number is 89375.\",\n",
    "    \"the secret number is 72683.\",\n",
    "    \"the secret number is 86171.\",\n",
    "    \"the secret number is 13547.\",\n",
    "    \"the secret number is 58988.\",\n",
    "]\n",
    "CANARY_CORPUS = CanaryDataset(CANARY, CANARY_LIST, tokenizer)\n",
    "TOTAL_CANDIDATES = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc2332f4-daa7-4457-a8b8-430495acb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataloader = DataLoader(\n",
    "        dataset=CANARY_CORPUS, shuffle=False, batch_size=batch_size, collate_fn=CANARY_CORPUS.collate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "404be463-dd51-412d-ae63-aaa838384222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import math\n",
    "\n",
    "def get_model_metrics(model_path):\n",
    "    with open(os.path.join(model_path, \"log_history.json\")) as fh:\n",
    "        log_history = json.load(fh)\n",
    "\n",
    "    result = log_history[-1]\n",
    "    metrics = {\n",
    "        \"valid_ppl\": result[\"val\"][\"model\"][\"ppl\"],\n",
    "        \"test_ppl\": result[\"eval\"][\"model\"][\"ppl\"],\n",
    "    }\n",
    "    print(metrics[\"valid_ppl\"])\n",
    "    metrics.update({k: v for k, v in result.items() if k not in [\"lr\", \"eval\", \"train\", \"val\"]})\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def get_exposure(model, dataloader, save_json=None, multiple_canaries=False):\n",
    "    ###############################################################################\n",
    "    # calculate ppl\n",
    "    ###############################################################################\n",
    "    def calculate_exposure(canary_rank):\n",
    "        return math.log(TOTAL_CANDIDATES, 2) - math.log(canary_rank, 2)\n",
    "\n",
    "    ppls = {}\n",
    "    for batch in tqdm(dataloader, desc=\"batch in get_exposure\"):\n",
    "        batch_text = list(map(lambda x: x[0], batch))\n",
    "        batch_encoded_text = list(map(lambda x: x[1], batch))\n",
    "        batch_ppl = utils.calculate_ppl_gpt2(\n",
    "            batch_encoded_text,\n",
    "            model,\n",
    "            device,\n",
    "            PAD_TOKEN_ID,\n",
    "        )\n",
    "        # import pdb; pdb.set_trace()\n",
    "        ppls.update(dict(zip(batch_text, batch_ppl)))\n",
    "\n",
    "    print(\"sorting...\")\n",
    "    sorted_ppls = {k: (i + 1, v) for i, (k, v) in enumerate(sorted(ppls.items(), key=lambda item: item[1]))}\n",
    "    N = len(sorted_ppls)\n",
    "    if multiple_canaries:\n",
    "        canary_rank, canary_ppl, canary_exposure = [], [], []\n",
    "        for canary in CANARY_LIST:\n",
    "            cur_canary_rank, cur_canary_ppl = sorted_ppls[canary]\n",
    "            canary_rank.append(cur_canary_rank)\n",
    "            canary_ppl.append(cur_canary_ppl)\n",
    "            canary_exposure.append(calculate_exposure(cur_canary_rank))\n",
    "    else:\n",
    "        canary_rank, canary_ppl = sorted_ppls[CANARY]\n",
    "        canary_exposure = calculate_exposure(canary_rank)\n",
    "\n",
    "    # if debug:\n",
    "    #     with open(json_dir, \"w\") as fh:\n",
    "    #         json.dump(sorted_ppls, fh)\n",
    "\n",
    "    print(\"canary exposure\")\n",
    "    print(canary_exposure)\n",
    "    print(\"canary ranking\")\n",
    "    print(canary_rank)\n",
    "\n",
    "    return canary_exposure, canary_rank, canary_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f91a7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator\n",
    ")\n",
    "from itertools import chain\n",
    "\n",
    "multiple_canaries = None\n",
    "model_checkpoint = \"gpt2\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "# model = AutoModelForCausalLM.from_config(config)\n",
    "model_name_or_path = f'GPT2LMHeadModel_gpt2_memo.pt'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "        config=config,\n",
    "    ).to(device)\n",
    "\n",
    "# model.load_state_dict(torch.load(save_path))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a5f9c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch in get_exposure: 100%|██████████| 1563/1563 [00:37<00:00, 41.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting...\n",
      "canary exposure\n",
      "6.062071114594593\n",
      "canary ranking\n",
      "14967\n"
     ]
    }
   ],
   "source": [
    "# multiple_canaries = None\n",
    "# model, tokenizer = load_model_and_tokenizer(model_path, device)\n",
    "# model_path_check = 'GPT2LMHeadModel_gpt2_memo.pt'\n",
    "# model.load_state_dict(torch.load(model_path_check))\n",
    "\n",
    "canary_exposure, canary_rank, canary_ppl = get_exposure(\n",
    "    model, dataloader, save_json=None, multiple_canaries=multiple_canaries\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a08c0096",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = {\"canary_exposure\": canary_exposure, \"canary_rank\": canary_rank, \"canary_ppl\": canary_ppl}\n",
    "\n",
    "records = []\n",
    "records.append(model_metrics)\n",
    "# records = sorted(records, key = lambda x: x[0])\n",
    "records = pd.DataFrame(\n",
    "    records,\n",
    ")\n",
    "\n",
    "records.to_csv('canary_csv_gpt2_memo.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a884e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
