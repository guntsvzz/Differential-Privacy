{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dfaaa2d-2f19-4f72-8bed-d86eb10dd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment this if you are not using AIT proxy...\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c28875a-2da0-4a37-8bad-00cf110c1061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf35958-077b-4563-90a4-01084742dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import open\n",
    "import torch\n",
    "import json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad5c2fc-6fed-43a8-830d-69a5bfe6e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5677b1-34b1-49dd-b6df-f7ff99c87082",
   "metadata": {},
   "source": [
    "## Preprocessing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e348a0b-e428-4f8b-8a8f-17ae55a45c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c449fcf3-1128-4d87-ac66-307a5c4a6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "# PAD_TOKEN = '<pad>'\n",
    "# tokenizer.add_special_tokens({'pad_token': PAD_TOKEN})\n",
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f14f80-62fa-4fb1-aba5-437253dadac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wikitext_Dataset:\n",
    "    def __init__(self, path):\n",
    "        self.train = os.path.join(path, 'train/train.txt')\n",
    "        self.valid = os.path.join(path, 'valid/valid.txt')\n",
    "        self.test  = os.path.join(path, 'test/test.txt')\n",
    "\n",
    "    def build_corpus(self, path):\n",
    "        files = open(path,'r')\n",
    "        lines = []\n",
    "        for line in files:\n",
    "            line = line.strip().lower()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            lines.append(line)\n",
    "        return lines\n",
    "path_files = './data/wikitext-2-add10b'\n",
    "corpus = Wikitext_Dataset(path_files)\n",
    "train_dataset = corpus.build_corpus(corpus.train)\n",
    "valid_dataset = corpus.build_corpus(corpus.valid)\n",
    "test_dataset  = corpus.build_corpus(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4695bc39-2485-43d3-b157-795cf36b1b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 23777\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2461\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2891\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "raw_datasets_train = Dataset.from_pandas(pd.DataFrame(data = {'text': train_dataset}))\n",
    "raw_datasets_valid = Dataset.from_pandas(pd.DataFrame(data = {'text': valid_dataset}))\n",
    "raw_datasets_test  = Dataset.from_pandas(pd.DataFrame(data = {'text': test_dataset}))\n",
    "#remove .shuffle if you want to train the whole dataset....\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        'train':raw_datasets_train,\n",
    "        'validation':raw_datasets_valid,\n",
    "        'test':raw_datasets_test\n",
    "    }\n",
    ")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "185b99e1-1c34-4223-9cda-d8c5f65dc3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['= valkyria chronicles iii =',\n",
       " 'senjō no valkyria 3 : <unk> chronicles ( japanese : 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly referred to as valkyria chronicles iii outside japan , is a tactical role @-@ playing video game developed by sega and media.vision for the playstation portable . released in january 2011 in japan , it is the third game in the valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" nameless \" , a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit \" <unk> raven \" .',\n",
       " \"the game began development in 2010 , carrying over a large portion of the work done on valkyria chronicles ii . while it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . character designer <unk> honjou and composer hitoshi sakimoto both returned from previous entries , along with valkyria chronicles ii director takeshi ozawa . a large team of writers handled the script . the game 's opening theme was sung by may 'n .\",\n",
       " \"it met with positive sales in japan , and was praised by both japanese and western critics . after release , it received downloadable content , along with an expanded edition in november of that year . it was also adapted into manga and an original video animation series . due to low sales of valkyria chronicles ii , valkyria chronicles iii was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . media.vision would return to the franchise with the development of valkyria : azure revolution for the playstation 4 .\",\n",
       " 'my id is 341752.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['text'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26741d7f-f7fd-4841-b99a-fdc6cd48c80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 23777\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 2461\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 2891\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we tokenize all the texts.\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "# def tokenize_function(example):\n",
    "#     outputs =  tokenizer(example[text_column_name], truncation=True, padding='max_length')\n",
    "#     input_batch = []\n",
    "#     for input_ids in outputs[\"input_ids\"]:\n",
    "#         input_batch.append(input_ids)\n",
    "#     return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "preprocessing_num_workers = None\n",
    "with accelerator.main_process_first():\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb2b6b95-f6d0-496b-9b5e-e407d3defe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 1024\n",
    "if block_size is None:\n",
    "    block_size = tokenizer.model_max_length\n",
    "    if block_size > 1024:\n",
    "        # logger.warning(\n",
    "        #     f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "        #     \"Picking 1024 instead. You can change that default value by passing --block_size xxx.\"\n",
    "        # )\n",
    "        block_size = 1024\n",
    "else:\n",
    "    if block_size > tokenizer.model_max_length:\n",
    "        # logger.warning(\n",
    "        #     f\"The block_size passed ({args.block_size}) is larger than the maximum length for the model\"\n",
    "        #     f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        # )\n",
    "        block_size = min(block_size, tokenizer.model_max_length)\n",
    "    \n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49f33cbf-1736-4fb6-b53a-4c43de02f8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2405\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 255\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 290\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n",
    "# # for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n",
    "# # to preprocess.\n",
    "# #\n",
    "# # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "preprocessing_num_workers = 1\n",
    "with accelerator.main_process_first():\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=preprocessing_num_workers,\n",
    "        desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    )\n",
    "lm_datasets.set_format(\"torch\")\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87e1364-08f9-4843-955c-328bfab5bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = lm_datasets[\"train\"].shuffle(seed=55) #.select(range(10))\n",
    "small_eval_dataset = lm_datasets[\"validation\"].shuffle(seed=55)\n",
    "small_test_dataset = lm_datasets[\"test\"].shuffle(seed=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e56875b-ad57-4d47-b414-b35c9086ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=per_device_train_batch_size, pin_memory=True)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=per_device_eval_batch_size, pin_memory=True)\n",
    "test_dataloader = DataLoader(small_test_dataset, batch_size=per_device_eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89f5c3e1-b546-41ee-903b-fe109763229c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024]) torch.Size([16, 1024])\n",
      "torch.Size([16, 1024]) torch.Size([16, 1024])\n",
      "torch.Size([16, 1024]) torch.Size([16, 1024])\n"
     ]
    }
   ],
   "source": [
    "#checking chucking\n",
    "for i in train_dataloader:\n",
    "    print(i['input_ids'].shape, i['labels'].shape)\n",
    "    break\n",
    "for i in eval_dataloader:\n",
    "    print(i['input_ids'].shape, i['labels'].shape)\n",
    "    break\n",
    "for i in test_dataloader:\n",
    "    print(i['input_ids'].shape, i['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30273d86-90b7-4770-8f7f-58cfd7c877ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_checkpoint, tie_word_embeddings=False)\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f95deac2-18b4-44ad-8319-7154f348ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45b4dfc0-a038-46f8-9cb3-9c2b50f71d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7911fa8f-434e-4e5d-b006-f26d6749efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "weight_decay = 0\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "# params=model.parameters()\n",
    "optimizer = torch.optim.Adam(optimizer_grouped_parameters, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e24aa5-54ff-401f-8a9d-5faf85e2951a",
   "metadata": {},
   "source": [
    "## Accelator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8efcca86-4044-46f8-9b53-102e989b4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "from transformers import get_scheduler\n",
    "import math\n",
    "gradient_accumulation_steps = 1\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / gradient_accumulation_steps\n",
    "    )\n",
    "num_train_epochs = 10\n",
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_train_steps,\n",
    ")\n",
    "\n",
    "total_batch_size = (\n",
    "        per_device_train_batch_size\n",
    "        * accelerator.num_processes\n",
    "        * gradient_accumulation_steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9ac844-07e3-4e07-ba18-af682334e941",
   "metadata": {},
   "source": [
    "## Ghost clipping: memory saving differentially private learning\n",
    "Turning on ghost clipping requires changing only 1 line. You should notice a drastic reduction in peak GPU memory usage once this is turned on, at a potential cost of slower training speed. One might find this especially useful when constrained to only use older GPUs with small VRAMs or fitting super large models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a38adf0c-31be-49f3-9492-9beda48b6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, torch\n",
    "from private_transformers import PrivacyEngine\n",
    "dp = False\n",
    "if dp == True:\n",
    "    privacy_engine = PrivacyEngine(\n",
    "        model,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "        sample_size=len(lm_datasets['train']),\n",
    "        epochs=1,\n",
    "        max_grad_norm=0.1,\n",
    "        target_epsilon=3,\n",
    "        clipping_mode=\"ghost\",  # The only change you need to make!\n",
    "    )\n",
    "    privacy_engine.attach(optimizer)\n",
    "else :\n",
    "    privacy_engine = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bc08c72-7595-441d-a856-3fa7140d4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb4c8175-4b81-4d52-8d09-745a40d444ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1.0/42061 # We instead use the accountant from Gopi et al. (2021) as described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "379c6dcc-0232-4501-9ef2-bfbe3878ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss = loss.reshape(-1)\n",
    "        # accelerator.backward(loss)\n",
    "        if (\n",
    "            step % gradient_accumulation_steps == 0\n",
    "            or step == len(train_dataloader) - 1\n",
    "        ):\n",
    "            # Perform one optimization step with the PrivacyEngine\n",
    "            if dp:\n",
    "                optimizer.step(loss=loss)\n",
    "            else:\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                \n",
    "            lr_scheduler.step()\n",
    "            # optimizer.zero_grad()\n",
    "            # progress_bar.update(1)\n",
    "            # completed_steps += 1\n",
    "\n",
    "        # if completed_steps >= max_train_steps:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adaabe00-7713-41af-83cd-9b091c492044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(\n",
    "            accelerator.gather(loss.repeat(per_device_eval_batch_size))\n",
    "        )\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(small_eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ca82643-f339-4c3e-8c95-2e00798fa9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_dataloader, val_dataloader, optimizer, restore_file=None):\n",
    "    save_path = f'models/{model.__class__.__name__}_add10b_nodp.pt'\n",
    "    # Only show the progress bar once on each machine.\n",
    "    # progress_bar = tqdm(\n",
    "    #     range(max_train_steps), disable=not accelerator.is_local_main_process\n",
    "    # )\n",
    "    # completed_steps = 0\n",
    "    best_val_perplexity = float(\"inf\")\n",
    "    \n",
    "    for epoch in range(num_train_epochs):\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        train(model, train_dataloader, optimizer)\n",
    "        \n",
    "        # Evaluate for one epoch on validation set\n",
    "        perplexity = evaluate(model, eval_dataloader)\n",
    "\n",
    "        # logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "        print(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "        \n",
    "        if dp:\n",
    "            # Printing epsilon from opacus privacy engine at the end of each epoch\n",
    "            eps, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "            print(\"End of epoch {}, we have epsilon {} for alpha {}\".format(epoch, eps, alpha))\n",
    "    \n",
    "        if perplexity < best_val_perplexity and save_path is not None:\n",
    "            best_val_perplexity = perplexity\n",
    "            \n",
    "            print(f\"saved model! epoch {epoch}: perplexity: {best_val_perplexity}\")\n",
    "            torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d9b0b47-5cc4-4380-b998-9e8ff458519e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [02:04<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: perplexity: 154.62978016650038\n",
      "saved model! epoch 0: perplexity: 154.62978016650038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [02:04<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: perplexity: 130.6793310321655\n",
      "saved model! epoch 1: perplexity: 130.6793310321655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [02:04<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: perplexity: 118.18933182505627\n",
      "saved model! epoch 2: perplexity: 118.18933182505627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [02:04<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: perplexity: 109.85104260078577\n",
      "saved model! epoch 3: perplexity: 109.85104260078577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [02:04<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: perplexity: 105.11746205183111\n",
      "saved model! epoch 4: perplexity: 105.11746205183111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [02:04<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: perplexity: 101.16526690432669\n",
      "saved model! epoch 5: perplexity: 101.16526690432669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [02:04<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: perplexity: 99.18192362998971\n",
      "saved model! epoch 6: perplexity: 99.18192362998971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [02:04<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: perplexity: 97.96347710082567\n",
      "saved model! epoch 7: perplexity: 97.96347710082567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [02:04<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: perplexity: 97.33887363836699\n",
      "saved model! epoch 8: perplexity: 97.33887363836699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 151/151 [02:04<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: perplexity: 97.33887363836699\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model, train_dataloader, eval_dataloader, optimizer, restore_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c306b6-c31a-4e8c-97b1-cb47c8a52a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = \"./savemodel/\"\n",
    "# save_path = f'models/{model.__class__.__name__}_add10b.pt'\n",
    "\n",
    "# # Only show the progress bar once on each machine.\n",
    "# progress_bar = tqdm(\n",
    "#     range(max_train_steps), disable=not accelerator.is_local_main_process\n",
    "# )\n",
    "# completed_steps = 0\n",
    "# best_val_perplexity = float(\"inf\")\n",
    "\n",
    "# for epoch in range(num_train_epochs):\n",
    "#     model.train()\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         loss = loss / gradient_accumulation_steps\n",
    "#         loss = loss.reshape(-1)\n",
    "#         # accelerator.backward(loss)\n",
    "#         if (\n",
    "#             step % gradient_accumulation_steps == 0\n",
    "#             or step == len(train_dataloader) - 1\n",
    "#         ):\n",
    "#             # Perform one optimization step with the PrivacyEngine\n",
    "#             optimizer.step(loss=loss)\n",
    "#             lr_scheduler.step()\n",
    "#             # optimizer.zero_grad()\n",
    "#             progress_bar.update(1)\n",
    "#             completed_steps += 1\n",
    "\n",
    "#         if completed_steps >= max_train_steps:\n",
    "#             break\n",
    "\n",
    "#     model.eval()\n",
    "#     losses = []\n",
    "#     for step, batch in enumerate(eval_dataloader):\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**batch)\n",
    "\n",
    "#         loss = outputs.loss\n",
    "#         losses.append(\n",
    "#             accelerator.gather(loss.repeat(per_device_eval_batch_size))\n",
    "#         )\n",
    "\n",
    "#     losses = torch.cat(losses)\n",
    "#     losses = losses[: len(small_eval_dataset)]\n",
    "#     try:\n",
    "#         perplexity = math.exp(torch.mean(losses))\n",
    "#     except OverflowError:\n",
    "#         perplexity = float(\"inf\")\n",
    "\n",
    "#     # logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "#     print(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "\n",
    "#     # Printing epsilon from opacus privacy engine at the end of each epoch\n",
    "#     eps, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "#     print(\"End of epoch {}, we have epsilon {} for alpha {}\".format(epoch, eps, alpha))\n",
    "\n",
    "#     if perplexity < best_val_perplexity and output_dir is not None:\n",
    "#         best_val_perplexity = perplexity\n",
    "#     #     accelerator.wait_for_everyone()\n",
    "#     #     unwrapped_model = accelerator.unwrap_model(model)\n",
    "#     #     unwrapped_model.save_pretrained(\n",
    "#     #         output_dir, save_function=accelerator.save\n",
    "#     #     )\n",
    "#         # logger.info(\n",
    "#         #     f\"saved model! epoch {epoch}: perplexity: {best_val_perplexity}\"\n",
    "#         # )\n",
    "#         print(f\"saved model! epoch {epoch}: perplexity: {best_val_perplexity}\")\n",
    "#         torch.save(model.state_dict(), save_path)\n",
    "#         # tokenizer.save_pretrained(output_dir)\n",
    "#         # if accelerator.is_main_process:\n",
    "#         #     # tokenizer.save_pretrained(output_dir)\n",
    "#         #     if push_to_hub:\n",
    "#         #         repo.push_to_hub(\n",
    "#         #             commit_message=\"Best val perplexity\", auto_lfs_prune=True\n",
    "#         #         )\n",
    "\n",
    "#     # if push_to_hub and epoch < num_train_epochs - 1:\n",
    "#     #     accelerator.wait_for_everyone()\n",
    "#     #     unwrapped_model = accelerator.unwrap_model(model)\n",
    "#     #     unwrapped_model.save_pretrained(\n",
    "#     #         output_dir, save_function=accelerator.save\n",
    "#     #     )\n",
    "#     #     if accelerator.is_main_process:\n",
    "#     #         tokenizer.save_pretrained(output_dir)\n",
    "#     #         repo.push_to_hub(\n",
    "#     #             commit_message=f\"Training in progress epoch {epoch}\",\n",
    "#     #             blocking=False,\n",
    "#     #             auto_lfs_prune=True,\n",
    "#     #         )\n",
    "\n",
    "#     # if epoch == (num_train_epochs - 1):\n",
    "#     #     save_fir = output_dir + f\"_epoch_{num_train_epochs - 1}\"\n",
    "#     #     accelerator.wait_for_everyone()\n",
    "#     #     unwrapped_model = accelerator.unwrap_model(model)\n",
    "#     #     unwrapped_model.save_pretrained(save_fir, save_function=accelerator.save)\n",
    "#     #     tokenizer.save_pretrained(save_fir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa0a0e-30d9-47ae-9f6b-1c33187d6e50",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b670759-2f28-43f7-9ad4-47ab1474ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = f'models/{model.__class__.__name__}_add10b.pt'\n",
    "# model.load_state_dict(torch.load(save_path,  map_location=device))\n",
    "# perplexity = evaluate(model, test_dataloader)\n",
    "# print(f'Test Perplexity: {perplexity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc9ebe-4cf9-4317-bdc6-739991b53838",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cbc4c68-5629-4bb3-b8e0-8ce2356e045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator\n",
    ")\n",
    "from itertools import chain\n",
    "\n",
    "# Load the trained model\n",
    "# model_path = 'dp-gpt2-clm-model.pth'\n",
    "save_path = f'models/{model.__class__.__name__}_add10b_nodp.pt'\n",
    "\n",
    "model_checkpoint = \"distilgpt2\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2385acc-efd7-4e5f-a6d1-7db4cbf14d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a92f16-6ff2-4843-a6c4-8200bceb6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = tokenizer.encode('My ID is ', return_tensors='pt').to(device)\n",
    "# input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d63bdfe-ae9c-4736-b092-da25c010327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_interval = 10\n",
    "# max_seq_len = 200\n",
    "# temperature = 1\n",
    "\n",
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, device, seed=None):\n",
    "    tokens = \"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with open('nodp-distillgpt2-generated.txt', 'w') as output_files:\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # no tracking history\n",
    "            for i in range(max_seq_len):\n",
    "                \n",
    "                output = model(input_ids)\n",
    "                word_weights = output[0].squeeze().div(temperature).exp().cpu()\n",
    "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "                word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
    "                input = torch.cat([input_ids, word_tensor], 1)\n",
    "    \n",
    "                word = tokenizer.decode(word_idx)\n",
    "                tokens = tokens + word + ('\\n' if i % 20 == 19 else '')\n",
    "                output_files.write(word + ('\\n' if i % 20 == 19 else ''))\n",
    "    \n",
    "                # if i % log_interval == 0:\n",
    "                #     print('| Generated {}/{} words'.format(i, max_seq_len))\n",
    "                \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f039560-e5d2-470b-b0a3-9b03ec43b2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      " at to to at. at to a at, inen to,, in, in with,\n",
      " = at. at, @ in and to caused =, = in, to. to in in\n",
      " in to at at in =,, in a to at in at to to to, at to\n",
      ",. at to, at to, to at to = at to at,, at at at\n",
      " =, with to in in.,, to in =,, at, in, in at\n",
      "\n",
      "\n",
      "0.7\n",
      " at. to at.in to a at a inen to a, in, in with,\n",
      " = at.ed, @ in and < caused =, = in, to before to in in\n",
      " in to at at in = on, in its to ham of @ to to to, at to\n",
      " in. over to in at to, to attained into = at to aters, at at at\n",
      " = of with to in in. <, he < =, about at which in, in at\n",
      "\n",
      "\n",
      "0.75\n",
      " at. to at.in to a at a inen to a, in, in with,\n",
      " = at.ed billboard @ in and < caused =, = in, to before to in in\n",
      " in to at at in = on, in its to ham of @ to to to, at to\n",
      " in. over to in at to, to attained into = at to atn, at ; at\n",
      " = of with to with in. <, he < =, about at which in, in at\n",
      "\n",
      "\n",
      "0.8\n",
      " at. to at. tourism to a at later inen to a, in, in with,\n",
      " = at.ed billboard @ in and < caused =, responses hur, to before to in in\n",
      " in to at at aer = on, in its to ham of @ to to to, at to\n",
      " in. over to in at to, after attained into = at to 'n, at ; at\n",
      " = of with to with in. ton, amounted < =, about at whichiy, in at\n",
      "\n",
      "\n",
      "1.0\n",
      " mar. tobecause. tourism to a high cells relationshipen to a 11 in ancestry in with x\n",
      " national 2 effortsed billboard @ pre and beyond caused across, responses hur gal to before pa in.\n",
      "rand to worked at aer over on, in its to ham of @ designated to burial, at to\n",
      " in. over to in fant to of after attained into 1 at seed 'n hoped at thereafter at\n",
      " = of with mainly use he. ton, amounted < = taking about at whichiy, inel\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'my id is'\n",
    "max_seq_len = 100\n",
    "seed = 0\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, device, seed)\n",
    "    print(f'{str(temperature)}\\n{generation}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97afe37-1c64-4501-86a9-5c6f834ee18e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
