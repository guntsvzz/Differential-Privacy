{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dfaaa2d-2f19-4f72-8bed-d86eb10dd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment this if you are not using AIT proxy...\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c28875a-2da0-4a37-8bad-00cf110c1061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf35958-077b-4563-90a4-01084742dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import open\n",
    "import torch\n",
    "import json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad5c2fc-6fed-43a8-830d-69a5bfe6e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbc352-6296-481c-aafa-df0fe8b0d63c",
   "metadata": {},
   "source": [
    "## 1.Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5677b1-34b1-49dd-b6df-f7ff99c87082",
   "metadata": {},
   "source": [
    "### Preprocessing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e348a0b-e428-4f8b-8a8f-17ae55a45c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c449fcf3-1128-4d87-ac66-307a5c4a6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "# PAD_TOKEN = '<pad>'\n",
    "# tokenizer.add_special_tokens({'pad_token': PAD_TOKEN})\n",
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f14f80-62fa-4fb1-aba5-437253dadac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wikitext_Dataset:\n",
    "    def __init__(self, path):\n",
    "        self.train = os.path.join(path, 'train/train.txt')\n",
    "        self.valid = os.path.join(path, 'valid/valid.txt')\n",
    "        self.test  = os.path.join(path, 'test/test.txt')\n",
    "\n",
    "    def build_corpus(self, path):\n",
    "        files = open(path,'r')\n",
    "        lines = []\n",
    "        for line in files:\n",
    "            line = line.strip().lower()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            lines.append(line)\n",
    "        return lines\n",
    "path_files = './data/wikitext-2-add10b'\n",
    "corpus = Wikitext_Dataset(path_files)\n",
    "train_dataset = corpus.build_corpus(corpus.train)\n",
    "valid_dataset = corpus.build_corpus(corpus.valid)\n",
    "test_dataset  = corpus.build_corpus(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4695bc39-2485-43d3-b157-795cf36b1b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 23777\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2461\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2891\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "raw_datasets_train = Dataset.from_pandas(pd.DataFrame(data = {'text': train_dataset}))\n",
    "raw_datasets_valid = Dataset.from_pandas(pd.DataFrame(data = {'text': valid_dataset}))\n",
    "raw_datasets_test  = Dataset.from_pandas(pd.DataFrame(data = {'text': test_dataset}))\n",
    "#remove .shuffle if you want to train the whole dataset....\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        'train':raw_datasets_train,\n",
    "        'validation':raw_datasets_valid,\n",
    "        'test':raw_datasets_test\n",
    "    }\n",
    ")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb57088-b3b7-4be0-9977-8f1157626113",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26741d7f-f7fd-4841-b99a-fdc6cd48c80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 23777\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 2461\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 2891\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we tokenize all the texts.\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "# def tokenize_function(example):\n",
    "#     outputs =  tokenizer(example[text_column_name], truncation=True, padding='max_length')\n",
    "#     input_batch = []\n",
    "#     for input_ids in outputs[\"input_ids\"]:\n",
    "#         input_batch.append(input_ids)\n",
    "#     return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "preprocessing_num_workers = None\n",
    "with accelerator.main_process_first():\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb2b6b95-f6d0-496b-9b5e-e407d3defe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 1024\n",
    "if block_size is None:\n",
    "    block_size = tokenizer.model_max_length\n",
    "    if block_size > 1024:\n",
    "        # logger.warning(\n",
    "        #     f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "        #     \"Picking 1024 instead. You can change that default value by passing --block_size xxx.\"\n",
    "        # )\n",
    "        block_size = 1024\n",
    "else:\n",
    "    if block_size > tokenizer.model_max_length:\n",
    "        # logger.warning(\n",
    "        #     f\"The block_size passed ({args.block_size}) is larger than the maximum length for the model\"\n",
    "        #     f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        # )\n",
    "        block_size = min(block_size, tokenizer.model_max_length)\n",
    "    \n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49f33cbf-1736-4fb6-b53a-4c43de02f8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2405\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 255\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 290\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n",
    "# # for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n",
    "# # to preprocess.\n",
    "# #\n",
    "# # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "preprocessing_num_workers = 1\n",
    "with accelerator.main_process_first():\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=preprocessing_num_workers,\n",
    "        desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    )\n",
    "lm_datasets.set_format(\"torch\")\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87e1364-08f9-4843-955c-328bfab5bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = lm_datasets[\"train\"].shuffle(seed=55) #.select(range(10))\n",
    "small_eval_dataset = lm_datasets[\"validation\"].shuffle(seed=55) #.select(range(10))\n",
    "small_test_dataset = lm_datasets[\"test\"].shuffle(seed=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa8573-15f6-4a1d-ab30-942a0ccd0074",
   "metadata": {},
   "source": [
    "## 3. Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e56875b-ad57-4d47-b414-b35c9086ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=per_device_train_batch_size, pin_memory=True)\n",
    "val_dataloader = DataLoader(small_eval_dataset, batch_size=per_device_eval_batch_size, pin_memory=True)\n",
    "test_dataloader = DataLoader(small_test_dataset, batch_size=per_device_eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89f5c3e1-b546-41ee-903b-fe109763229c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024]) torch.Size([4, 1024])\n",
      "torch.Size([4, 1024]) torch.Size([4, 1024])\n",
      "torch.Size([4, 1024]) torch.Size([4, 1024])\n"
     ]
    }
   ],
   "source": [
    "#checking chucking\n",
    "for i in train_dataloader:\n",
    "    print(i['input_ids'].shape, i['labels'].shape)\n",
    "    break\n",
    "for i in val_dataloader:\n",
    "    print(i['input_ids'].shape, i['labels'].shape)\n",
    "    break\n",
    "for i in test_dataloader:\n",
    "    print(i['input_ids'].shape, i['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7562a75d-8e10-4596-a4cf-ee30ea7d46cc",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30273d86-90b7-4770-8f7f-58cfd7c877ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained(model_checkpoint, tie_word_embeddings=False)\n",
    "# model = AutoModelForCausalLM.from_config(config)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f95deac2-18b4-44ad-8319-7154f348ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45b4dfc0-a038-46f8-9cb3-9c2b50f71d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82bc059f-b92c-4526-8b5d-f0aa1ff88fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1600)\n",
       "    (wpe): Embedding(1024, 1600)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a configuration for a 48-layer GPT2 model\n",
    "teacher = 'gpt2-xl'\n",
    "teacher_config = AutoConfig.from_pretrained(teacher, tie_word_embeddings=False)\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    teacher,\n",
    "    config=teacher_config,\n",
    ")\n",
    "teacher_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99e46760-0cc5-4fed-8a18-fc2be303b0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a configuration for a 12-layer GPT2 model\n",
    "student = \"gpt2\"\n",
    "student_config = AutoConfig.from_pretrained(student, tie_word_embeddings=False)\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\n",
    "    student,\n",
    "    config=student_config,\n",
    ")\n",
    "student_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7911fa8f-434e-4e5d-b006-f26d6749efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "weight_decay = 0\n",
    "teacher_optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in teacher_model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in teacher_model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "student_optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in student_model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in student_model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "# params=model.parameters()\n",
    "teacher_optimizer = torch.optim.Adam(teacher_optimizer_grouped_parameters, lr=1e-4)\n",
    "student_optimizer = torch.optim.Adam(student_optimizer_grouped_parameters, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e24aa5-54ff-401f-8a9d-5faf85e2951a",
   "metadata": {},
   "source": [
    "## Accelator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8efcca86-4044-46f8-9b53-102e989b4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare everything with our `accelerator`.\n",
    "# model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "#     model, optimizer, train_dataloader, eval_dataloader\n",
    "# )\n",
    "teacher_model = accelerator.prepare(teacher_model)\n",
    "student_model, student_optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
    "    student_model, student_optimizer, train_dataloader, val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e16bb899-22e0-4b23-8277-997e098aed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "import math\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / gradient_accumulation_steps\n",
    "    )\n",
    "num_train_epochs = 10\n",
    "max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "teacher_lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=teacher_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_train_steps,\n",
    ")\n",
    "\n",
    "student_lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=student_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=max_train_steps,\n",
    ")\n",
    "\n",
    "total_batch_size = (\n",
    "        per_device_train_batch_size\n",
    "        * accelerator.num_processes\n",
    "        * gradient_accumulation_steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9ac844-07e3-4e07-ba18-af682334e941",
   "metadata": {},
   "source": [
    "## Ghost clipping: memory saving differentially private learning\n",
    "Turning on ghost clipping requires changing only 1 line. You should notice a drastic reduction in peak GPU memory usage once this is turned on, at a potential cost of slower training speed. One might find this especially useful when constrained to only use older GPUs with small VRAMs or fitting super large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0491606b-a435-4762-a64d-3dfbf32ae52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ml_swissknife\n",
    "# !pip install opt_einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a38adf0c-31be-49f3-9492-9beda48b6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, torch\n",
    "from private_transformers import PrivacyEngine\n",
    "dp = False\n",
    "if dp == True:\n",
    "    #student_model\n",
    "    privacy_engine = PrivacyEngine(\n",
    "        student_model,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "        sample_size=len(lm_datasets['train']),\n",
    "        epochs=1,\n",
    "        max_grad_norm=0.1,\n",
    "        target_epsilon=3,\n",
    "        clipping_mode=\"ghost\",  # The only change you need to make!\n",
    "    )\n",
    "    privacy_engine.attach(student_optimizer)\n",
    "    #Teacher Model\n",
    "    privacy_engine = PrivacyEngine(\n",
    "        teacher_model,\n",
    "        batch_size=per_device_train_batch_size,\n",
    "        sample_size=len(lm_datasets['train']),\n",
    "        epochs=per_device_train_batch_size,\n",
    "        max_grad_norm=0.1,\n",
    "        target_epsilon=3,\n",
    "        clipping_mode=\"ghost\",  # The only change you need to make!\n",
    "    )\n",
    "    privacy_engine.attach(teacher_optimizer)\n",
    "\n",
    "else :\n",
    "    privacy_engine = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bc08c72-7595-441d-a856-3fa7140d4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40cb8a11-ffee-48e5-810e-9399d0e31281",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1.0/42061 # We instead use the accountant from Gopi et al. (2021) as described in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd5fa8-9915-4dbc-a8bd-d1a607e95da8",
   "metadata": {},
   "source": [
    "### Loss Objective \n",
    "The Kullback-Leibler divergence loss. For tensors of the same shape $y_{pred}, y_{true}$ where $y_{pred}$ is the input and $y_{true}$ ​ is the target, we define the pointwise KL-divergence as \n",
    "\n",
    "$$L(y_{pred}, y_{true}) = y_{pred}\\cdot \\log \\frac{y_{true}}{y_{pred}}  = y_{true} \\cdot (\\log y_{true} -\\log y_{true})$$\n",
    "\n",
    "format : torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)\n",
    "more infomation click [link](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fb8f25b-2d3c-447b-b1c2-0fe1dba45dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(student_outputs, labels, teacher_outputs, alpha = 0.9, T = 1):\n",
    "    \"\"\"\n",
    "    Compute the knowledge-distillation (KD) loss given outputs, labels.\n",
    "    \"Hyperparameters\": temperature and alpha\n",
    "\n",
    "    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher\n",
    "    and student expects the input tensor to be log probabilities! See Issue #2\n",
    "    \"\"\"\n",
    "    # student_outputs.logits.shape = (batch_size, class)\n",
    "    # teacher_outputs.logits.shape = (batch_size, class)\n",
    "    # labels.shape = (batch_size, )\n",
    "    \n",
    "    loss_fn = nn.KLDivLoss(reduction=\"none\")\n",
    "    kd_loss = loss_fn(F.log_softmax(student_outputs.logits/T, dim=1),\n",
    "                             F.softmax(teacher_outputs.logits/T, dim=1) * (T ** 2)) #(batch_size, labels)\n",
    "    \n",
    "    kd_loss = kd_loss.mean(dim=1) #(batch_size, )\n",
    "    CELoss = F.cross_entropy(student_outputs.logits, labels, reduction=\"none\") #.mean(dim=1) #(batch_size, )\n",
    "    total_losses = alpha * CELoss + (1. - alpha) * kd_loss\n",
    "\n",
    "    # total_losses = CELoss + alpha * kd_loss\n",
    "    return total_losses.mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4246ffd-21f0-4fab-9436-860f06807136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverage():\n",
    "    \"\"\"A simple class that maintains the running average of a quantity\n",
    "    \n",
    "    Example:\n",
    "    ```\n",
    "    loss_avg = RunningAverage()\n",
    "    loss_avg.update(2)\n",
    "    loss_avg.update(4)\n",
    "    loss_avg() = 3\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.total = 0\n",
    "    \n",
    "    def update(self, val):\n",
    "        self.total += val\n",
    "        self.steps += 1\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.total/float(self.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "379c6dcc-0232-4501-9ef2-bfbe3878ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining train_kd & train_and_evaluate_kd functions\n",
    "def train_kd(student_model, teacher_model, optimizer, loss_fn_kd, train_dataloader):\n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "    \n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = RunningAverage()\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        outputs_student_batch = student_model(**batch)\n",
    "\n",
    "        # get one batch output from teacher_outputs list\n",
    "        with torch.no_grad():\n",
    "            output_teacher_batch = teacher_model(**batch)\n",
    "        \n",
    "        loss = outputs_student_batch.loss\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss = loss.reshape(-1)\n",
    "        # accelerator.backward(loss)\n",
    "        if (\n",
    "            step % gradient_accumulation_steps == 0\n",
    "            or step == len(train_dataloader) - 1\n",
    "        ):\n",
    "            # Perform one optimization step with the PrivacyEngine\n",
    "            if dp:\n",
    "                optimizer.step(loss=loss)\n",
    "            else:\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "            student_lr_scheduler.step()\n",
    "            # optimizer.zero_grad()\n",
    "            # progress_bar.update(1)\n",
    "            # completed_steps += 1\n",
    "\n",
    "        # if completed_steps >= max_train_steps:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adaabe00-7713-41af-83cd-9b091c492044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kd(model, eval_dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(\n",
    "            accelerator.gather(loss.repeat(per_device_eval_batch_size))\n",
    "        )\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(small_eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ca82643-f339-4c3e-8c95-2e00798fa9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_kd(student_model, teacher_model, train_dataloader, val_dataloader, optimizer, save_path, restore_file=None):\n",
    "    # Only show the progress bar once on each machine.\n",
    "    # progress_bar = tqdm(\n",
    "    #     range(max_train_steps), disable=not accelerator.is_local_main_process\n",
    "    # )\n",
    "    # completed_steps = 0\n",
    "    best_val_perplexity = float(\"inf\")\n",
    "    \n",
    "    for epoch in range(num_train_epochs):\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        train_kd(student_model, teacher_model, optimizer, loss_fn_kd, train_dataloader)\n",
    "        \n",
    "        # Evaluate for one epoch on validation set\n",
    "        perplexity = evaluate_kd(student_model, val_dataloader)\n",
    "\n",
    "        # logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "        print(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "    \n",
    "        if dp:\n",
    "            # Printing epsilon from opacus privacy engine at the end of each epoch\n",
    "            eps, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "            print(\"End of epoch {}, we have epsilon {} for alpha {}\".format(epoch, eps, alpha))\n",
    "    \n",
    "        if perplexity < best_val_perplexity and save_path is not None:\n",
    "            best_val_perplexity = perplexity\n",
    "            \n",
    "            print(f\"saved model! epoch {epoch}: perplexity: {best_val_perplexity}\")\n",
    "            torch.save(student_model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d9b0b47-5cc4-4380-b998-9e8ff458519e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602/602 [15:12<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: perplexity: 62.039091960351925\n",
      "saved model! epoch 0: perplexity: 62.039091960351925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602/602 [15:10<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: perplexity: 35.098700946666845\n",
      "saved model! epoch 1: perplexity: 35.098700946666845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602/602 [15:10<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: perplexity: 27.92503402467137\n",
      "saved model! epoch 2: perplexity: 27.92503402467137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602/602 [15:11<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: perplexity: 24.751212161435863\n",
      "saved model! epoch 3: perplexity: 24.751212161435863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602/602 [15:12<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: perplexity: 23.0325412823581\n",
      "saved model! epoch 4: perplexity: 23.0325412823581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602/602 [15:13<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: perplexity: 22.058142592249506\n",
      "saved model! epoch 5: perplexity: 22.058142592249506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602/602 [15:14<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: perplexity: 21.46707685688\n",
      "saved model! epoch 6: perplexity: 21.46707685688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602/602 [15:13<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: perplexity: 21.187855697136396\n",
      "saved model! epoch 7: perplexity: 21.187855697136396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602/602 [15:11<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: perplexity: 20.97347257683864\n",
      "saved model! epoch 8: perplexity: 20.97347257683864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602/602 [15:10<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: perplexity: 20.950063469817046\n",
      "saved model! epoch 9: perplexity: 20.950063469817046\n"
     ]
    }
   ],
   "source": [
    "save_path = f'models/{student_model.__class__.__name__}_distill_nodp.pt'\n",
    "train_and_evaluate_kd(student_model, teacher_model, train_dataloader, val_dataloader, student_optimizer, save_path, restore_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78c306b6-c31a-4e8c-97b1-cb47c8a52a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = \"./savemodel/\"\n",
    "# save_path = f'models/{model.__class__.__name__}_add10b.pt'\n",
    "\n",
    "# # Only show the progress bar once on each machine.\n",
    "# progress_bar = tqdm(\n",
    "#     range(max_train_steps), disable=not accelerator.is_local_main_process\n",
    "# )\n",
    "# completed_steps = 0\n",
    "# best_val_perplexity = float(\"inf\")\n",
    "\n",
    "# for epoch in range(num_train_epochs):\n",
    "#     model.train()\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         loss = loss / gradient_accumulation_steps\n",
    "#         loss = loss.reshape(-1)\n",
    "#         # accelerator.backward(loss)\n",
    "#         if (\n",
    "#             step % gradient_accumulation_steps == 0\n",
    "#             or step == len(train_dataloader) - 1\n",
    "#         ):\n",
    "#             # Perform one optimization step with the PrivacyEngine\n",
    "#             optimizer.step(loss=loss)\n",
    "#             lr_scheduler.step()\n",
    "#             # optimizer.zero_grad()\n",
    "#             progress_bar.update(1)\n",
    "#             completed_steps += 1\n",
    "\n",
    "#         if completed_steps >= max_train_steps:\n",
    "#             break\n",
    "\n",
    "#     model.eval()\n",
    "#     losses = []\n",
    "#     for step, batch in enumerate(eval_dataloader):\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**batch)\n",
    "\n",
    "#         loss = outputs.loss\n",
    "#         losses.append(\n",
    "#             accelerator.gather(loss.repeat(per_device_eval_batch_size))\n",
    "#         )\n",
    "\n",
    "#     losses = torch.cat(losses)\n",
    "#     losses = losses[: len(small_eval_dataset)]\n",
    "#     try:\n",
    "#         perplexity = math.exp(torch.mean(losses))\n",
    "#     except OverflowError:\n",
    "#         perplexity = float(\"inf\")\n",
    "\n",
    "#     # logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "#     print(f\"epoch {epoch}: perplexity: {perplexity}\")\n",
    "\n",
    "#     # Printing epsilon from opacus privacy engine at the end of each epoch\n",
    "#     eps, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "#     print(\"End of epoch {}, we have epsilon {} for alpha {}\".format(epoch, eps, alpha))\n",
    "\n",
    "#     if perplexity < best_val_perplexity and output_dir is not None:\n",
    "#         best_val_perplexity = perplexity\n",
    "#     #     accelerator.wait_for_everyone()\n",
    "#     #     unwrapped_model = accelerator.unwrap_model(model)\n",
    "#     #     unwrapped_model.save_pretrained(\n",
    "#     #         output_dir, save_function=accelerator.save\n",
    "#     #     )\n",
    "#         # logger.info(\n",
    "#         #     f\"saved model! epoch {epoch}: perplexity: {best_val_perplexity}\"\n",
    "#         # )\n",
    "#         print(f\"saved model! epoch {epoch}: perplexity: {best_val_perplexity}\")\n",
    "#         torch.save(model.state_dict(), save_path)\n",
    "#         # tokenizer.save_pretrained(output_dir)\n",
    "#         # if accelerator.is_main_process:\n",
    "#         #     # tokenizer.save_pretrained(output_dir)\n",
    "#         #     if push_to_hub:\n",
    "#         #         repo.push_to_hub(\n",
    "#         #             commit_message=\"Best val perplexity\", auto_lfs_prune=True\n",
    "#         #         )\n",
    "\n",
    "#     # if push_to_hub and epoch < num_train_epochs - 1:\n",
    "#     #     accelerator.wait_for_everyone()\n",
    "#     #     unwrapped_model = accelerator.unwrap_model(model)\n",
    "#     #     unwrapped_model.save_pretrained(\n",
    "#     #         output_dir, save_function=accelerator.save\n",
    "#     #     )\n",
    "#     #     if accelerator.is_main_process:\n",
    "#     #         tokenizer.save_pretrained(output_dir)\n",
    "#     #         repo.push_to_hub(\n",
    "#     #             commit_message=f\"Training in progress epoch {epoch}\",\n",
    "#     #             blocking=False,\n",
    "#     #             auto_lfs_prune=True,\n",
    "#     #         )\n",
    "\n",
    "#     # if epoch == (num_train_epochs - 1):\n",
    "#     #     save_fir = output_dir + f\"_epoch_{num_train_epochs - 1}\"\n",
    "#     #     accelerator.wait_for_everyone()\n",
    "#     #     unwrapped_model = accelerator.unwrap_model(model)\n",
    "#     #     unwrapped_model.save_pretrained(save_fir, save_function=accelerator.save)\n",
    "#     #     tokenizer.save_pretrained(save_fir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa0a0e-30d9-47ae-9f6b-1c33187d6e50",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b670759-2f28-43f7-9ad4-47ab1474ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = f'models/{model.__class__.__name__}_add10b.pt'\n",
    "# model.load_state_dict(torch.load(save_path,  map_location=device))\n",
    "# perplexity = evaluate(model, test_dataloader)\n",
    "# print(f'Test Perplexity: {perplexity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc9ebe-4cf9-4317-bdc6-739991b53838",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cbc4c68-5629-4bb3-b8e0-8ce2356e045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator\n",
    ")\n",
    "from itertools import chain\n",
    "\n",
    "# Load the trained model\n",
    "# model_path = 'dp-gpt2-clm-model.pth'\n",
    "model_checkpoint = \"gpt2\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "student_model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "save_path = f'models/{student_model.__class__.__name__}_distill_nodp.pt'\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2385acc-efd7-4e5f-a6d1-7db4cbf14d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a92f16-6ff2-4843-a6c4-8200bceb6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = tokenizer.encode('My ID is ', return_tensors='pt').to(device)\n",
    "# input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d63bdfe-ae9c-4736-b092-da25c010327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_interval = 10\n",
    "# max_seq_len = 200\n",
    "# temperature = 1\n",
    "\n",
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, device, seed=None):\n",
    "    tokens = \"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with open('nodp-distill-gpt2-generated.txt', 'w') as output_files:\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # no tracking history\n",
    "            for i in range(max_seq_len):\n",
    "                \n",
    "                output = model(input_ids)\n",
    "                word_weights = output[0].squeeze().div(temperature).exp().cpu()\n",
    "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "                word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
    "                input = torch.cat([input_ids, word_tensor], 1)\n",
    "    \n",
    "                word = tokenizer.decode(word_idx)\n",
    "                tokens = tokens + word + ('\\n' if i % 20 == 19 else '')\n",
    "                output_files.write(word + ('\\n' if i % 20 == 19 else ''))\n",
    "    \n",
    "                # if i % log_interval == 0:\n",
    "                #     print('| Generated {}/{} words'.format(i, max_seq_len))\n",
    "            # print(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f039560-e5d2-470b-b0a3-9b03ec43b2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      " by. toston. polls < aica < 358 < to aem in is, withl\n",
      " national 2.ed billboard @ @ and < 2008ason,jo <ay to by. ind '\n",
      " @ to is at @ the on, in a to @ of @ @ tob,, @\n",
      ",. and to,>, of < attained < 1acre < 'n andomed '\n",
      " another of the ' @ @ < <, by < a <iges whichiy < only <\n",
      "\n",
      "\n",
      "0.7\n",
      " marruary jston was pollsca asica cells 358en production aemique aug,ochl\n",
      " national 2.ed billboard @ pre andb 2008ason,jo <ay to by now ind '\n",
      " @ to is at aer over on inn in bed to ham of @ road to burial, at @\n",
      ",.e tots> emerges of grade attained 7 1acre seed hourn hopedomed38\n",
      " another ofie mainly use one and ton dag amounted < matt takingiges whichiy <uhel\n",
      "\n",
      "\n",
      "0.75\n",
      " marruary jston was pollsca asica cells 358en production massemique aug,ochl\n",
      " national 2.ed billboard @ pre andb alarmason redjo huray corresponds by now ind '\n",
      " @ to cup department aer overicon inn in bed to ham aug @ roadf burial, at @\n",
      ",.e tots> emerges of grade attained 7 1acre seed hourn hopedomed38\n",
      " another ofie mainly use one known ton dag amounted < matt takingiges whichiy <uhel\n",
      "\n",
      "\n",
      "0.8\n",
      " marruary jston was pollsca asica cells 358 foundations production massemique aug casochl\n",
      " national 2 effortsed billboard @ pre andb alarmason redjo huray corresponds by now ind '\n",
      "sym to worked department aer overicon inn clothes bed to ham aug @ marine identified burialg ath\n",
      ",wheree tots> emerges honour grade attained 7 1acre seed hourn hopedomed38\n",
      "� ofie mainly use oneitzerland ton dag amounted < matt takingiges whichiy <uhel\n",
      "\n",
      "\n",
      "1.0\n",
      " marruary encountersston contributing pollsca mustica cells 358 foundations production massemique telescopes casochl\n",
      " nationalful effortsed billboard @ pre andb alarmason redjo hur hours corresponds by deliver ind arrangement\n",
      "sym to worked department aer affiliationicon inn clothes bed pyramid ham domains @ marine identified burialgho imagery\n",
      ",wheree tots> emerges honour resign attained puzzled 1acre seed hourn hopedomed38\n",
      "� practice 117 mainly use oneitzerland ton dag amountedind matt taking detectores whichiy photuhel\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'I am going to'\n",
    "max_seq_len = 100\n",
    "seed = 0\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, device, seed)\n",
    "    print(f'{str(temperature)}\\n{generation}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e99fd-e452-4407-b05e-4dfaee0310a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c2e85-d255-427d-bd16-7feeed68f306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
