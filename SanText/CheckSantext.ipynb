{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e8bc23-ef21-40a4-99cb-19f8a5d95366",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GPTModify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f199e51-b170-40c3-af0c-75de498749e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import GPT2Model, GPT2Config\n",
    "\n",
    "# class CustomGPT2Model(GPT2Model):\n",
    "#     def __init__(self, config):\n",
    "#         super(CustomGPT2Model, self).__init__(config)\n",
    "#         # Create your custom embedding layer\n",
    "#         self.wpv = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "#         # Initialize the weights as needed\n",
    "        \n",
    "#     def forward(self, input_ids, pv_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
    "#         # Run the base GPT-2 forward pass\n",
    "#         gpt2_output = super().forward(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask)\n",
    "        \n",
    "#         # Get the embeddings from the GPT-2 output\n",
    "#         gpt2_embeddings = gpt2_output.last_hidden_state\n",
    "        \n",
    "#         # Apply your custom embedding\n",
    "#         pv_embedded = self.wpv(pv_ids)\n",
    "        \n",
    "#         # Concatenate or combine your custom embeddings with the GPT-2 embeddings\n",
    "#         combined_embeddings = gpt2_embeddings + pv_embedded#torch.cat([gpt2_embeddings, pv_embedded], dim=1)\n",
    "        \n",
    "#         return combined_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6ad2912-403a-400a-89cb-501d3c9f4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained('gpt2') # Load the GPT-2 configuration\n",
    "# model = CustomGPT2Model(config)  # Instantiate your custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2df06849-3e83-42e5-ac32-ba5feeef0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71ff8629-34f2-4a69-9574-93903e1d2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Provide input to both GPT-2 and your custom embedding\n",
    "# input_ids = torch.tensor([[1, 2, 3]])  # Example input_ids for GPT-2\n",
    "# pv_ids = torch.tensor([[4, 5, 6]])  # Example custom input_ids\n",
    "\n",
    "# output = model(input_ids, custom_input_ids)\n",
    "# output.shape #bs, seq_len, emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56bf099b-d3a2-4dbf-b842-c4f4af26f0b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class CustomGPT2Model(AutoModelForCausalLM):\n",
    "#     def __init__(self, config, *model_args, **kwargs):\n",
    "#         super(CustomGPT2Model).__init__(config, *model_args, **kwargs)\n",
    "#         # Create a custom embedding layer\n",
    "#         self.wpv = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "\n",
    "#     def forward(self, input_ids, pv_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
    "        \n",
    "#         # Run the base GPT-2 forward pass\n",
    "#         gpt2_output = super().forward(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask)\n",
    "        \n",
    "#         # Get the embeddings from the GPT-2 output\n",
    "#         gpt2_embeddings = gpt2_output.last_hidden_state\n",
    "        \n",
    "#         # Apply your custom embedding\n",
    "#         pv_embedded = self.wpv(pv_ids)\n",
    "#         combined_embeddings = gpt2_embeddings + pv_embedded#torch.cat([gpt2_embeddings, pv_embedded], dim=1)\n",
    "        \n",
    "#         return combined_embeddings\n",
    "        \n",
    "# config = AutoConfig.from_pretrained(\"gpt2\")  # Load the GPT-2 configuration\n",
    "# custom_model = CustomGPT2Model.from_config(config)\n",
    "# custom_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67a58849-cedf-44bd-abed-37ccc032a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode input texts\n",
    "# input_text = [\"This is an example sentence.\"]\n",
    "# custom_input_text = [\"Custom input text.\"]\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "# custom_input_ids = tokenizer(custom_input_text, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
    "# # Generate outputs using your custom model\n",
    "# outputs = custom_model(input_ids, input_ids=custom_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f750fea7-5457-454c-bc59-2e17129a9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the device to GPU if available\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# # from transformers import GPT2Model, GPT2Config\n",
    "# from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# class PrivateGPT2(AutoModelForCausalLM):\n",
    "#     def __init__(self, config):\n",
    "#         super(PrivateGPT2, self).__init__()\n",
    "#         self.embed_dim = config.hidden_size\n",
    "#         self.pv_embed    = nn.Embedding(config.vocab_size, self.embed_dim) \n",
    "#         self.transformer = GPT2Model(config)\n",
    "#         self.lm_head     = nn.Linear(self.embed_dim, config.vocab_size, bias=False)\n",
    "\n",
    "#     def forward(self, input_ids, private_mask=None, attention_mask=None, labels = None, noise_stddev = 0.2, noise = 'gaussian', device = device):\n",
    "#         # Get token embeddings from GPT-2\n",
    "#         input_embeded = self.transformer.wte(input_ids)\n",
    "#         pv_embeded    = self.pv_embed(input_ids)\n",
    "        \n",
    "#         if private_mask is not None:\n",
    "#             # Identify positions with value 1\n",
    "#             positions_with_noise = private_mask == 1 #bs, seq_len\n",
    "#             if noise == 'gaussian':\n",
    "#                 # Generate Gaussian noise\n",
    "#                 noise_gauss = torch.normal(mean=0.0, std=noise_stddev, size=pv_embeded.size()).to(device) #bs, seq_len, emb_dim\n",
    "#                 noise_pv = noise_gauss * positions_with_noise.unsqueeze(-1)  # Broadcasting noise to embeddings shape\n",
    "#             else:\n",
    "#                 ones = torch.ones(size=pv_embeded.size()).to(device) #bs, seq_len, emb_dim\n",
    "#                 noise_pv = ones * positions_with_noise.unsqueeze(-1)  # Broadcasting noise to embeddings shape\n",
    "#             # Add noise to the original embeddings\n",
    "#             pv_embeded = pv_embeded * noise_pv #element wise multiplication #bs, seq_len, emb_dim\n",
    "\n",
    "#         token_embeddings = input_embeded + pv_embeded\n",
    "#         # Pass through the rest of the GPT-2 model\n",
    "#         transformer_outputs = self.transformer(inputs_embeds=token_embeddings, attention_mask=attention_mask)\n",
    "#         hidden_states = transformer_outputs.last_hidden_state\n",
    "#         # print(hidden_states.shape)\n",
    "#         lm_logits = self.lm_head(hidden_states)\n",
    "        \n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             # move labels to correct device to enable model parallelism\n",
    "#             labels = labels.to(lm_logits.device)\n",
    "#             # Shift so that tokens < n predict n\n",
    "#             shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "#             shift_labels = labels[..., 1:].contiguous()\n",
    "#             # Flatten the tokens\n",
    "#             loss_fct = nn.CrossEntropyLoss()\n",
    "#             loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "#         # return {'loss': loss, 'logits': lm_logits} \n",
    "#         return loss, lm_logits \n",
    "        \n",
    "# # Example usage\n",
    "# config = GPT2Config.from_pretrained('gpt2')\n",
    "# model = PrivateGPT2.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b062e9e1-1e7d-456a-8e7b-6d5f6675ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b065893-4145-42b0-864f-38b3a5d83471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import spacy\n",
    "\n",
    "# class GPT2WithPrivacyEmbeddings:\n",
    "#     def __init__(self, model_name):\n",
    "#         self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "#         self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "#         self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#         # Add a privacy embedding layer\n",
    "#         self.privacy_embedding = nn.Embedding(2, self.model.config.hidden_size)\n",
    "\n",
    "#         # Define the types of named entities to consider as privacy-related\n",
    "#         self.privacy_entity_types = set([\n",
    "#             \"PERSON\", \"ORG\", \"GPE\", \"FAC\", \"LOC\"  # Add more as needed\n",
    "#         ])\n",
    "\n",
    "#     def generate_with_privacy(self, input_ids):\n",
    "#         # Convert input_ids back to text\n",
    "#         input_text = self.tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "#         # Perform NER on the input text\n",
    "#         doc = self.nlp(input_text)\n",
    "#         assert len(doc) == len(input_ids)\n",
    "        \n",
    "#         # Create privacy mask based on named entity types\n",
    "#         privacy_mask = [1 if token.ent_type_ in self.privacy_entity_types else 0 for token in doc]\n",
    "#         privacy_mask = torch.tensor(privacy_mask)\n",
    "\n",
    "#         privacy_embeds = self.privacy_embedding(privacy_mask)\n",
    "#         combined_embeds = self.model.transformer.wte(input_ids)\n",
    "\n",
    "#         # Apply privacy embeddings to corresponding positions\n",
    "#         privacy_indices = torch.arange(len(input_ids))[privacy_mask == 1]\n",
    "#         combined_embeds.index_add_(dim=1, index=privacy_indices.unsqueeze(0), source=privacy_embeds)\n",
    "\n",
    "#         outputs = self.model(inputs_embeds=combined_embeds)\n",
    "#         return outputs.last_hidden_state\n",
    "\n",
    "# # Example usage\n",
    "# gpt2_privacy = GPT2WithPrivacyEmbeddings('gpt2')\n",
    "\n",
    "# # Generate text with privacy embeddings\n",
    "# input_text = \"John Doe's address is 123 Main Street.\"\n",
    "# input_ids = gpt2_privacy.tokenizer.encode(input_text, add_special_tokens=True, return_tensors='pt').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b9bb68d-f729-439c-a962-e3f644785772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# privacy_entity_types = set([\n",
    "#             \"PERSON\", \"ORG\", \"GPE\", \"FAC\", \"LOC\"  # Add more as needed\n",
    "#         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f5e6903-90c2-4027-82a8-ea16f25a64b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# privacy_mask = [1 if token.ent_type_ in privacy_entity_types else 0 for token in doc]\n",
    "# privacy_mask = torch.tensor(privacy_mask)\n",
    "# privacy_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1dfae-6600-4047-b93a-4ad87e45d8c8",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b35ab9f7-b47b-4a62-b17b-08be4dd79268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "class Agrument:\n",
    "    def __init__(self):\n",
    "        self.bert_model_path = \"bert-base-uncased\"\n",
    "        self.data_dir =\"./data/SST-2/\"\n",
    "        self.sensitive_word_percentage = 1\n",
    "        self.epsilon = 15\n",
    "        \n",
    "args = Agrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1215b1d-6308-4f2c-bc36-ff6b721912ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer  = BertTokenizer.from_pretrained(args.bert_model_path)\n",
    "tokenizer_type = \"subword\"\n",
    "\n",
    "model=BertForMaskedLM.from_pretrained(args.bert_model_path)\n",
    "embedding_matrix = model.bert.embeddings.word_embeddings.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69089256-6df4-42f3-ad12-10b24f681ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ff5d623-3bb2-47c6-ada9-00459b5099b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape #vocab_size, emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a4a3d06-45d9-4160-86a5-7ea2d1d2dc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [00:04<00:00, 13806.92it/s]\n",
      "100%|██████████| 872/872 [00:00<00:00, 7907.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "def get_vocab_SST2(data_dir,tokenizer,tokenizer_type=\"subword\"):\n",
    "    vocab=Counter()\n",
    "    for split in ['train','dev']:\n",
    "        data_file_path=os.path.join(data_dir,split+\".tsv\")\n",
    "        num_lines = sum(1 for _ in open(data_file_path))\n",
    "        with open(data_file_path, 'r') as csvfile:\n",
    "                next(csvfile)\n",
    "                for line in tqdm(csvfile,total=num_lines-1):\n",
    "                    line=line.strip().split(\"\\t\")\n",
    "                    text = line[0]\n",
    "                    if tokenizer_type==\"subword\":\n",
    "                        tokenized_text = tokenizer.tokenize(text)\n",
    "                    elif tokenizer_type==\"word\":\n",
    "                        tokenized_text = [token.text for token in tokenizer(text)]\n",
    "                    for token in tokenized_text:\n",
    "                        vocab[token]+=1\n",
    "    if tokenizer_type == \"subword\":\n",
    "        for token in tokenizer.vocab:\n",
    "            vocab[token]+=1\n",
    "    return vocab\n",
    "    \n",
    "vocab = get_vocab_SST2(args.data_dir, tokenizer, tokenizer_type=tokenizer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9663dd77-4f3b-40c9-b9f1-2b8d29c96fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Total Words: 30522, #Sensitive Words: 30522\n"
     ]
    }
   ],
   "source": [
    "sensitive_word_count = int(args.sensitive_word_percentage * len(vocab))\n",
    "words = [key for key, _ in vocab.most_common()]\n",
    "sensitive_words = words[-sensitive_word_count - 1:]\n",
    "\n",
    "sensitive_words2id = {word: k for k, word in enumerate(sensitive_words)}\n",
    "print(\"#Total Words: %d, #Sensitive Words: %d\" % (len(words),len(sensitive_words2id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9957e971-59eb-4d6c-9783-71491fb4be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_word_embed = []\n",
    "all_word_embed=[]\n",
    "\n",
    "word2id = {}\n",
    "sword2id = {}\n",
    "sensitive_count = 0\n",
    "all_count = 0\n",
    "for cur_word in tokenizer.vocab:\n",
    "    if cur_word in vocab and cur_word not in word2id:\n",
    "        word2id[cur_word] = all_count\n",
    "        emb = embedding_matrix[tokenizer.convert_tokens_to_ids(cur_word)]\n",
    "        all_word_embed.append(emb)\n",
    "        all_count += 1\n",
    "\n",
    "        if cur_word in sensitive_words2id:\n",
    "                sword2id[cur_word] = sensitive_count\n",
    "                sensitive_count += 1\n",
    "                sensitive_word_embed.append(emb)\n",
    "        assert len(word2id) == len(all_word_embed)\n",
    "        assert len(sword2id) == len(sensitive_word_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a6dd60f-03ec-4169-8320-2f0e820b90c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Word Embedding Matrix: (30522, 768)\n",
      "Sensitive Word Embedding Matrix: (30522, 768)\n",
      "Calculating Prob Matrix for Exponential Mechanism...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_word_embed=np.array(all_word_embed, dtype='f')\n",
    "sensitive_word_embed = np.array(sensitive_word_embed, dtype='f')\n",
    "\n",
    "print(\"All Word Embedding Matrix: %s\" % str(all_word_embed.shape))\n",
    "print(\"Sensitive Word Embedding Matrix: %s\" % str(sensitive_word_embed.shape))\n",
    "\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "def cal_probability(word_embed_1, word_embed_2, epsilon=2.0):\n",
    "    distance = euclidean_distances(word_embed_1, word_embed_2)\n",
    "    sim_matrix = -distance\n",
    "    prob_matrix = softmax(epsilon * sim_matrix / 2, axis=1)\n",
    "    return prob_matrix\n",
    "\n",
    "print(\"Calculating Prob Matrix for Exponential Mechanism...\")\n",
    "prob_matrix = cal_probability(all_word_embed, sensitive_word_embed, args.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bf5ea36-9ea7-4e50-97b1-c7e862163ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 30522)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c3addf3-ad23-4ea4-87f6-ec75be578559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.5674386e-03, 9.6986000e-04, 9.8571449e-04, ..., 3.8443824e-05,\n",
       "        3.0685129e-05, 1.0674237e-06],\n",
       "       [1.0004048e-03, 3.6797917e-03, 9.8736666e-04, ..., 3.9825387e-05,\n",
       "        3.1109194e-05, 1.0706297e-06],\n",
       "       [9.9128089e-04, 9.6262537e-04, 3.5875842e-03, ..., 3.8657694e-05,\n",
       "        3.2715223e-05, 1.0582394e-06],\n",
       "       ...,\n",
       "       [6.7392748e-04, 6.7683042e-04, 6.7387125e-04, ..., 6.2537871e-02,\n",
       "        3.8942115e-04, 6.5036838e-06],\n",
       "       [6.1992632e-04, 6.0930417e-04, 6.5722846e-04, ..., 4.4879198e-04,\n",
       "        7.2072342e-02, 5.4097072e-06],\n",
       "       [1.5426155e-04, 1.5000072e-04, 1.5207547e-04, ..., 5.3615910e-05,\n",
       "        3.8697464e-05, 5.1555777e-01]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4abd27-9519-4d86-845a-2afeb41ebb60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c295a9d2-61ac-49bd-bd9e-fb1cd77f217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "class Agrument:\n",
    "    def __init__(self):\n",
    "        self.gpt_model_path = \"gpt2\"\n",
    "        self.data_dir =\"./data/SST-2/\"\n",
    "        self.sensitive_word_percentage = 0.1\n",
    "        self.epsilon = 15\n",
    "        \n",
    "args = Agrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d1c72c9-25c9-4c7a-838c-f4a1d509986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = GPT2Tokenizer.from_pretrained(args.gpt_model_path)\n",
    "tokenizer_type = \"subword\"\n",
    "\n",
    "model = GPT2Model.from_pretrained(args.gpt_model_path)\n",
    "# Access the model's embedding layer\n",
    "embedding_layer = model.get_input_embeddings()\n",
    "# Get the embedding matrix\n",
    "embedding_matrix = embedding_layer.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0861ae9d-0d2c-4a5f-8473-7a3f5db31b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer_ref = embedding_layer\n",
    "# embedding_layer_ref.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a01fa2b-ddfc-4d01-87d3-0f6d7253476c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape #vocab_size, emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7479c58-3508-4d72-bc5a-42f4984871d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [00:01<00:00, 35108.24it/s]\n",
      "100%|██████████| 872/872 [00:00<00:00, 18358.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "def get_vocab_SST2(data_dir,tokenizer,tokenizer_type=\"subword\"):\n",
    "    vocab=Counter()\n",
    "    for split in ['train','dev']:\n",
    "        data_file_path=os.path.join(data_dir,split+\".tsv\")\n",
    "        num_lines = sum(1 for _ in open(data_file_path))\n",
    "        with open(data_file_path, 'r') as csvfile:\n",
    "                next(csvfile)\n",
    "                for line in tqdm(csvfile,total=num_lines-1):\n",
    "                    line=line.strip().split(\"\\t\")\n",
    "                    text = line[0]\n",
    "                    if tokenizer_type==\"subword\":\n",
    "                        tokenized_text = tokenizer.tokenize(text)\n",
    "                    elif tokenizer_type==\"word\":\n",
    "                        tokenized_text = [token.text for token in tokenizer(text)]\n",
    "                    for token in tokenized_text:\n",
    "                        vocab[token]+=1\n",
    "    if tokenizer_type == \"subword\":\n",
    "        for token in tokenizer.get_vocab():\n",
    "            vocab[token]+=1\n",
    "    return vocab\n",
    "    \n",
    "vocab = get_vocab_SST2(args.data_dir, tokenizer, tokenizer_type=tokenizer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be3e89e-2485-4a64-b215-eefe8b060bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ġ', 68282),\n",
       " ('Ġ,', 24998),\n",
       " ('Ġthe', 23450),\n",
       " ('Ġand', 20306),\n",
       " ('Ġa', 17420),\n",
       " ('Ġof', 17307),\n",
       " ('Ġ.', 13518),\n",
       " ('-', 12715),\n",
       " ('Ġto', 11970),\n",
       " (\"Ġ'\", 10388)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfce5267-b825-466c-b9a0-8bcce7726c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Total Words: 50257, #Sensitive Words: 5026\n"
     ]
    }
   ],
   "source": [
    "sensitive_word_count = int(args.sensitive_word_percentage * len(vocab))\n",
    "words = [key for key, _ in vocab.most_common()]\n",
    "sensitive_words = words[-sensitive_word_count - 1:]\n",
    "\n",
    "sensitive_words2id = {word: k for k, word in enumerate(sensitive_words)}\n",
    "print(\"#Total Words: %d, #Sensitive Words: %d\" % (len(words),len(sensitive_words2id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57fbd73c-e941-4f51-8551-d61a558918b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_word_embed = []\n",
    "all_word_embed=[]\n",
    "\n",
    "word2id = {}\n",
    "sword2id = {}\n",
    "sensitive_count = 0\n",
    "all_count = 0\n",
    "for cur_word in tokenizer.get_vocab():\n",
    "    if cur_word in vocab and cur_word not in word2id:\n",
    "        word2id[cur_word] = all_count\n",
    "        emb = embedding_matrix[tokenizer.convert_tokens_to_ids(cur_word)]\n",
    "        all_word_embed.append(emb)\n",
    "        all_count += 1\n",
    "\n",
    "        if cur_word in sensitive_words2id:\n",
    "                sword2id[cur_word] = sensitive_count\n",
    "                sensitive_count += 1\n",
    "                sensitive_word_embed.append(emb)\n",
    "        assert len(word2id) == len(all_word_embed)\n",
    "        assert len(sword2id) == len(sensitive_word_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1147aacc-e243-4470-934c-657071019499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Word Embedding Matrix: (50257, 768)\n",
      "Sensitive Word Embedding Matrix: (5026, 768)\n",
      "Calculating Prob Matrix for Exponential Mechanism...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_word_embed=np.array(all_word_embed, dtype='f')\n",
    "sensitive_word_embed = np.array(sensitive_word_embed, dtype='f')\n",
    "\n",
    "print(\"All Word Embedding Matrix: %s\" % str(all_word_embed.shape))\n",
    "print(\"Sensitive Word Embedding Matrix: %s\" % str(sensitive_word_embed.shape))\n",
    "\n",
    "from scipy.special import softmax #softmax(x) = np.exp(x)/sum(np.exp(x))\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "def cal_probability(word_embed_1, word_embed_2, epsilon=2.0):\n",
    "    distance = euclidean_distances(word_embed_1, word_embed_2)\n",
    "    sim_matrix = -distance #inverse protional to the distance between x and y\n",
    "    prob_matrix = softmax(epsilon * sim_matrix / 2, axis=1)\n",
    "    return prob_matrix\n",
    "\n",
    "print(\"Calculating Prob Matrix for Exponential Mechanism...\")\n",
    "prob_matrix = cal_probability(all_word_embed, sensitive_word_embed, args.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "580be1ac-4301-4d32-811d-c2b2e2d1bfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257, 5026)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68db16-f1f3-415d-adeb-61f39589c4a5",
   "metadata": {},
   "source": [
    "*Note If we know which idx is a sensitive word already*\n",
    "*Can we add noise during training when model found these idxs*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a019209-4744-4b1c-a372-0856b6882046",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Rerference\n",
    "https://github.com/xiangyue9607/SanText/tree/main\n",
    "https://github.com/xiangyue9607/SanText/blob/main/utils.py#L19\n",
    "https://github.com/xiangyue9607/SanText/blob/main/SanText.py#L31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "813b932a-64d3-407d-a565-b914e64d5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the vocabulary size and embedding dimensions\n",
    "vocab_size = 50257  # GPT-2's vocabulary size\n",
    "embedding_dim = 768  # GPT-2's embedding dimensions\n",
    "\n",
    "# Create an instance of the nn.Embedding module\n",
    "embedding_matrix = nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "24e0054a-cea9-4907-80ef-689cf788f8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix #vocab_size, emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4157ac11-a5c6-49b1-a34d-6e147251c0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input indices shape: torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Example: Create embeddings for an input tensor with shape (2, 1024)\n",
    "input_indices = torch.randint(0, vocab_size, (2, 1024))  # Replace with your desired input\n",
    "print(\"Input indices shape:\", input_indices.shape) #bs, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "40754bfd-87f5-41cb-88fd-feccf59464be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([2, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedding_matrix(input_indices)\n",
    "print(\"Embeddings shape:\", embeddings.shape) #bs, seq_len, emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3bd089-3719-4f78-981f-15a328b4e2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
