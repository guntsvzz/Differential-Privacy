{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64b6255c-3190-42ac-81d6-62ad23208cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.special import softmax\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, cpu_count\n",
    "class Agrument:\n",
    "    def __init__(self):\n",
    "        self.task = 'SST-2'\n",
    "        self.embedding_type = 'bert'\n",
    "        self.bert_model_path = \"bert-base-uncased\"\n",
    "        self.data_dir =\"./data/SST-2/\"\n",
    "        self.sensitive_word_percentage = 0.5\n",
    "        self.epsilon = 14\n",
    "        self.output_dir = \"./output_SanText_bert/SST-2/\"\n",
    "        self.threads = 12\n",
    "        self.p = 0.2\n",
    "        self.seed = 42\n",
    "        self.method = 'SanText'\n",
    "        \n",
    "args = Agrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed63c302-e122-4db1-81d9-999b6a33b027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "if args.method == \"SanText\":\n",
    "    args.sensitive_word_percentage = 1.0\n",
    "    args.output_dir = os.path.join(args.output_dir, \"eps_%.2f\" % args.epsilon)\n",
    "else:\n",
    "    args.output_dir = os.path.join(args.output_dir, \"eps_%.2f\" % args.epsilon, \"sword_%.2f_p_%.2f\"%(args.sensitive_word_percentage,args.p))\n",
    "\n",
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "\n",
    "print(\"Building Vocabulary...\")\n",
    "\n",
    "if args.embedding_type==\"glove\":\n",
    "    tokenizer = English()\n",
    "    tokenizer_type=\"word\"\n",
    "else:\n",
    "    tokenizer  = BertTokenizer.from_pretrained(args.bert_model_path)\n",
    "    tokenizer_type = \"subword\"    \n",
    "\n",
    "model=BertForMaskedLM.from_pretrained(args.bert_model_path)\n",
    "embedding_matrix = model.bert.embeddings.word_embeddings.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84da5ba7-7045-4f7c-8ad0-dfa1f7ffa423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [00:04<00:00, 13636.46it/s]\n",
      "100%|██████████| 872/872 [00:00<00:00, 7794.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "def get_vocab_SST2(data_dir,tokenizer,tokenizer_type=\"subword\"):\n",
    "    vocab=Counter()\n",
    "    for split in ['train','dev']:\n",
    "        data_file_path=os.path.join(data_dir,split+\".tsv\")\n",
    "        num_lines = sum(1 for _ in open(data_file_path))\n",
    "        with open(data_file_path, 'r') as csvfile:\n",
    "                next(csvfile)\n",
    "                for line in tqdm(csvfile,total=num_lines-1):\n",
    "                    line=line.strip().split(\"\\t\")\n",
    "                    text = line[0]\n",
    "                    if tokenizer_type==\"subword\":\n",
    "                        tokenized_text = tokenizer.tokenize(text)\n",
    "                    elif tokenizer_type==\"word\":\n",
    "                        tokenized_text = [token.text for token in tokenizer(text)]\n",
    "                    for token in tokenized_text:\n",
    "                        vocab[token]+=1\n",
    "    if tokenizer_type == \"subword\":\n",
    "        for token in tokenizer.vocab:\n",
    "            vocab[token]+=1\n",
    "    return vocab\n",
    "    \n",
    "vocab = get_vocab_SST2(args.data_dir, tokenizer, tokenizer_type=tokenizer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5fbc690-5005-44e0-be14-db10ef92d145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Total Words: 30522, #Sensitive Words: 30522\n"
     ]
    }
   ],
   "source": [
    "sensitive_word_count = int(args.sensitive_word_percentage * len(vocab))\n",
    "words = [key for key, _ in vocab.most_common()]\n",
    "sensitive_words = words[-sensitive_word_count - 1:]\n",
    "\n",
    "sensitive_words2id = {word: k for k, word in enumerate(sensitive_words)}\n",
    "print(\"#Total Words: %d, #Sensitive Words: %d\" % (len(words),len(sensitive_words2id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b97de8a2-b65a-4721-81a0-989375b6b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_word_embed = []\n",
    "all_word_embed=[]\n",
    "\n",
    "word2id = {}\n",
    "sword2id = {}\n",
    "sensitive_count = 0\n",
    "all_count = 0\n",
    "for cur_word in tokenizer.vocab:\n",
    "    if cur_word in vocab and cur_word not in word2id:\n",
    "        word2id[cur_word] = all_count\n",
    "        emb = embedding_matrix[tokenizer.convert_tokens_to_ids(cur_word)]\n",
    "        all_word_embed.append(emb)\n",
    "        all_count += 1\n",
    "\n",
    "        if cur_word in sensitive_words2id:\n",
    "                sword2id[cur_word] = sensitive_count\n",
    "                sensitive_count += 1\n",
    "                sensitive_word_embed.append(emb)\n",
    "        assert len(word2id) == len(all_word_embed)\n",
    "        assert len(sword2id) == len(sensitive_word_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65a15391-59ea-492c-9995-5261a5150fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Word Embedding Matrix: (30522, 768)\n",
      "Sensitive Word Embedding Matrix: (30522, 768)\n",
      "Calculating Prob Matrix for Exponential Mechanism...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_word_embed=np.array(all_word_embed, dtype='f')\n",
    "sensitive_word_embed = np.array(sensitive_word_embed, dtype='f')\n",
    "\n",
    "print(\"All Word Embedding Matrix: %s\" % str(all_word_embed.shape))\n",
    "print(\"Sensitive Word Embedding Matrix: %s\" % str(sensitive_word_embed.shape))\n",
    "\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "def cal_probability(word_embed_1, word_embed_2, epsilon=2.0):\n",
    "    distance = euclidean_distances(word_embed_1, word_embed_2)\n",
    "    sim_matrix = -distance\n",
    "    prob_matrix = softmax(epsilon * sim_matrix / 2, axis=1)\n",
    "    return prob_matrix\n",
    "\n",
    "print(\"Calculating Prob Matrix for Exponential Mechanism...\")\n",
    "prob_matrix = cal_probability(all_word_embed, sensitive_word_embed, args.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880c79eb-129b-400e-a52b-706ef853d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./data/SST-2/train.tsv. Will write to: ./output_SanText_bert/SST-2/train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [00:04<00:00, 13755.96it/s]\n",
      "Sanitize docs using SanText: 100%|██████████| 67349/67349 [00:01<00:00, 61980.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ...\n",
      "Processing file: ./data/SST-2/dev.tsv. Will write to: ./output_SanText_bert/SST-2/dev.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 872/872 [00:00<00:00, 7410.60it/s]\n",
      "Sanitize docs using SanText: 100%|██████████| 872/872 [00:00<00:00, 18262.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from SanText import SanText_plus,SanText_plus_init\n",
    "\n",
    "threads = min(args.threads, cpu_count())\n",
    "\n",
    "for file_name in ['train.tsv','dev.tsv']:\n",
    "    data_file = os.path.join(args.data_dir, file_name)\n",
    "    out_file = open(os.path.join(args.output_dir, file_name), 'w')\n",
    "    print(\"Processing file: %s. Will write to: %s\" % (data_file,os.path.join(args.output_dir, file_name)))\n",
    "\n",
    "    num_lines = sum(1 for _ in open(data_file))\n",
    "    with open(data_file, 'r') as rf:\n",
    "        # header\n",
    "        header = next(rf)\n",
    "        out_file.write(header)\n",
    "        labels = []\n",
    "        docs = []\n",
    "        if args.task == \"SST-2\":\n",
    "            for line in tqdm(rf, total=num_lines - 1):\n",
    "                content = line.strip().split(\"\\t\")\n",
    "                text = content[0]\n",
    "                label = int(content[1])\n",
    "                if args.embedding_type == \"glove\":\n",
    "                    doc = [token.text for token in tokenizer(text)]\n",
    "                else:\n",
    "                    doc = tokenizer.tokenize(text)\n",
    "                docs.append(doc)\n",
    "                labels.append(label)\n",
    "        rf.close()\n",
    "        \n",
    "        with Pool(threads, initializer=SanText_plus_init, initargs=(prob_matrix, word2id, sword2id, words, args.p, tokenizer)) as p:\n",
    "            annotate_ = partial(\n",
    "                SanText_plus,\n",
    "            )\n",
    "            results = list(\n",
    "                tqdm(\n",
    "                    p.imap(annotate_, docs, chunksize=32),\n",
    "                    total=len(docs),\n",
    "                    desc=\"Sanitize docs using SanText\",\n",
    "                )\n",
    "            )\n",
    "            p.close()\n",
    "\n",
    "        print(\"Saving ...\")\n",
    "        \n",
    "        if args.task == \"SST-2\":\n",
    "            for i, predicted_text in enumerate(results):\n",
    "                write_content = predicted_text + \"\\t\" + str(labels[i]) + \"\\n\"\n",
    "                out_file.write(write_content)\n",
    "\n",
    "        out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "082c700b-177a-49a7-8c06-9e3f302f07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SanText_plus_init(prob_matrix_init, word2id_init, sword2id_init, all_words_init, p_init, tokenizer_init):\n",
    "    global prob_matrix\n",
    "    global word2id\n",
    "    global sword2id\n",
    "    global id2sword\n",
    "    global all_words\n",
    "    global p\n",
    "    global tokenizer\n",
    "\n",
    "    prob_matrix = prob_matrix_init\n",
    "    word2id = word2id_init\n",
    "    sword2id=sword2id_init\n",
    "\n",
    "    id2sword = {v: k for k, v in sword2id.items()}\n",
    "\n",
    "    all_words = all_words_init\n",
    "    p=p_init\n",
    "    tokenizer=tokenizer_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff0fb222-db5c-46a6-a11e-9cb321857a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "SanText_plus_init(prob_matrix, word2id, sword2id, words, args.p, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cde06da-29a5-4172-8b37-fefb7556671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SanText_plus(doc):\n",
    "    new_doc = []\n",
    "    for word in doc:\n",
    "        if word in word2id:\n",
    "            # In-vocab\n",
    "            if word in sword2id:\n",
    "                #Sensitive Words\n",
    "                index = word2id[word]\n",
    "                sampling_prob = prob_matrix[index]\n",
    "                sampling_index = np.random.choice(len(sampling_prob), 1, p=sampling_prob)\n",
    "                new_doc.append(id2sword[sampling_index[0]])\n",
    "            else:\n",
    "                #Non-sensitive words\n",
    "                flip_p=random.random()\n",
    "                if flip_p<=p:\n",
    "                    #sample a word from Vs based on prob matrix\n",
    "                    index = word2id[word]\n",
    "                    sampling_prob = prob_matrix[index]\n",
    "                    sampling_index = np.random.choice(len(sampling_prob), 1, p=sampling_prob)\n",
    "                    new_doc.append(id2sword[sampling_index[0]])\n",
    "                else:\n",
    "                    #keep as the original\n",
    "                    new_doc.append(word)\n",
    "        else:\n",
    "            #Out-of-Vocab words\n",
    "            sampling_prob = 1 / len(all_words) * np.ones(len(all_words), )\n",
    "            sampling_index = np.random.choice(len(sampling_prob), 1, p=sampling_prob)\n",
    "            new_doc.append(all_words[sampling_index[0]])\n",
    "\n",
    "    new_doc = \" \".join(new_doc)\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63359532-df63-4aab-8c91-eb01c8a9af12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence\tlabel\n",
      "\n",
      "it 's a charming and often affecting journey . \n",
      "['it', \"'\", 's', 'a', 'charming', 'and', 'often', 'affecting', 'journey', '.']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "data_file = os.path.join(args.data_dir, file_name)\n",
    "with open(data_file, 'r') as rf:\n",
    "    header = next(rf)\n",
    "    print(header)\n",
    "    for line in rf:\n",
    "        content = line.strip().split(\"\\t\")\n",
    "        text = content[0]\n",
    "        label = int(content[1])\n",
    "        doc = tokenizer.tokenize(text)\n",
    "        print(text)\n",
    "        print(list(doc))\n",
    "        print(label)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "324d5339-f0ff-4f9a-9f46-ae9e0ce698a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it diocesan s a charming induced often [unused138] ##ya .'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SanText_plus(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5e6ed315-8ed6-4003-81c1-64302665ab92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it -> it\n",
      "' -> follows\n",
      "s -> portraits\n",
      "a -> a\n",
      "charming -> 1885\n",
      "and -> shi\n",
      "often -> spotted\n",
      "affecting -> [unused211]\n",
      "journey -> april\n",
      ". -> 1908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'it follows portraits a 1885 shi spotted [unused211] april 1908'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = []\n",
    "for word in doc:\n",
    "    if word in word2id:\n",
    "        # In-vocab\n",
    "        if word in sword2id:\n",
    "            #Sensitive Words\n",
    "            index = word2id[word]\n",
    "            sampling_prob = prob_matrix[index]\n",
    "            sampling_index = np.random.choice(len(sampling_prob), 1, p=sampling_prob) #changable\n",
    "            print(word,'->',id2sword[sampling_index[0]])\n",
    "            new_doc.append(id2sword[sampling_index[0]])\n",
    "        else:\n",
    "            #Non-sensitive words\n",
    "            flip_p=random.random()\n",
    "            if flip_p<=p:\n",
    "                #sample a word from Vs based on prob matrix\n",
    "                index = word2id[word]\n",
    "                sampling_prob = prob_matrix[index]\n",
    "                sampling_index = np.random.choice(len(sampling_prob), 1, p=sampling_prob) #changable\n",
    "                new_doc.append(id2sword[sampling_index[0]])\n",
    "            else:\n",
    "                #keep as the original\n",
    "                new_doc.append(word)\n",
    "                \n",
    "new_doc = \" \".join(new_doc)\n",
    "new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54a41b-b7e7-48bb-9b86-4b04af9e5efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
