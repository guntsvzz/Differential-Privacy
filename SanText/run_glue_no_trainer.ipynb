{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec14672-d647-4f5f-ab46-cc0be2d96440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6112896c-122f-41c8-9f29-36fc01b99a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataArgument:\n",
    "    def __init__(self):\n",
    "        self.task_name =  'sst-2'\n",
    "        self.data_dir  = './output_SanText_glove/SST-2/eps_3.00/'\n",
    "        self.max_seq_length = 128 \n",
    "        self.overwrite_cache = True\n",
    "\n",
    "class TrainingArgument:\n",
    "    def __init__(self):\n",
    "        self.do_train   = True\n",
    "        self.do_eval    = True\n",
    "        self.do_predict = False\n",
    "\n",
    "class ModelArgument:\n",
    "    def __init__(self):\n",
    "        self.cache_dir = None\n",
    "    \n",
    "class Argument:\n",
    "    def __init__(self):\n",
    "        self.model_name_or_path  = 'bert-base-uncased'\n",
    "        self.task_name = None\n",
    "        self.data_dir  = './output_SanText_glove/SST-2/eps_3.00/'\n",
    "        self.train_file = self.data_dir + \"train.tsv\"\n",
    "        self.validation_file = self.data_dir + \"dev.tsv\"\n",
    "        self.max_length = 128\n",
    "        \n",
    "        self.per_device_train_batch_size = 64 \n",
    "        self.per_device_eval_batch_size = 64 \n",
    "        \n",
    "        self.weight_decay = 0.0\n",
    "        self.learning_rate = 2e-5 \n",
    "        self.num_train_epochs = 3 \n",
    "        self.max_train_steps = None\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.lr_scheduler_type = \"linear\"\n",
    "        self.num_warmup_steps = 0\n",
    "        self.seed = None\n",
    "        \n",
    "        self.output_dir  = './tmp/sst2-sanitize/'\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.save_steps = 2000\n",
    "        self.trust_remote_code = False\n",
    "        self.use_slow_tokenizer = False\n",
    "        self.pad_to_max_length = True\n",
    "        \n",
    "        self.checkpointing_steps = None\n",
    "        self.resume_from_checkpoint = None\n",
    "        self.report_to = \"all\"\n",
    "        self.ignore_mismatched_sizes = True\n",
    "\n",
    "        self.with_tracking = False\n",
    "        self.push_to_hub = False\n",
    "\n",
    "args = Argument()\n",
    "data_args = DataArgument()\n",
    "training_args = TrainingArgument()\n",
    "model_args = ModelArgument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89acb18-98e1-4ca6-8994-2b9ed3d796a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "accelerator = (\n",
    "    Accelerator(log_with=args.report_to, project_dir=args.output_dir) if args.with_tracking else Accelerator()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "938e91de-9ab3-4de4-ac40-c074c140a0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguntsvzz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/todsavadt/work/DP-Embedding/SanText/wandb/run-20230915_104520-r30se2ia</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/guntsvzz/SanText-SST/runs/r30se2ia' target=\"_blank\">curious-silence-5</a></strong> to <a href='https://wandb.ai/guntsvzz/SanText-SST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/guntsvzz/SanText-SST' target=\"_blank\">https://wandb.ai/guntsvzz/SanText-SST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/guntsvzz/SanText-SST/runs/r30se2ia' target=\"_blank\">https://wandb.ai/guntsvzz/SanText-SST/runs/r30se2ia</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/guntsvzz/SanText-SST/runs/r30se2ia?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f3ddd4cfcd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"SanText-SST\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": args.learning_rate,\n",
    "    \"architecture\": args.model_name_or_path,\n",
    "    \"dataset\": \"SST-2\",\n",
    "    \"epochs\": args.num_train_epochs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01eca247-8bca-4b3d-b2e4-92323a93bec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/todsavadt/.cache/huggingface/datasets/csv/default-d5e0641ac45cae53/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 2/2 [00:00<00:00, 1520.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
    "# or specify a GLUE benchmark task (the dataset will be downloaded automatically from the datasets Hub).\n",
    "\n",
    "# For CSV/JSON files, this script will use as labels the column called 'label' and as pair of sentences the\n",
    "# sentences in columns called 'sentence1' and 'sentence2' if such column exists or the first two columns not named\n",
    "# label if at least two columns are provided.\n",
    "\n",
    "# If the CSVs/JSONs contain only one non-label column, the script does single sentence classification on this\n",
    "# single column. You can easily tweak this behavior (see below)\n",
    "\n",
    "# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if args.task_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\"glue\", args.task_name)\n",
    "else:\n",
    "    # Loading the dataset from local csv or json file.\n",
    "    data_files = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "    extension = (args.train_file if args.train_file is not None else args.validation_file).split(\".\")[-1]\n",
    "    if extension == 'tsv':\n",
    "        raw_datasets = load_dataset(\"csv\", data_files=data_files, delimiter='\\t')\n",
    "    else:\n",
    "        raw_datasets = load_dataset(extension, data_files=data_files, delimiter='\\t')\n",
    "    \n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c680b2dd-b8d8-49d5-bef6-86c97ac0cea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels\n",
    "if args.task_name is not None:\n",
    "    is_regression = args.task_name == \"stsb\"\n",
    "    if not is_regression:\n",
    "        label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "        num_labels = len(label_list)\n",
    "    else:\n",
    "        num_labels = 1\n",
    "else:\n",
    "    # Trying to have good defaults here, don't hesitate to tweak to your needs.\n",
    "    is_regression = raw_datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "    if is_regression:\n",
    "        num_labels = 1\n",
    "    else:\n",
    "        # A useful fast method:\n",
    "        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "        label_list = raw_datasets[\"train\"].unique(\"label\")\n",
    "        label_list.sort()  # Let's sort it for determinism\n",
    "        num_labels = len(label_list)\n",
    "        \n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56bd62b9-47c4-4098-bbdc-57ebe83ed20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "config = AutoConfig.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=args.task_name,\n",
    "    trust_remote_code=args.trust_remote_code,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=args.ignore_mismatched_sizes,\n",
    "    trust_remote_code=args.trust_remote_code,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3a20fc6-e6bb-48ae-a2d8-02bec3793386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets\n",
    "if args.task_name is not None:\n",
    "    sentence1_key, sentence2_key = task_to_keys[args.task_name]\n",
    "else:\n",
    "    # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n",
    "    non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
    "    if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n",
    "        sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n",
    "    else:\n",
    "        if len(non_label_column_names) >= 2:\n",
    "            sentence1_key, sentence2_key = non_label_column_names[:2]\n",
    "        else:\n",
    "            sentence1_key, sentence2_key = non_label_column_names[0], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d9430a4-76e2-479d-87ab-7e90da6304be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some models have set the order of the labels to use, so let's make sure we do use it.\n",
    "label_to_id = None\n",
    "if (\n",
    "    model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n",
    "    and args.task_name is not None\n",
    "    and not is_regression\n",
    "):\n",
    "    # Some have all caps in their config, some don't.\n",
    "    label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
    "    if sorted(label_name_to_id.keys()) == sorted(label_list):\n",
    "        logger.info(\n",
    "            f\"The configuration of the model provided the following label correspondence: {label_name_to_id}. \"\n",
    "            \"Using it!\"\n",
    "        )\n",
    "        label_to_id = {i: label_name_to_id[label_list[i]] for i in range(num_labels)}\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "            f\"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}.\"\n",
    "            \"\\nIgnoring the model labels as a result.\",\n",
    "        )\n",
    "elif args.task_name is None and not is_regression:\n",
    "    label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "if label_to_id is not None:\n",
    "        model.config.label2id = label_to_id\n",
    "        model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "elif args.task_name is not None and not is_regression:\n",
    "    model.config.label2id = {l: i for i, l in enumerate(label_list)}\n",
    "    model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "padding = \"max_length\" if args.pad_to_max_length else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aceb4cf-0a6e-462c-8104-a4ed929975e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/todsavadt/.cache/huggingface/datasets/csv/default-d5e0641ac45cae53/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-c118854d8b6f229a.arrow\n",
      "Loading cached processed dataset at /home/todsavadt/.cache/huggingface/datasets/csv/default-d5e0641ac45cae53/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-cd5bb8b0202a7f4d.arrow\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    texts = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*texts, padding=padding, max_length=args.max_length, truncation=True)\n",
    "    \n",
    "    if \"label\" in examples:\n",
    "        if label_to_id is not None:\n",
    "            # Map labels to IDs (not necessary for GLUE tasks)\n",
    "            result[\"labels\"] = [label_to_id[l] for l in examples[\"label\"]]\n",
    "        else:\n",
    "            # In all cases, rename the column to labels because the model will expect that.\n",
    "            result[\"labels\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    processed_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"validation_matched\" if args.task_name == \"mnli\" else \"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "095ef904-465b-4dd7-8e3c-3c5b96670cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "# DataLoaders creation:\n",
    "if args.pad_to_max_length:\n",
    "    # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "    # to tensors.\n",
    "    data_collator = default_data_collator\n",
    "else:\n",
    "    # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
    "    # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "    # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fa68d4e-a6d9-4105-99fc-b68e885902fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.max_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cacb710e-9220-4448-8148-abd52b3d43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "# Figure out how many steps we should save the Accelerator states\n",
    "checkpointing_steps = args.checkpointing_steps\n",
    "if checkpointing_steps is not None and checkpointing_steps.isdigit():\n",
    "    checkpointing_steps = int(checkpointing_steps)\n",
    "    \n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "# The trackers initializes automatically on the main process.\n",
    "if args.with_tracking:\n",
    "    experiment_config = vars(args)\n",
    "    # TensorBoard cannot log Enums, need the raw value\n",
    "    experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n",
    "    accelerator.init_trackers(\"glue_no_trainer\", experiment_config)\n",
    "\n",
    "# Get the metric function\n",
    "if args.task_name is not None:\n",
    "    metric = evaluate.load(\"glue\", args.task_name)\n",
    "else:\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Train!\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5446fe4d-713e-4cd2-ad54-4b95917bdefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 67349\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3159\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Running training *****\")\n",
    "print(f\"  Num examples = {len(train_dataset)}\")\n",
    "print(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "print(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "print(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "print(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "print(f\"  Total optimization steps = {args.max_train_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c81a82d3-8965-464e-9ac9-aa2b4cd80315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3159 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if args.resume_from_checkpoint:\n",
    "    if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n",
    "        checkpoint_path = args.resume_from_checkpoint\n",
    "        path = os.path.basename(args.resume_from_checkpoint)\n",
    "    else:\n",
    "        # Get the most recent checkpoint\n",
    "        dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n",
    "        dirs.sort(key=os.path.getctime)\n",
    "        path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n",
    "        checkpoint_path = path\n",
    "        path = os.path.basename(checkpoint_path)\n",
    "\n",
    "    accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n",
    "    accelerator.load_state(path)\n",
    "    # Extract `epoch_{i}` or `step_{i}`\n",
    "    training_difference = os.path.splitext(path)[0]\n",
    "\n",
    "    if \"epoch\" in training_difference:\n",
    "        starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n",
    "        resume_step = None\n",
    "        completed_steps = starting_epoch * num_update_steps_per_epoch\n",
    "    else:\n",
    "        # need to multiply `gradient_accumulation_steps` to reflect real steps\n",
    "        resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n",
    "        starting_epoch = resume_step // len(train_dataloader)\n",
    "        completed_steps = resume_step // args.gradient_accumulation_steps\n",
    "        resume_step -= starting_epoch * len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5360d8a-4238-4b6f-93d8-6018428346a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 3156/3159 [15:50<00:00,  3.31it/s]"
     ]
    }
   ],
   "source": [
    "# update the progress_bar if load from checkpoint\n",
    "progress_bar.update(completed_steps)\n",
    "\n",
    "for epoch in range(starting_epoch, args.num_train_epochs):\n",
    "    model.train()\n",
    "    if args.with_tracking:\n",
    "        total_loss = 0\n",
    "    if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n",
    "        # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n",
    "        active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n",
    "    else:\n",
    "        active_dataloader = train_dataloader\n",
    "    for step, batch in enumerate(active_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        # We keep track of the loss at each epoch\n",
    "        if args.with_tracking:\n",
    "            total_loss += loss.detach().float()\n",
    "        loss = loss / args.gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "\n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps % checkpointing_steps == 0:\n",
    "                output_dir = f\"step_{completed_steps}\"\n",
    "                if args.output_dir is not None:\n",
    "                    output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                accelerator.save_state(output_dir)\n",
    "\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    samples_seen = 0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1) if not is_regression else outputs.logits.squeeze()\n",
    "        predictions, references = accelerator.gather((predictions, batch[\"labels\"]))\n",
    "        # If we are in a multiprocess environment, the last batch has duplicates\n",
    "        if accelerator.num_processes > 1:\n",
    "            if step == len(eval_dataloader) - 1:\n",
    "                predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]\n",
    "                references = references[: len(eval_dataloader.dataset) - samples_seen]\n",
    "            else:\n",
    "                samples_seen += references.shape[0]\n",
    "        metric.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "        )\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    logger.info(f\"epoch {epoch}: {eval_metric}\")\n",
    "    wandb.log({\"Epoch\" : epoch, \"eval_metric\": eval_metric})\n",
    "\n",
    "    # if args.with_tracking:\n",
    "    #     accelerator.log(\n",
    "    #         {\n",
    "    #             \"accuracy\" if args.task_name is not None else \"glue\": eval_metric,\n",
    "    #             \"train_loss\": total_loss.item() / len(train_dataloader),\n",
    "    #             \"epoch\": epoch,\n",
    "    #             \"step\": completed_steps,\n",
    "    #         },\n",
    "    #         step=completed_steps,\n",
    "    #     )\n",
    "\n",
    "    if args.push_to_hub and epoch < args.num_train_epochs - 1:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "        )\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(args.output_dir)\n",
    "            # repo.push_to_hub(\n",
    "            #     commit_message=f\"Training in progress epoch {epoch}\", blocking=False, auto_lfs_prune=True\n",
    "            # )\n",
    "\n",
    "    if args.checkpointing_steps == \"epoch\":\n",
    "        output_dir = f\"epoch_{epoch}\"\n",
    "        if args.output_dir is not None:\n",
    "            output_dir = os.path.join(args.output_dir, output_dir)\n",
    "        accelerator.save_state(output_dir)\n",
    "\n",
    "# if args.with_tracking:\n",
    "#     accelerator.end_training()\n",
    "\n",
    "if args.output_dir is not None:\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "        if args.push_to_hub:\n",
    "            repo.push_to_hub(commit_message=\"End of training\", auto_lfs_prune=True)\n",
    "\n",
    "# if args.task_name == \"mnli\":\n",
    "#     # Final evaluation on mismatched validation set\n",
    "#     eval_dataset = processed_datasets[\"validation_mismatched\"]\n",
    "#     eval_dataloader = DataLoader(\n",
    "#         eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
    "#     )\n",
    "#     eval_dataloader = accelerator.prepare(eval_dataloader)\n",
    "\n",
    "#     model.eval()\n",
    "#     for step, batch in enumerate(eval_dataloader):\n",
    "#         outputs = model(**batch)\n",
    "#         predictions = outputs.logits.argmax(dim=-1)\n",
    "#         metric.add_batch(\n",
    "#             predictions=accelerator.gather(predictions),\n",
    "#             references=accelerator.gather(batch[\"labels\"]),\n",
    "#         )\n",
    "\n",
    "#     eval_metric = metric.compute()\n",
    "#     logger.info(f\"mnli-mm: {eval_metric}\")\n",
    "\n",
    "# if args.output_dir is not None:\n",
    "#     all_results = {f\"eval_{k}\": v for k, v in eval_metric.items()}\n",
    "#     with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n",
    "#         json.dump(all_results, f)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
