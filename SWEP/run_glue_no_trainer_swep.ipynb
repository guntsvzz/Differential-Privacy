{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5066a67e-2afd-454f-8abb-4d0aca12ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b540742-bf54-4142-82a1-6a2a3cf44a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataArgument:\n",
    "    def __init__(self):\n",
    "        self.task_name =  'sst-2'\n",
    "        self.data_dir  = './output_SanText_glove/SST-2/eps_3.00/'\n",
    "        self.max_seq_length = 128 \n",
    "        self.overwrite_cache = True\n",
    "\n",
    "class TrainingArgument:\n",
    "    def __init__(self):\n",
    "        self.do_train   = True\n",
    "        self.do_eval    = True\n",
    "        self.do_predict = False\n",
    "\n",
    "class ModelArgument:\n",
    "    def __init__(self):\n",
    "        self.cache_dir = None\n",
    "    \n",
    "class Argument:\n",
    "    def __init__(self):\n",
    "        self.model_name_or_path  = 'bert-base-uncased'\n",
    "        self.task_name = None\n",
    "        self.data_dir  = './output_SanText_glove/SST-2/eps_3.00/'\n",
    "        self.train_file = self.data_dir + \"train.tsv\"\n",
    "        self.validation_file = self.data_dir + \"dev.tsv\"\n",
    "        self.max_length = 128\n",
    "        \n",
    "        self.per_device_train_batch_size = 64 \n",
    "        self.per_device_eval_batch_size = 64 \n",
    "        \n",
    "        self.weight_decay = 0.0\n",
    "        self.learning_rate = 2e-5 \n",
    "        self.num_train_epochs = 1\n",
    "        self.max_train_steps = None\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.lr_scheduler_type = \"linear\"\n",
    "        self.num_warmup_steps = 0\n",
    "        self.seed = None\n",
    "        \n",
    "        self.output_dir  = './tmp/sst2-swep/'\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.save_steps = 2000\n",
    "        self.trust_remote_code = False\n",
    "        self.use_slow_tokenizer = False\n",
    "        self.pad_to_max_length = True\n",
    "        \n",
    "        self.checkpointing_steps = None\n",
    "        self.resume_from_checkpoint = None\n",
    "        self.report_to = \"all\"\n",
    "        self.ignore_mismatched_sizes = True\n",
    "\n",
    "        self.with_tracking = False\n",
    "        self.push_to_hub = False\n",
    "\n",
    "        #swep\n",
    "        self.baseline = False\n",
    "        self.hidden_size = 768\n",
    "        self.dropout =0.1\n",
    "        self.beta = 1\n",
    "\n",
    "args = Argument()\n",
    "data_args = DataArgument()\n",
    "training_args = TrainingArgument()\n",
    "model_args = ModelArgument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f797c559-52ee-4d98-89d3-f0649e5a5120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    PretrainedConfig,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "accelerator = (\n",
    "    Accelerator(log_with=args.report_to, project_dir=args.output_dir) if args.with_tracking else Accelerator()\n",
    ")\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d65b71b8-4344-433c-afc0-4d591ae33a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.task_name = \"sst2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbc6043-0fdd-40e2-9e7d-95025f4944d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/todsavadt/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1750.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
    "# or specify a GLUE benchmark task (the dataset will be downloaded automatically from the datasets Hub).\n",
    "\n",
    "# For CSV/JSON files, this script will use as labels the column called 'label' and as pair of sentences the\n",
    "# sentences in columns called 'sentence1' and 'sentence2' if such column exists or the first two columns not named\n",
    "# label if at least two columns are provided.\n",
    "\n",
    "# If the CSVs/JSONs contain only one non-label column, the script does single sentence classification on this\n",
    "# single column. You can easily tweak this behavior (see below)\n",
    "\n",
    "# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if args.task_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\"glue\", args.task_name)\n",
    "else:\n",
    "    # Loading the dataset from local csv or json file.\n",
    "    data_files = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "    extension = (args.train_file if args.train_file is not None else args.validation_file).split(\".\")[-1]\n",
    "    if extension == 'tsv':\n",
    "        raw_datasets = load_dataset(\"csv\", data_files=data_files, delimiter='\\t')\n",
    "    else:\n",
    "        raw_datasets = load_dataset(extension, data_files=data_files, delimiter='\\t')\n",
    "    \n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5401348-b8fa-407a-ac88-fd66696eef00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels\n",
    "if args.task_name is not None:\n",
    "    is_regression = args.task_name == \"stsb\"\n",
    "    if not is_regression:\n",
    "        label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "        num_labels = len(label_list)\n",
    "    else:\n",
    "        num_labels = 1\n",
    "else:\n",
    "    # Trying to have good defaults here, don't hesitate to tweak to your needs.\n",
    "    is_regression = raw_datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "    if is_regression:\n",
    "        num_labels = 1\n",
    "    else:\n",
    "        # A useful fast method:\n",
    "        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "        label_list = raw_datasets[\"train\"].unique(\"label\")\n",
    "        label_list.sort()  # Let's sort it for determinism\n",
    "        num_labels = len(label_list)\n",
    "        \n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d72a08-1399-421a-adcb-d1c866b4cfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from models import VariationalBert\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "config = AutoConfig.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=args.task_name,\n",
    "    trust_remote_code=args.trust_remote_code,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n",
    ")\n",
    "if args.baseline:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "        ignore_mismatched_sizes=args.ignore_mismatched_sizes,\n",
    "        trust_remote_code=args.trust_remote_code,\n",
    "    )\n",
    "else:\n",
    "    model = VariationalBert(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a57e9537-f498-406d-ae2a-b519fb50f077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariationalBert(\n",
       "  (bert_model): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       "  (noise_net): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=768, out_features=1536, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37e92035-9e8c-4009-a406-65ca22aba004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets\n",
    "if args.task_name is not None:\n",
    "    sentence1_key, sentence2_key = task_to_keys[args.task_name]\n",
    "else:\n",
    "    # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n",
    "    non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
    "    if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n",
    "        sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n",
    "    else:\n",
    "        if len(non_label_column_names) >= 2:\n",
    "            sentence1_key, sentence2_key = non_label_column_names[:2]\n",
    "        else:\n",
    "            sentence1_key, sentence2_key = non_label_column_names[0], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fdc622d-033e-4a7f-b69d-254ac487a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some models have set the order of the labels to use, so let's make sure we do use it.\n",
    "label_to_id = None\n",
    "if (\n",
    "    model.bert_model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n",
    "    and args.task_name is not None\n",
    "    and not is_regression\n",
    "):\n",
    "    # Some have all caps in their config, some don't.\n",
    "    label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
    "    if sorted(label_name_to_id.keys()) == sorted(label_list):\n",
    "        logger.info(\n",
    "            f\"The configuration of the model provided the following label correspondence: {label_name_to_id}. \"\n",
    "            \"Using it!\"\n",
    "        )\n",
    "        label_to_id = {i: label_name_to_id[label_list[i]] for i in range(num_labels)}\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "            f\"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}.\"\n",
    "            \"\\nIgnoring the model labels as a result.\",\n",
    "        )\n",
    "elif args.task_name is None and not is_regression:\n",
    "    label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "if label_to_id is not None:\n",
    "    model.bert_model.config.label2id = label_to_id\n",
    "    model.bert_model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "elif args.task_name is not None and not is_regression:\n",
    "    model.bert_model.config.label2id = {l: i for i, l in enumerate(label_list)}\n",
    "    model.bert_model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "padding = \"max_length\" if args.pad_to_max_length else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a37c960f-cb12-42b7-a58b-baffec9d2e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    texts = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*texts, padding=padding, max_length=args.max_length, truncation=True)\n",
    "    \n",
    "    if \"label\" in examples:\n",
    "        if label_to_id is not None:\n",
    "            # Map labels to IDs (not necessary for GLUE tasks)\n",
    "            result[\"labels\"] = [label_to_id[l] for l in examples[\"label\"]]\n",
    "        else:\n",
    "            # In all cases, rename the column to labels because the model will expect that.\n",
    "            result[\"labels\"] = examples[\"label\"]\n",
    "    return result\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    processed_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"validation_matched\" if args.task_name == \"mnli\" else \"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6e33491-9a58-41c6-b89d-be1e0b9ba088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aa80798-ffec-4f25-afad-d40ac2ac45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "# DataLoaders creation:\n",
    "if args.pad_to_max_length:\n",
    "    # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "    # to tensors.\n",
    "    data_collator = default_data_collator\n",
    "else:\n",
    "    # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
    "    # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "    # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b62ebf39-dead-43a5-b90a-55d00bc90636",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = train_dataset.shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = eval_dataset.shuffle(seed=42).select(range(100))\n",
    "args.test_small = False\n",
    "if args.test_small:\n",
    "    train_dataloader = DataLoader(\n",
    "        small_train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        small_eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n",
    "else:\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48a51ffd-c642-4470-9e38-bd913f46e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.max_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9796157-edf7-40a8-89db-4f7c4296b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "# Figure out how many steps we should save the Accelerator states\n",
    "checkpointing_steps = args.checkpointing_steps\n",
    "if checkpointing_steps is not None and checkpointing_steps.isdigit():\n",
    "    checkpointing_steps = int(checkpointing_steps)\n",
    "    \n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "# The trackers initializes automatically on the main process.\n",
    "if args.with_tracking:\n",
    "    experiment_config = vars(args)\n",
    "    # TensorBoard cannot log Enums, need the raw value\n",
    "    experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n",
    "    accelerator.init_trackers(\"glue_no_trainer\", experiment_config)\n",
    "\n",
    "# Get the metric function\n",
    "if args.task_name is not None:\n",
    "    metric = evaluate.load(\"glue\", args.task_name)\n",
    "else:\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Train!\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2bd7abc-d5cd-4ea3-92eb-402567cc875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 67349\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1053\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Running training *****\")\n",
    "print(f\"  Num examples = {len(train_dataset)}\")\n",
    "print(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "print(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "print(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "print(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "print(f\"  Total optimization steps = {args.max_train_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edbfee1a-0027-4289-8b56-d9cd195643db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1053/1053 [12:06<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: Original {'accuracy': 0.9225917431192661}\n"
     ]
    }
   ],
   "source": [
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# # update the progress_bar if load from checkpoint\n",
    "# progress_bar.update(completed_steps)\n",
    "\n",
    "for epoch in range(starting_epoch, args.num_train_epochs):\n",
    "    model.train()\n",
    "    if args.with_tracking:\n",
    "        total_loss = 0\n",
    "    if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n",
    "        # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n",
    "        active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n",
    "    else:\n",
    "        active_dataloader = train_dataloader\n",
    "    for step, batch in enumerate(active_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        nll, kl, outputs, outputs_noise = outputs[0], outputs[1], outputs[2], outputs[3]\n",
    "        loss = nll + kl * args.beta\n",
    "        # loss = outputs.loss\n",
    "        # We keep track of the loss at each epoch\n",
    "        if args.with_tracking:\n",
    "            total_loss += loss.detach().float()\n",
    "        loss = loss / args.gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "\n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps % checkpointing_steps == 0:\n",
    "                output_dir = f\"step_{completed_steps}\"\n",
    "                if args.output_dir is not None:\n",
    "                    output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                accelerator.save_state(output_dir)\n",
    "\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    samples_seen = 0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs[2].logits.argmax(dim=-1) if not is_regression else outputs[2].logits.squeeze()\n",
    "        predictions, references = accelerator.gather((predictions, batch[\"labels\"]))\n",
    "\n",
    "        predictions_noise = outputs[3].logits.argmax(dim=-1) if not is_regression else outputs[2].logits.squeeze()\n",
    "        predictions_noise, references = accelerator.gather((predictions_noise, batch[\"labels\"]))\n",
    "        \n",
    "        # If we are in a multiprocess environment, the last batch has duplicates\n",
    "        if accelerator.num_processes > 1:\n",
    "            if step == len(eval_dataloader) - 1:\n",
    "                predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]\n",
    "                predictions_noise = predictions_noise[: len(eval_dataloader.dataset) - samples_seen]\n",
    "                references = references[: len(eval_dataloader.dataset) - samples_seen]\n",
    "            else:\n",
    "                samples_seen += references.shape[0]\n",
    "                \n",
    "        metric.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "        )\n",
    "    \n",
    "        metric.add_batch(\n",
    "            predictions=predictions_noise,\n",
    "            references=references,\n",
    "        )\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    # even_metric_noise = metric.compute()\n",
    "    # logger.info(f\"epoch {epoch}: {eval_metric}\")\n",
    "    print(f\"epoch {epoch+1}: Original {eval_metric}\") # : Noise {even_metric_noise}\")\n",
    "    # wandb.log({\"Epoch\" : epoch, \"eval_metric\": eval_metric})\n",
    "\n",
    "    # if args.with_tracking:\n",
    "    #     accelerator.log(\n",
    "    #         {\n",
    "    #             \"accuracy\" if args.task_name is not None else \"glue\": eval_metric,\n",
    "    #             \"train_loss\": total_loss.item() / len(train_dataloader),\n",
    "    #             \"epoch\": epoch,\n",
    "    #             \"step\": completed_steps,\n",
    "    #         },\n",
    "    #         step=completed_steps,\n",
    "    #     )\n",
    "\n",
    "    if args.push_to_hub and epoch < args.num_train_epochs - 1:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "        )\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(args.output_dir)\n",
    "            # repo.push_to_hub(\n",
    "            #     commit_message=f\"Training in progress epoch {epoch}\", blocking=False, auto_lfs_prune=True\n",
    "            # )\n",
    "\n",
    "    if args.checkpointing_steps == \"epoch\":\n",
    "        output_dir = f\"epoch_{epoch}\"\n",
    "        if args.output_dir is not None:\n",
    "            output_dir = os.path.join(args.output_dir, output_dir)\n",
    "        accelerator.save_state(output_dir)\n",
    "\n",
    "# if args.with_tracking:\n",
    "#     accelerator.end_training()\n",
    "\n",
    "# if args.output_dir is not None:\n",
    "#     accelerator.wait_for_everyone()\n",
    "#     unwrapped_model = accelerator.unwrap_model(model)\n",
    "#     unwrapped_model.save_pretrained(\n",
    "#         args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "#     )\n",
    "#     if accelerator.is_main_process:\n",
    "#         tokenizer.save_pretrained(args.output_dir)\n",
    "#         if args.push_to_hub:\n",
    "#             repo.push_to_hub(commit_message=\"End of training\", auto_lfs_prune=True)\n",
    "\n",
    "# if args.task_name == \"mnli\":\n",
    "#     # Final evaluation on mismatched validation set\n",
    "#     eval_dataset = processed_datasets[\"validation_mismatched\"]\n",
    "#     eval_dataloader = DataLoader(\n",
    "#         eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
    "#     )\n",
    "#     eval_dataloader = accelerator.prepare(eval_dataloader)\n",
    "\n",
    "#     model.eval()\n",
    "#     for step, batch in enumerate(eval_dataloader):\n",
    "#         outputs = model(**batch)\n",
    "#         predictions = outputs.logits.argmax(dim=-1)\n",
    "#         metric.add_batch(\n",
    "#             predictions=accelerator.gather(predictions),\n",
    "#             references=accelerator.gather(batch[\"labels\"]),\n",
    "#         )\n",
    "\n",
    "#     eval_metric = metric.compute()\n",
    "#     logger.info(f\"mnli-mm: {eval_metric}\")\n",
    "\n",
    "# if args.output_dir is not None:\n",
    "#     all_results = {f\"eval_{k}\": v for k, v in eval_metric.items()}\n",
    "#     with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n",
    "#         json.dump(all_results, f)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99104373-ee19-4955-8f10-6d4ff03ca60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch 0: {'accuracy': 0.9231651376146789}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de282d86-3957-4013-83a1-e66253e6414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(args, model):\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    ckpt_file = os.path.join(args.output_dir, \"bert_base_swep.pt\")\n",
    "    ckpt = {\"args\": args, \"state_dict\": model.state_dict()}\n",
    "    torch.save(ckpt, ckpt_file)\n",
    "\n",
    "save_model(args, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03036636-882d-463c-aa18-4a808829e7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
